Abstract—Zab is a crash-recovery atomic broadcast algorithm we designed for the ZooKeeper coordination service.
ZooKeeper implements a primary-backup scheme in which a primary process executes clients operations and uses Zab to propagate the corresponding incremental state changes to backup processes1
Due the dependence of an incremental state change on the sequence of changes previously generated, Zab must guarantee that if it delivers a given state change, then all other changes it depends upon must be delivered first.
Since primaries may crash, Zab must satisfy this requirement despite crashes of primaries.
Applications using ZooKeeper demand high-performance from the service, and consequently, one important goal is the ability of having multiple outstanding client operations at a time.
Zab enables multiple outstanding state changes by guaranteeing that at most one primary is able to broadcast state changes and have them incorporated into the state, and by using a synchronization phase while establishing a new primary.
Before this synchronization phase completes, a new primary does not broadcast new state changes.
Finally, Zab uses an identification scheme for state changes that enables a process to easily identify missing changes.
Experiments and experience so far in production show that our design enables an implementation that meets the performance requirements of our applications.
Our implementation of Zab can achieve tens of thousands of broadcasts per second, which is sufficient for demanding systems such as our Web-scale applications.
Atomic broadcast is a commonly used primitive in distributed computing and ZooKeeper is yet another application to use atomic broadcast.
ZooKeeper is a highly-available coordination service used in production Web systems such as the Yahoo! crawler for over three years.
Such applications often comprise a large number of processes and rely upon ZooKeeper to perform important coordination tasks, such as storing configuration data reliably and keeping the status of running processes.
Given the reliance of large applications on ZooKeeper, the service must be able to mask and recover from failures.
ZooKeeper is a replicated service, and it requires that a majority (or more generally a quorum) of servers has not crashed for progress.
With ZooKeeper, a primary process receives all incoming client requests, executes them, and propagates the resulting non-commutative, incremental state changes in the form of transactions to the backup replicas using Zab, the ZooKeeper atomic broadcast protocol.
Upon primary crashes, processes execute a recovery protocol both to agree upon a common consistent state before resuming regular operation and to establish a new primary to broadcast state changes.
To exercise the primary role, a process must have the support of a quorum of processes.
As processes can crash and recover, there can be over time multiple primaries and in fact the same process may exercise the primary role multiple times.
To distinguish the different primaries over time, we associate an instance value with each established primary.
A given instance value maps to at most one process.
Note that our notion of instance shares some of the properties of views of group communication [8], but it presents some key differences.
With group communication, all processes in a given view are able to broadcast, and configuration changes happen when any process joins or leaves.
With Zab, processes change to a new view (or primary instance) only when a primary crashes or loses support from a quorum.
Critical to the design of Zab is the observation that each state change is incremental with respect to the previous state, so there is an implicit dependence on the order of the state changes.
State changes consequently cannot be applied in any arbitrary order, and it is critical to guarantee that a prefix of the state changes generated by a given primary are delivered and applied to the service state.
State changes are idempotent and applying the same state change multiple times does not lead to inconsistencies as long as the application order is consistent with the delivery order.
Consequently, guaranteeing at-least once semantics is sufficient and simplifies the implementation.
As Zab is a critical component of the ZooKeeper core, it must perform well.
Some applications of ZooKeeper encompass a large number of processes and use ZooKeeper extensively.
We designed ZooKeeper to have high throughput and low latency, so that applications could use it extensively on cluster environments: data centers with a large number of wellconnected nodes.
When designing ZooKeeper, however, we found it difficult to reason about atomic broadcast in isolation.
Multiple outstanding transactions: It is important in our setting that we enable multiple outstanding ZooKeeper operations and that a prefix of operations submitted concurrently by a ZooKeeper client are committed according to FIFO order.
Traditional protocols to implement replicated state machines, like Paxos [2], do not enable such a feature directly, however.
If primaries propose transactions individually, then the order of learned transactions might not satisfy the order dependencies and consequently the sequence of learned transactions cannot be used unmodified.
One known solution to this problem is batching multiple transactions into a single Paxos proposal and having at most one outstanding proposal at a time.
Such a design affects either throughput or latency adversely depending on the choice of the batch size.
Figure 1 illustrates a problem we found with Paxos under our requirements.
It shows a run with three distinct proposers that violates our requirement for the order of generated state changes.
Such a run is not acceptable because the state change represented by B causally depends upon A, and not C.
Consequently, B can only be chosen for sequence number i+1 if A has been chosen for sequence number i, and C cannot be chosen before B, since the state change that B represents cannot commute with C and can only be applied after A.
Efficient recovery: One important goal in our setting is to recover efficiently from primary crashes.
For fast recovery, we use a transaction identification scheme that enables a new primary to determine in a simple manner which sequence of transactions to use to recover the application state.
In our scheme, transaction identifiers are pairs of values: an instance value and the position of a given transaction in the sequence broadcast by the primary process for that instance.
Under this scheme, only the process having accepted the transaction with the highest identifier may have to copy transactions to the new primary, and no other transaction requires recovery.
This observation implies that a new primary is able to decide which transactions to recover and from which process simply by collecting the highest transaction identifier from each process in a quorum.
For recovery with Paxos, having the last sequence number for which a process accepted a value is not sufficient, since processes might accept different values (with different ballot numbers) for every sequence number.
Consequently, a new primary has to execute Phases 1 of Paxos for all previous.
Summary of contributions: We describe here the design of Zab, an atomic broadcast protocol for the ZooKeeper coordination service.
Zab is a high-performance atomic broadcast protocol for primary-backup systems.
Compared to previous atomic broadcast protocols, Zab satisfies a different set of correctness properties.
In particular, we propose a property called primary order that is important for primary-backup systems.
This property is critical to enable the correct ordering of state changes over time as different processes exercise the primary role while allowing multiple outstanding transactions.
Primary order is different from causal order, as we discuss in Section III-B.
Given our use of the primary order property, we say that Zab is a PO atomic broadcast protocol.
Finally, our scheme for identifying transactions enables faster recovery compared to classic algorithms such as Paxos, since Zab transaction identifiers map to at most one transaction and processes accept them in order.
We say that a process is up if it is not crashed, and down otherwise.
A process that is recovering is up, and for progress, we assume that eventually enough processes are up for sufficiently long.
In fact, we have progress if a quorum of processes is up and able to pairwise exchange messages for sufficiently long.
We assume that a quorum system Q is defined a priori, and that Q satisfies the following: Definition II.1
More precisely, the channel cij between processes pi and pj is such that each of the processes has a pair of buffers: an input buffer and an output buffer.
A call to send a message m to process pj is represented by an event send(m, pj), which inserts m into the output buffer of pi for cij.
Messages are transmitted following the order of send events, and they are inserted into the input buffer.
A call to receive the next message m in the input buffer is represented by an event recv(m, pi)
To specify the properties of channels, we use the notion of iterations, since the algorithm we propose proceeds in iterations, and in each iteration we have three phases.
We assume that the channel between processes pi and pj satisfies the following properties: Integrity: Process pj receives a message m from pi only if.
Single iteration: The input buffer of a process pj for channel cij contains messages from at most one iteration.
Implementation of channels: To implement the properties we state for channels and ensure liveness, it is sufficient to assume fair-lossy links (a precise definition of fair-lossy in the crash-recovery model appears in the work of Boichat and Guerraoui [12])
At the beginning of a new iteration, we establish a connection between pi and pj.
By doing so, we guarantee that only messages sent are received (Integrity), a prefix of the sequence of messages sent from a process pi to a process pj are received, and once we close a connection and establish a new one, we guarantee that a process only has messages from a single iteration.
ZooKeeper uses a primary-backup scheme to execute requests and propagate state changes to backup processes using.
If a primary process crashes, we assume an external mechanism exists for selecting a new primary.
It is important, however, to guarantee that at any time there is at most one active primary process that is allowed to broadcast.
In our implementation, the primary election mechanism is tightly coupled with the mechanisms we use in the broadcast layer.
For specification purposes, it is sufficient to assume that some mechanism exists to select primaries and such a mechanism guarantees that at most one primary is active at any time.
Over time, we have an unbounded sequence of primaries: ρ1ρ2
Precedence of primaries refers to the sequence of processes that are primaries over time.
In fact, since processes can recover, there can be but refer to different instances.
To guarantee that the transactions a primary broadcast are consistent, we need to make sure that a primary only starts generating state updates once the Zab layer indicates that recovery has completed.
For this purpose, we assume that processes implement a ready(e) call, which the Zab layer uses to signal to the application (primary and backup replicas) that it can start broadcasting state changes.
A call to ready(e) also sets the value of the variable instance that a primary uses to determine its instance.
The primary uses the value of instance to set the epoch of transaction identifiers when broadcasting, and we assume that the value of e is unique for different primary instances.
We call transactions the state changes a primary propagates to the backup processes.
We use epoch(z) to denote the epoch of a transaction identifier and counter(z) to denote the counter value of z.
Similarly, we say that an epoch e is later that e′
Consequently, from the sequence of state changes a primary broadcasts, only a prefix of the sequence of state updates is delivered.
Upon delivering a transaction, a process adds it to a txns set.
These two properties guarantee that no transaction is created spontaneously or corrupted and that processes that deliver transactions must deliver them according to a consistent order.
The total order property, however, allows runs in which two processes deliver disjoint sequences of transactions.
Note that the statement of agreement is different compared to previous work.
We instead state agreement as a safety property, which guarantees that the state of two processes do not diverge.
The three safety properties above guarantee that processes are consistent.
However, we need to satisfy one more property to enable multiple changes in progress from a given primary.
Since each state change is based on a previous state if the change for that previous state is skipped, the dependent changes must also be skipped.
Note that local primary order corresponds to FIFO order for a single primary instance, and that global primary order prevents runs such as the one described in Figure 1
Finally, a primary has to guarantee that the state updates generated are consistent.
Comparison with causal atomic broadcast PO atomic broadcast is designed to preserve the causal.
In this section, we compare causal atomic broadcast and PO atomic broadcast, and argue that they are not comparable.
The definition of causal order is based on the precedence (or happens before) relation of events [14]
For broadcast protocols, the events are either broadcast or deliver events.
The causal order property for atomic broadcast protocols is typically defined as (adapted from the definition of De´fago et al.
To simplify the discussion, we present only events for two processes.
The delivery order of PO atomic broadcast respects a primary causal order relation ≺po that is strictly weaker than causal order.
In fact, transactions sent by different primaries are not necessarily considered as causally related even if they are actually sent by the same process.
Strict causality is needed because transactions are incremental updates so they can only be applied to the state used to produce them, which is the result of a chain of causally related updates.
With causal order, however, there can be transactions delivered that are not causally related.
This example shows that none of the two primitives is stronger than the other.
It follows directly from the core properties that PO atomic broadcast implements PO causal order and strict causality [15]
Each process executes one iteration of this protocol at a time, and at any time, a process may drop the current iteration and start a new one by proceeding to Phase 1
There are two roles Zab process can perform according to the protocol: leader and follower.
A leader concurrently executes the primary role and proposes transactions according to the order of broadcast calls of the primary.
Followers accept transactions according to the steps of the protocol.
Each process implements a leader oracle, and the leader oracle provides the identifier of the prospective leader `
If the leader oracle of a process determines that it is the leader, then it executes the leader steps of the protocol.
Being selected the leader according to its oracle, however, is not sufficient to establish its leadership.
To establish leadership, a process needs to complete the synchronization phase (Phase 2)
In the phase description of Zab, and later in the analysis, we use the following notation:
History) Each follower f has a history hf of accepted transactions.
Initial history) The initial history of an epoch e, Ie, is the history of a prospective leader of e at the end of phase 1 of epoch e.
The acknowledgment ACK-E(f.a, hf ) contains the current epoch f.a of the follower and its history.
It is also possible to omit values in acknowledgements and commit messages in Phase 3 to reduce the size of messages.
The following section discusses the Zab protocol in more detail along with some implementation aspects.
In our implementation of Zab, a Zab process can be looking for a leader (ELECTION state), following (FOLLOWING state), or leading (LEADING state)
While in this state the process tries to elect a new leader or become a leader.
If the process finds an elected leader, it moves to the FOLLOWING state and begins to follow the leader.
If the process is elected leader, it moves to the LEADING state and becomes the leader.
Given that a process that leads also follows, states LEADING and FOLLOWING are not exclusive.
A follower transitions to ELECTION if it detects that the leader has failed or relinquished leadership, while a leader transitions to ELECTION once it observes that it no longer has a quorum of followers supporting its leadership.
The basic delivery protocol is similar in spirit to two phase commit [16] without aborts.
Note that a follower does not send the acknowledgment back until it writes the proposal to local stable storage.
When a process receives a commit message for a proposal.
Co-locating the leader and the primary on the same process has practical advantages.
The primary-backup scheme we use requires that at most one process at a time is able to generate updates that can be incorporated into the service state.
A primary propagates state updates using Zab, which in turn requires a leader to initiate proposals.
Leader and primary correspond to different functionality, but they share a common requirement: election.
By co-locating them, we do not need separate elections for primary and leader.
Also important is the fact that calls to broadcast transactions are local when they are co-located.
First, we run a leader election algorithm that outputs a new process as the leader.
We can use any protocol that, with high probability, chooses a process that is up and that a quorum of processes selects.
This property can be fulfilled by an Ω failure detector [17]
Figure 5 shows the events for both the leader and followers when establishing a new leader.
An elected leader does not become established for a given epoch e until it completes Phase 2, in which it successfully achieves consensus on the proposal history and on itself as the leader of e.
We define a established leader and a established epoch as follows:
Established epoch) An epoch e is established if there is an established leader for e.
Once a process determines that it is a prospective leader by inspecting the output of the leader election algorithm, it starts a new iteration in Phase 1
It initially collects the latest epoch of a quorum of followers Q, proposes a later epoch, and collects the latest epoch and highest zxid of each of the followers in Q.
The leader completes Phase 1 once it selects the history from a follower f with latest epoch and highest zxid in a ACK-E(f.a, hf )
These steps are necessary to guarantee that once the prospective leader completes Phase 1, none of the followers in Q accept proposals from earlier epochs.
Given that the history of a follower can be arbitrarily long, it is not efficient to send the entire history in a ACK-E(f.a, hf )
The last zxid of a follower is sufficient for the prospective leader to determine if it needs to copy transactions from any given follower, and only copies missing transactions.
In Phase 2, the leader proposes itself as the leader of the new epoch and sends a NEWLEADER(e, Ie) proposal, which contains the initial history of the new epoch.
As with ACK-E(f.a, hf ), it is not necessary to send the complete initial history, but instead only the transactions missing.
A leader becomes established once it receives the acknowledgments to the new leader proposal from a quorum of followers, at which point it commits the new proposal.
One interesting optimization is to use a leader election primitive that selects a leader that has the latest epoch and has accepted the transaction with highest zxid among a quorum of processes.
Such a leader provides directly the initial history of the new epoch.
A leader proposes operations by queuing them to all connected followers.
To achieve high throughput and low latency, the leader has a steady stream of proposals to the followers.
By the channel properties, we guarantee that followers receive proposals in order.
In our implementation, we use TCP connections to exchange messages between processes.
If a connection to a given follower closes, then the proposals queued to the connection are discarded and the leader considers the corresponding follower down.
Detecting crashes through connections closing was not a suitable choice for us.
Timeout values for a connection can be of minutes or even hours, depending on the operating system configuration and the state of the connection.
To mutually detect crashes in a fine-grained and convenient manner, avoiding operating system reconfiguration, leader and followers exchange periodic heartbeats.
If the leader does not receive heartbeats from a quorum of followers within a timeout interval, the leader renounces leadership of the epoch, and transitions to the ELECTION state.
Once it elects a leader, it starts a new iteration of the algorithm, and starts a new iteration of the protocol proceeding to Phase 1
When a follower emerges from leader election, it connects to the leader.
To support a leader, a follower f acknowledges its new epoch proposal, and it only does so if the new epoch proposed is later than f.p.
A follower only follows one leader at a time and stays connected to a leader as long as it receives heartbeats within a timeout interval.
If there is an interval with no heartbeat or the TCP connection closes, the follower.
Figure 5 shows the protocol a follower executes to support a leader.
The follower sends its current epoch f.a in a current epoch message (CEPOCH(f.a)) to the leader.
The leader sends a new epoch proposal (NEWEPOCH(e)) once it receives a current epoch message from a quorum Q of followers.
The new proposed epoch e must be greater than the current epoch of any follower in Q.
A follower acknowledges the new epoch proposal with its latest epoch and highest zxid, which the leader uses to select the initial history for the new epoch.
In Phase 2, a follower acknowledges the new leader proposal (NEWLEADER(e, Ie)) by setting its f.a value to e and accepting the transactions in the initial history.
Note that once a follower accepts a new epoch proposal for an epoch e, it does not send an acknowledgement for any other new epoch proposal for the same epoch e.
This property guarantees that no two processes can become established leaders for the same epoch e.
In Phase 3, the follower receives new proposals from the leader.
A follower adds new proposals to its history and acknowledges them.
It delivers these proposals when it receives commit messages from the leader.
Note that a follower and a leader follow the recovery protocol both when a new leader is emerging and when a follower connects to an established leader.
If the leader is already established, the NEWLEADER(e, Ie) proposal has already been committed so any acknowledgements for the NEWLEADER(e, Ie) proposal are ignored.
Zab requires the presence of a leader to propose and commit operations.
To sustain leadership, a leader process ` needs to be able to send messages to and receive messages from followers.
A thorough analysis and discussion of liveness requirements, comparing in particular with the work of Malkhi et al., is out of the scope of this work.
In this section, we present an argument for the correctness of Zab.
A more detailed proof appear in a technical report [15]
We present initially a list of definitions followed by a set of invariants that the protocol must satisfy.
Sequence of chosen transactions) Ce is the sequence of chosen transactions in epoch e.
Sequence of chosen proposals broadcast in the broadcast phase) CBe is the sequence of chosen proposals during the broadcast phase of epoch e;
Sequence of transactions delivered) ∆f is the sequence of transactions follower f uniquely delivered, which is the sequence induced by the identifiers of the elements in txns.
Sequence of transactions delivered in the broadcast phase) Df is the sequence of transactions follower f delivered while in the B phase of epoch f.a.
Last committed epoch of a follower) Given a follower f , we use f.e to denote the last epoch e such that f has learned that e has been committed.
The following properties are invariants that the protocol maintains at each step, and that can be verified against the protocol of Section IV in a straightforward manner.
We use them when proving the core properties of Zab.
During the broadcast phase of epoch e, a transactions according to zxid order.
The initial history Ie of an epoch e is the history of some follower.
We now present proof sketches for the properties we introduced in Section III.
Note that we use in some statements the terms follower, leader, and primary, instead of process to better match our definitions and the algorithm description.
Proof sketch: By the algorithm and the properties of channels, only transactions broadcast by primaries are delivered.
Proof sketch: leaders propose different transactions with the same zxid.
By the choice of initial history of epoch(z′) and the definition of a chosen transaction:
By the algorithm, the leader selects the new epoch number e′ to be a number larger than any epoch number received in a CEPOCH(e) from the followers in Q.
We have written our implementation of Zab and the rest of the ZooKeeper server in Java.
Because Zab is not separable from ZooKeeper, we wrote a special benchmark wrapper that hooks into the internals of ZooKeeper to interact directly with Zab.
When Java first starts there is class loading and incremental compilation that takes place that adversely affects the initial runs.
We also allocate files for logging transactions in the initial runs that are reused in later runs.
To avoid these startup effects we run some warmup batches and then run approximately 10 batches sequentially.
We ran our benchmark with 1024-byte operations, which represents a typical operation size.
Throughput of 1K messages as the number of servers increases.
The error bars show the range of the throughput values from the runs.
Figure 6 shows how throughput varies with the number of servers.
One line shows the throughput when nothing is written to disk.
This isolates the performance of just the protocol itself and the network.
Because we have a single gigabit network interface we have a cap on outgoing bandwidth.
We also show the theoretical maximum replication throughput given this cap.
Although we sync requests to disk using the fdatasync system call, this call only forces the request to the disk, and not necessarily to the disk media.
By default, disks have a write cache on the disk itself and acknowledge the write before it is written to the disk media.
In the event of a power failure, writes can be lost if they have not reached the disk media.
As shown in this figure and the next, there is a high price to pay when the disk cache is turned off.
When running with a disk cache, or with a battery backed cache, such as those in raid controllers, the performance with the disk is almost identical to network only and both are saturating the network.
When we turn the disk write cache off, Zab becomes I/O bound and the throughput is roughly constant with the number of servers.
With more than seven servers, throughput decreases with more servers, since the same network-only bottlenecks are present when the transactions are logged to disk.
As we scale the number of servers we saturate the network card of the leader which causes the throughput to decrease as the number of servers increases.
We can use a broadcast tree or chain replication [7] to broadcast the proposals to avoid this saturation, but our performance is much higher than we need in production, so we have not explored these alternatives.
Using ping we measured the basic latency between servers to be 100 microseconds.
Latency of 1K operations as the number of servers increases.
As with throughput, turning off the disk write cache causes a significant performance impact.
We use preallocated files, write sequentially, and use the fdatasync to only sync the data to disk.
When we sync, the performance penalty should be no more than a rotational delay and a seek (around 20 milliseconds)
This extra access time affects both the latency and the throughput.
In the first phase, called read phase, the new leader contacts all other processes to read any possible value that has been proposed by previous leaders and committed.
In the second phase, called write phase, the new leader proposes its own value.
Compared to Paxos, one important difference with Zab is the use of an additional phase: synchronization.
A new prospective leader ` tries to become established by executing a read phase, that we call discovery, followed by a synchronization phase.
During the synchronization phase, the new leader ` makes sure that a quorum of followers delivers chosen transactions proposed by established leaders of prior epochs.
This synchronization phase prevents causal conflicts and ensures that this property is respected.
In fact, it guarantees that all processes in the quorum deliver transactions of prior epochs before transactions of the new epoch e are proposed.
Once the synchronization phase completes, Zab executes a write phase that is similar to the one of Paxos.
An established leader is always associated with an epoch number.
Similar to the ballot number in Paxos, each epoch number can only be associated with a single process.
Such zxids are ordered first by epoch and then by the counter.
After reading accepted histories from each process of a quorum, the new established leader selects a history with the highest zxid among the followers with the latest epoch to replicate across a quorum of followers.
Such a choice is critical to guarantee that the sequence of transactions delivered is consistent with the order of primaries broadcasting over time.
With Paxos, the leader instead selects transactions (or values) for each sequence number independently.
For each instance of consensus, a Zab leader chooses a value that is anchored, it tries to get a quorum of agents (followers) to accept it, and it finishes by recording the value on a quorum of agents.
In Zab, determining which value is anchored for a consensus instance is simple because we grant the right to propose a value for a given consensus instance to exactly one leader, and, by the algorithm, a leader proposes at most one value to each instance.
Consequently, the anchored value is either the single value the leader proposed or no value (no-op)
Zab splits the sequence of consensus instances into epochs, and to the consensus instances of an epoch, only one leader can propose values.
Lampson observes that the Viewstamped replication protocol has a consensus algorithm embedded [19]
The approach proposed by Viewstamped replication combines transaction processing with a view change algorithm [20]
The view change algorithm guarantees that events known to a majority of replicas (or cohorts in their terminology) in a view survive into subsequent views.
Like Zab, the replication algorithm guarantees the order of events proposed within an epoch.
With passive replication, a single process executes clients operations and propagates state changes to the remaining replicas.
Primary-backup is also a special case of Vertical Paxos [21], which is a family of Paxos variants that enable reconfigurations over time and requires fewer acceptors.
Vertical Paxos relies upon a configuration master for configuration changes.
Each configuration is associated to a ballot number, which increases for every new configuration, and the proposer of each configuration uses the corresponding ballot number to propose values.
Vertical Paxos is still Paxos, and each instance of consensus can have multiple values proposed over time under different ballots, thus causing the undesirable behavior for our setting we discuss previously in the Introduction.
Rodrigues and Raynal propose a crash-recovery atomic broadcast protocol using a consensus implementation [3]
To avoid duplicates of delivered messages, they use a call A-deliver-sequence to obtain the sequence of ordered messages.
Mena and Schiper propose to add a commit primitive to the specification of atomic broadcast [4]
Messages that have not been committed can be delivered twice.
With Zab, messages can be delivered twice as long as they respect the order agreed upon.
Boichat and Guerraoui propose a modular and incremental approach to total-order broadcast, and their strongest algorithm corresponds to Paxos [12]
The general idea is to guarantee that all processes observe the same events in the same order.
This guarantee applies not only to message delivery events, but also to failures, recoveries, group membership changes, etc.
Although atomic broadcast is important for virtually synchronous environments, other weaker forms of broadcast, such as causal broadcast, also enable applications to obtain the property of virtual synchrony.
Different from such a programming model, Zab assumes a static ensemble of processes and does not perform view or epoch changes upon failures of processes other than the leader, unless the leader has no quorum supporting it.
Also, different from the ABCAST protocol of Birman and Joseph, Zab uses a sequencer to disseminate messages because it naturally matches the ZooKeeper application.
They present three total order properties: strong total order, weak total order, and reliable total order.
Reliable total order is the strongest property, and guarantees that a prefix of messages totally ordered by a timestamp function are delivered in a view.
Zab properties match more closely this property, with one key difference: each view has at most one process broadcasting.
Having a single process broadcasting in a view simplifies the implementation of the property, since the ordering is established directly by the process broadcasting.
COReL is an atomic broadcast protocol for partitionable environments [24]
It relies upon Transis, a group communication layer [25] and enables processes in a primary component to totally order messages.
Like Zab, upon a configuration change, COReL does not introduce new messages until recovery ends to guarantee a.
COReL, however, assumes that all processes can initiate messages, leading to different ordering guarantees.
Even if we restrict a single process broadcast in a given primary component, we still cannot replace Zab with COReL in our design because of the delivery guarantee with respect to causality.
Causality holds across configurations, leading to executions in which a message broadcast during an earlier configuration is delivered after messages from a later configuration.
In our design, such a behavior causes inconsistencies because the state updates are not commutative.
Two key requirements in our design were efficient recovery upon primary changes and state consistency.
We observed that primary order was a necessary property for guaranteeing correct recovery in our use of primary-backup.
We considered protocols in the literature like Paxos, but even though Paxos is a popular choice for implementing replicated systems, we found that it does not satisfy this property when there are multiple outstanding transactions without batching.
Our implementation of Zab has been able to provide us excellent throughput performance while guaranteeing these properties.
To guarantee primary order despite primary crashes, Zab implements three phases.
One particular phase critical to guarantee that the property is satisfied is synchronization.
Upon a change of primary, a quorum of processes has to execute a synchronization phase before the new primary broadcasts new transactions.
Executing this phase guarantees that all transactions broadcast in previous epochs that have been or will be chosen are in the initial history of transactions of the new epoch.
Zab uses a scheme for assigning transaction identifiers that guarantees at most one value for each identifier.
This scheme enables efficient recovery of primary crashes by allowing correct transaction histories to be chosen by simply comparing the last transaction identifier accepted by a process.
Zab is in production as part of ZooKeeper and has met the demands of our workloads.
The performance of ZooKeeper has been key for its wide adoption.
