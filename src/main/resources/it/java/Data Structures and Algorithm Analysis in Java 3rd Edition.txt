Many of the designations by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Chapter 5 has been extensively revised and enlarged and now contains material on two newer algorithms: cuckoo hashing and hopscotch hashing.
Additionally, a new section on universal hashing has been added.
Chapter 7 now contains material on radix sort, and a new section on lower bound proofs has been added.
Throughout the text, the code has been updated to use the diamond operator from Java 7
Complete versions of the data structures, in both Java and C++, are available on the Internet.
We use similar coding conventions to make the parallels between the two languages more evident.
Overview Chapter 1 contains review material on discrete math and recursion.
I believe the only way to be comfortable with recursion is to see good uses over and over.
Therefore, recursion is prevalent in this text, with examples in every chapter except Chapter 5
Chapter 1 also presents material that serves as a review of inheritance in Java.
Many examples are provided, including an in-depth explanation of logarithmic running time.
Simple recursive programs are analyzed by intuitively converting them into iterative programs.
More complicated divide-and-conquer programs are introduced, but some of the analysis (solving recurrence relations) is implicitly delayed until Chapter 7, where it is performed in detail.
Chapter 5 discusses hash tables, including the classic algorithms such as separate chaining and linear and quadratic probing, as well as several newer algorithms, namely cuckoo hashing and hopscotch hashing.
Universal hashing is also discussed, and extendible hashing is covered at the end of the chapter.
Binary heaps are covered, and there is additional material on some of the theoretically interesting implementations of priority queues.
Algorithms on graphs are interesting, not only because they frequently occur in practice, but also because their running time is so heavily dependent on the proper use of data structures.
Virtually all the standard algorithms are presented along with appropriate data structures, pseudocode, and analysis of running time.
To place these problems in a proper context, a short discussion on complexity theory (including NP-completeness and undecidability) is provided.
References References are placed at the end of each chapter.
Generally the references either are historical, representing the original source of the material, or they represent extensions and improvements to the results given in the text.
Acknowledgments Many, many people have helped me in the preparation of books in this series.
Some are listed in other versions of the book; thanks to all.
See that how a program performs for reasonably large input is just as important as its performance on moderate amounts of input.
One way to solve this problem would be to read the N numbers into an array, sort the array in decreasing order by some simple algorithm such as bubblesort, and then return the element in position k.
Again, there are at least two straightforward algorithms that solve the problem.
For each word in the word list, we check each ordered triple (row, column, orientation) for the presence of the word.
This amounts to lots of nested for loops but is basically straightforward.
Alternatively, for each ordered quadruple (row, column, orientation, number of characters) that doesn’t run off an end of the puzzle, we can test whether the word indicated is in the word list.
It is possible to save some time if the maximum number of characters in any word is known.
It is relatively easy to code up either method of solution and solve many of the real-life puzzles commonly published in magazines.
Suppose, however, we consider the variation where only the puzzle board is given and the word list is essentially an English dictionary.
Both of the solutions proposed require considerable time to solve this problem and therefore are not acceptable.
However, it is possible, even with a large word list, to solve the problem in a matter of seconds.
This section lists some of the basic formulas you need to memorize or be able to derive and reviews basic proof techniques.
Some other useful formulas, which can all be derived in a similar manner, follow.
Another type of common series in analysis is the arithmetic series.
Any such series can be evaluated from the basic formula.
The next two formulas pop up now and then but are fairly uncommon.
The two most common ways of proving statements in data structure analysis are proof by induction and proof by contradiction (and occasionally proof by intimidation, used by professors only)
The best way of proving that a theorem is false is by exhibiting a counterexample.
Most mathematical functions that we are familiar with are described by a simple formula.
For instance, we can convert temperatures from Fahrenheit to Celsius by applying the formula.
Using recursion for numerical calculations is usually a bad idea.
In Chapter 3, the how and why issues are formally resolved.
You must always have some base cases, which can be solved without recursion.
For the cases that are to be solved recursively, the recursive call must always be to a case that makes progress toward a base case.
Printing Out Numbers Suppose we have a positive integer, n, that we wish to print out.
Assume that the only I/O routines available will take a single-digit number and output it to the terminal.
Recursion and Induction Let us prove (somewhat) rigorously that the recursive number-printing program works.
When writing recursive routines, it is crucial to keep in mind the four basic rules of recursion:
You must always have some base cases, which can be solved without recursion.
For the cases that are to be solved recursively, the recursive call must always be to a case that makes progress toward a base case.
Never duplicate work by solving the same instance of a problem in separate recursive calls.
An important goal of object-oriented programming is the support of code reuse.
An important mechanism that supports this goal is the generic mechanism: If the implementation is identical except for the basic type of the object, a generic implementation can be used to describe the basic functionality.
For instance, a method can be written to sort an array of items; the logic is independent of the types of objects being sorted, so a generic method could be used.
Instead, generic programming was implemented using the basic concepts of inheritance.
This section describes how generic methods and classes can be implemented in Java using the basic principles of inheritance.
Direct support for generic methods and classes was announced by Sun in June 2001 as a future language addition.
However, using generic classes requires an understanding of the pre-Java 5 idioms for generic programming.
As a result, an understanding of how inheritance is used to implement generic programs is essential, even in Java 5
The basic idea in Java is that we can implement a generic class by using an appropriate superclass, such as Object.
An example is the MemoryCell class shown in Figure 1.5
A second important detail is that primitive types cannot be used.
When we implement algorithms, often we run into a language typing problem: We have an object of one type, but the language syntax requires an object of a different type.
This technique illustrates the basic theme of a wrapper class.
One typical use is to store a primitive type, and add operations that the primitive type either does not support or does not support correctly.
In Java, we have already seen that although every reference type is compatible with Object, the eight primitive types are not.
As a result, Java provides a wrapper class for each of the eight primitive types.
For instance, the wrapper for the int type is Integer.
Each wrapper object is immutable (meaning its state can never change), stores one primitive value that is set when the object is constructed, and provides a method to retrieve the value.
The wrapper classes also contain a host of static utility methods.
As an example, Figure 1.7 shows how we can use the MemoryCell to store integers.
Using Object as a generic type works only if the operations that are being performed can be expressed using only methods available in the Object class.
First, only objects that implement the Comparable interface can be passed as elements of the Comparable array.
Objects that have a compareTo method but do not declare that they implement Comparable are not Comparable, and do not have the requisite IS-A relationship.
It is also implicit in the test program that Circle, Square, and Rectangle are subclasses of Shape.
Second, if the Comparable array were to have two objects that are incompatible (e.g., a String and a Shape), the compareTo method would throw a ClassCastException.
Third, as before, primitives cannot be passed as Comparables, but the wrappers work because they implement the Comparable interface.
Fourth, it is not required that the interface be a standard library interface.
Finally, this solution does not always work, because it might be impossible to declare.
Both assignments compile, yet arr[0] is actually referencing an Employee, and Student IS-NOT-A Employee.
The runtime system cannot throw a ClassCastException since there is no cast.
The easiest way to avoid this problem is to specify that the arrays are not typecompatible.
Each array keeps track of the type of object it is allowed to store.
Java 5 supports generic classes that are very easy to use.
In this section, we illustrate the basics of how generic classes and methods are written.
We do not attempt to cover all the constructs of the language, which are quite complex and sometimes tricky.
Instead, we show the syntax and idioms that are used throughout this book.
Here, we have changed the name to GenericMemoryCell because neither class is in a package and thus the names cannot be the same.
For example, prior to Java 5 the Comparable interface was not generic, and its compareTo method took an Object as the parameter.
As a result, any reference variable passed to the compareTo method would compile, even if the variable was not a sensible type, and only at runtime would the error be reported as a ClassCastException.
The String class, for instance, now implements Comparable<String> and has a compareTo method that takes a String as a parameter.
By making the class generic, many of the errors that were previously only reported at runtime become compile-time errors.
Figure 1.12 shows a static method that computes the total area in an array of Shapes (we assume Shape is a class with an area method; Circle and Square extend Shape)
Suppose we want to rewrite the method so that it works with a parameter that is Collection<Shape>
Collection is described in Chapter 3; for now, the only important thing about it is that it stores a collection of items that can be accessed with an enhanced for loop.
Because of the enhanced for loop, the code should be identical, and the resulting code is shown in Figure 1.13
Recall from Section 1.4.4 that the technical term for this is whether we have covariance.
In Java, as we mentioned in Section 1.4.4, arrays are covariant.
On the one hand, consistency would suggest that if arrays are covariant, then collections should be covariant too.
Because the entire reason to have generics is to generate compiler.
The type is used in more than one parameter type.
If so, then an explicit generic method with type parameters must be declared.
For instance, Figure 1.15 illustrates a generic static method that performs a sequential.
By using a generic method instead of a nongeneric method that uses Object as the parameter types, we can get compile-time errors if searching for an Apple in an array of Shapes.
The generic method looks much like the generic class in that the type parameter list uses the same syntax.
The type parameters in a generic method precede the return type.
This code cannot work because the compiler cannot prove that the call to compareTo at line 6 is valid; compareTo is guaranteed to exist only if AnyType is Comparable.
As a result, what we need to say is that AnyType IS-A Comparable<T> where T is a superclass of AnyType.
Since we do not need to know the exact type T, we can use a wildcard.
The compiler will accept arrays of types T only such that T implements the Comparable<S> interface, where T IS-A S.
Fortunately, we won’t see anything more complicated than this idiom.
If a generic class is used without a type parameter, the raw class is used.
Every one of the restrictions listed here is required because of type erasure.
Primitive Types Primitive types cannot be used for a type parameter.
Eventually, a runtime error results at the last line because the call to read tries to return a String but cannot.
As a result, the typecast will generate a warning, and a corresponding instanceof test is illegal.
Instantiation of Generic Types It is illegal to create an instance of a generic type.
Generic Array Objects It is illegal to create an array of a generic type.
Because we cannot create arrays of generic objects, generally we must create an array of the erased type and then use a typecast.
This typecast will generate a compiler warning about an unchecked type conversion.
Arrays of Parameterized Types Instantiation of arrays of parameterized types is illegal.
Thus, this code has no casts, yet it will eventually generate a ClassCastException at line 5, which is exactly the situation that generics are supposed to avoid.
The solution in these situations is to rewrite findMax to accept two parameters: an array of objects and a comparison function that explains how to decide which of two objects is the larger and which is the smaller.
In effect, the objects no longer know how to compare themselves; instead, this information is completely decoupled from the objects in the array.
Figure 1.18 Using a function object as a second parameter to findMax; output is ZEBRA.
In effect, a function is being passed by placing it inside an object.
This chapter sets the stage for the rest of the book.
The time taken by an algorithm confronted with large amounts of input will be an important criterion for deciding if it is a good algorithm.
What is fast for one problem on one machine might be slow for another problem or a different machine.
We will begin to address these issues in the next chapter and will use the mathematics discussed here to establish a formal model.
Draw a table showing the running time of your program for various values of N.
Provide public methods isEmpty, makeEmpty, insert, remove, findMin, and findMax.
The material in this chapter is meant to serve as an overview of the features that we will use in this text.
We also assume familiarity with recursion (the recursion summary in this chapter is meant to be a quick review)
We will attempt to provide hints on its use where appropriate throughout the textbook.
Readers not familiar with recursion should consult [14] or any good intermediate programming textbook.
When we apply this to the analysis of algorithms, we shall see why this is the important measure.
First, it is very bad style to include constants or low-order.
This means that in any analysis that will require a Big-Oh answer, all sorts of shortcuts are possible.
Lower-order terms can generally be ignored, and constants can be thrown away.
Usually the relation between f(N) and g(N) can be derived by simple algebra.
This is like determining which of log2 N or N grows faster.
This is a simple problem, because it is already known that N grows faster than any power of a log.
The most important resource to analyze is generally the running time.
Some, such as the compiler and computer used, are obviously beyond the scope of any theoretical model, so, although they are important, we cannot deal with them here.
The other main factors are the algorithm used and the input to the algorithm.
As an example, in the next section, we shall consider the following problem:
For convenience, the maximum subsequence sum is 0 if all the integers are negative.) Example:
The running time on some computer (the exact computer is unimportant) for these algorithms is given in Figure 2.2
Figure 2.2 Running times of several algorithms for maximum subsequence sum (in seconds)
Notice that algorithm 4, which is linear, exhibits the nice behavior that as the problem size increases by a factor of ten, the running time also increases by a factor of ten.
There are several ways to estimate the running time of a program.
If two programs are expected to take similar times, probably the best way to decide which is faster is to code them both up and run them!
To simplify the analysis, we will adopt the convention that there are no particular units of time.
We will also throw away low-order terms, so what we are essentially doing is computing a Big-Oh running time.
Since Big-Oh is an upper bound, we must be careful never to underestimate the running time of the program.
In effect, the answer provided is a guarantee that the program will terminate within a certain time period.
The program may stop earlier than this, but never later.
The running time of a for loop is at most the running time of the statements inside the for loop (including tests) times the number of iterations.
The total running time of a statement inside a group of nested loops is the running time of the statement multiplied by the product of the sizes of all the loops.
Clearly, this can be an overestimate in some cases, but it is never an underestimate.
Other rules are obvious, but a basic strategy of analyzing from the inside (or deepest.
Convince yourself that this algorithm works (this should not take much convincing)
There is a recursive and relatively complicated O(N logN) solution to this problem, which we now describe.
If there didn’t happen to be an O(N) (linear) solution, this would be an excellent example of the power of recursion.
The idea is to split the problem into two roughly equal subproblems, which are then solved recursively.
The “conquer” stage consists of patching together the two solutions of the subproblems, and possibly doing a small amount of additional work, to arrive at a solution for the whole problem.
We see, then, that among the three ways to form a large maximum subsequence, for our example, the best way is to include elements from both halves.
The general form of the call for the recursive method is to pass the input array along with the left and right borders, which.
If left == right, there is one element, and it is the maximum subsequence if the element is nonnegative.
The case left > right is not possible unless N is negative (although minor perturbations in the code could mess this up)
We can see that the recursive calls are always on a smaller problem than the original, although minor perturbations in the code could destroy this property.
The sum of these two values is the maximum sum that spans both halves.
The routine max3 (not shown) returns the largest of the three possibilities.
Algorithm 3 clearly requires more effort to code than either of the two previous algorithms.
As we have seen in the earlier table showing the running times of the algorithms, this algorithm is considerably faster than the other two for all but the smallest of input sizes.
The running time is analyzed in much the same way as for the program that computes the Fibonacci numbers.
Let T(N) be the time it takes to solve a maximum subsequence sum problem of size N.
These lines solve two subsequence problems of size N/2 (assuming N is even)
This algorithm is typical of many clever algorithms: The running time is obvious, but the correctness is not.
For these algorithms, formal correctness proofs (more formal than the sketch above) are almost always required; even then, however, many people still are not convinced.
In addition, many of these algorithms require trickier programming, leading to longer development.
But when these algorithms work, they run quickly, and we can.
An extra advantage of this algorithm is that it makes only one pass through the data, and once a[i] is read and processed, it does not need to be remembered.
Thus, if the array is on a disk or is being transmitted over the Internet, it can be read sequentially, and there is no need to store any part of it in main memory.
Furthermore, at any point in time, the algorithm can correctly give an answer to the subsequence problem for the data it has already read (the other algorithms do not share this property)
An online algorithm that requires only constant space and runs in linear time is just about as good as possible.
The most confusing aspect of analyzing algorithms probably centers around the logarithm.
We have already seen that some divide-and-conquer algorithms will run in O(N logN) time.
On the other hand, if constant time is required to merely reduce the problem by a constant amount (such as to make the problem smaller by 1), then the algorithm is O(N)
The algorithm works by continually computing remainders until 0 is reached.
As before, estimating the entire running time of the algorithm depends on determining how long the sequence of remainders is.
Indeed, the remainder does not decrease by a constant factor in one iteration.
However, we can prove that after two iterations, the remainder is at most half of its original value.
Exponentiation Our last example in this section deals with raising an integer to a power (which is also an integer)
Numbers that result from exponentiation are generally quite large, so an analysis works only if we can assume that we have a machine that can store such large integers.
We will count the number of multiplications as the measurement of running time.
For instance, to compute X62, the algorithm does the following calculations, which involve only nine multiplications:
The number of multiplications required is clearly at most 2 logN, because at most two multiplications (if N is odd) are required to halve the problem.
Indeed, the program will still run in O(logN), because the sequence of multiplications is the same as before.
However, all of the following alternatives for line 8 are bad, even though they look correct:
Figure 2.11 to use BigInteger instead of long is straightforward.
Give an analysis of the running time (Big-Oh will do)
Implement the code in Java, and give the running time for several values of N.
Prove that all three algorithms generate only legal permutations and that all permutations are equally likely.
Give as accurate (Big-Oh) an analysis as you can of the expected running time of each algorithm.
Write (separate) programs to execute each algorithm 10 times, to get a good average.
Interpolate the running times for these algorithms and estimate the time required to compute the maximum subsequence sum of 1 million numbers.
Modify them so that they return in a single object the value of the maximum subsequence and the indices of the actual sequence.
Write a program to determine if a positive integer, N, is prime.
In terms of N, what is the worst-case running time of your program? (You should.
Let B equal the number of bits in the binary representation of N.
In terms of B, what is the worst-case running time of your program? e.
Which program has the better guarantee on the running time, for large values of.
Which program has the better guarantee on the running time, for small values.
Is it possible that program B will run faster than program A on all possible.
If there is no majority element, your program should indicate this.
Here is a sketch of an algorithm to solve the problem:
Give an O(N) worst-case algorithm that decides if a number X is in the matrix.
The Java class allows for the implementation of ADTs, with appropriate hiding of implementation details.
Thus any other part of the program that needs to perform an operation on the ADT can do so by calling the appropriate method.
If for some reason implementation details need to be changed, it should be easy to do so by merely changing the routines that perform the ADT operations.
This change, in a perfect world, would be completely transparent to the rest of the program.
There is no rule telling us which operations must be supported for each ADT; this is a design decision.
Error handling and tie breaking (where appropriate) are also generally up to the program designer.
The three data structures that we will study in this chapter are.
We will see how each can be implemented in several ways, but if they are done correctly, the programs that use them will not necessarily need to know which implementation was used.
We could also add operations such as next and previous, which would take a position as argument and return the position of the successor and predecessor, respectively.
There are many situations where the list is built up by insertions at the high end, and then only array accesses (i.e., findKth operations) occur.
In such a case, the array is a suitable implementation.
However, if insertions and deletions occur throughout the list, and in particular, at the front of the list, then the array is not a good option.
The next subsection deals with the alternative: the linked list.
In order to avoid the linear cost of insertion and deletion, we need to ensure that the list is not stored contiguously, since otherwise entire parts of the list will need to be moved.
Figure 3.1 shows the general idea of a linked list.
The linked list consists of a series of nodes, which are not necessarily adjacent in memory.
Each node contains the element and a link to a node containing its successor.
The remove method can be executed in one next reference change.
Figure 3.2 shows the result of deleting the third element in the original list.
The insert method requires obtaining a new node from the system by using a new call and then executing two reference maneuvers.
As we can see, in principle, if we know where a change is to be made, inserting or removing an item from a linked list does not require moving lots of items and instead involves only a constant number of changes to node links.
The obvious idea of maintaining a third link to the next-to-last node doesn’t work, because it too would need to be updated during a remove.
Instead, we have every node maintain a link to its previous node in the list.
This is shown in Figure 3.4 and is known as a doubly linked list.
The Java language includes, in its library, an implementation of common data structures.
This part of the language is popularly known as the Collections API.
The List ADT is one of the data structures implemented in the Collections API.
The notion of a collection, which stores a collection of identically typed objects, is abstracted in the Collection interface.
Figure 3.5 shows the most important parts of this interface (some methods are not shown)
Many of the methods in the Collection interface do the obvious things that their names suggest.
So size returns the number of items in the collection; isEmpty returns true if and only if the size of the collection is zero.
Note that the interface doesn’t specify how the collection decides if x is in the collection—this is determined by the actual classes that implement the Collection interface.
For instance, a remove can fail if the item is not present in the collection, and if the particular collection does not allow duplicates, then add can fail when an attempt is made to insert a duplicate.
Classes that implement the Iterable interface can have the enhanced for loop used on them to view all their items.
For instance, the routine in Figure 3.6 can be used to print all the items in any collection.
Figure 3.5 Subset of the Collection interface in package java.util.
Figure 3.6 Using the enhanced for loop on an Iterable type.
The idea of the Iterator is that via the iterator method, each collection can create, and return to the client, an object that implements the Iterator interface and stores internally its notion of a current position.
Because of the limited set of methods available in the Iterator interface, it is hard to use the Iterator for anything more than a simple traversal through the Collection.
With this method you can remove the last item returned by next (after which you cannot call remove again until after another.
Figure 3.8 The enhanced for loop on an Iterable type rewritten by the compiler to use an iterator.
Although the Collection interface also contains a remove method, there are presumably advantages to using the Iterator’s remove method instead.
This is necessary to avoid ugly situations in which the iterator is prepared to give a certain item as the next item, and then that item is either removed, or perhaps a new item is inserted just prior to the next item.
This means that you shouldn’t obtain an iterator until immediately prior to the need to use it.
However, if the iterator invokes its remove method, then the iterator is still valid.
This is a second reason to prefer the iterator’s remove method sometimes.
Figure 3.9 Subset of the List interface in package java.util.
Regardless of whether an ArrayList or LinkedList is passed as a parameter, the running time of makeList1 is O(N) because each call to add, being at the end of the list, takes constant time (the occasional expansion of the ArrayList is safe to ignore)
On the other hand, if we construct a List by adding items at the front,
The next routine attempts to compute the sum of the numbers in a List:
As an example, we provide a routine that removes all even-valued items in a list.
There are several possible ideas for an algorithm that deletes items from the list as they are encountered.
Of course, one idea is to construct a new list containing all the odd numbers, and then clear the original list and copy the odd numbers back into it.
But we are more interested in writing a clean version that avoids making a copy and instead removes items from the list as they are encountered.
Figure 3.10 Removes the even numbers in a list; quadratic on all types of lists.
Figure 3.12 Removes the even numbers in a list; quadratic on ArrayList, but linear time for LinkedList.
Figure 3.13 Subset of the ListIterator interface in package java.util.
Figure 3.13 shows that a ListIterator extends the functionality of an Iterator for Lists.
The notion of the current position is abstracted by viewing the iterator as being between the item that would be given by a call to next and the item that would be given by a call to previous, an abstraction that is illustrated in Figure 3.14
As an example, it can be used to subtract 1 from all the even numbers in a List, which would be hard to do on a LinkedList without using the ListIterator’s set method.
In this section, we provide the implementation of a usable ArrayList generic class.
To avoid ambiguities with the library class, we will name our class MyArrayList.
We do not provide a MyCollection or MyList interface; rather, MyArrayList is standalone.
Before examining the (nearly one hundred lines of) MyArrayList code, we outline the main details.
The MyArrayList will maintain the underlying array, the array capacity, and the current number of items stored in the MyArrayList.
The MyArrayList will provide a mechanism to change the capacity of the underlying array.
The capacity is changed by obtaining a new array, copying the old array into the new array, and allowing the Virtual Machine to reclaim the old array.
The MyArrayList will provide an implementation of get and set.
The MyArrayList will provide basic routines, such as size, isEmpty, and clear, which are typically one-liners; a version of remove; and also two versions of add.
The add routines will increase capacity if the size and capacity are the same.
The MyArrayList will provide a class that implements the Iterator interface.
This class will store the index of the next item in the iteration sequence and provide implementations of next, hasNext, and remove.
The MyArrayList’s iterator method simply returns a newly constructed instance of the class that implements the Iterator interface.
As shown on lines 5–6, the MyArrayList stores the size and array as its data members.
A host of short routines, namely clear, doClear (used to avoid having the constructor.
The remaining routine deals with the iterator method and the implementation of the associated iterator class.
The ArrayListIterator stores the notion of a current position, and provides implementations of hasNext, next, and remove.
The current position represents the (array index of the) next element that is to be viewed, so initially the current position is 0
The ArrayListIterator class uses a tricky Java construct known as the inner class.
Clearly the class is declared inside of the MyArrayList class, a feature that is supported by many languages.
However, an inner class in Java has a more subtle property.
It doesn’t work because theItems and size() are not part of the ArrayListIterator class.
It doesn’t work because theItems is private in the MyArrayList class.
Instead, Figure 3.19 shows a solution that works: Make the ArrayListIterator class a nested class.
When we make ArrayListIterator a nested class, it is placed inside of another class (in this case MyArrayList) which is the outer class.
We must use the word static to signify that it is nested; without static we will get an inner class, which is sometimes good and sometimes bad.
The nested class is the type of class that is typical of many programming languages.
Observe that the nested class can be made private, which is nice.
It works because the nested class is considered part of the MyArrayList class.
More importantly, because the nested class is considered to be part of the outer class, there are no visibility issues that arise: theItems is a visible member of class MyArrayList, because next is part of MyArrayList.
Now that we have a nested class, we can discuss the inner class.
The problem with the nested class is that in our original design, when we wrote theItems without referring to MyArrayList that it was contained in, the code looked nice, and kind of made sense, but was illegal because it was impossible for the compiler to deduce which MyArrayList was being referred to.
It would be nice not to have to keep track of this ourselves.
This is exactly what an inner class does for you.
When you declare an inner class, the compiler adds an implicit reference to the outer class object that caused the inner class object’s construction.
If the name of the outer class is Outer, then the implicit reference is Outer.this.
Thus if ArrayListIterator is declared as an inner class, without the static, then MyArrayList.this and theList would both be referencing the same MyArrayList.
The inner class is useful in a situation in which each inner class object is associated with exactly one instance of an outer class object.
In such a case, the inner class object can never exist without having an outer class object with which to be associated.
In the case of the MyArrayList and its iterator, Figure 3.20 shows the relationship between the iterator class and MyArrayList class, when inner classes are used to implement the iterator.
First, the ArrayListIterator is implicitly generic, since it is now tied to MyArrayList, which is generic; we don’t have to say so.
The removal of theList as a data member also removes the associated constructor, so the code reverts back to the style in Version #1
They are not needed to write any Java code, but their presence in the language allows the Java programmer to write code in the style that was natural (like Version #1), with the compiler writing the extra code required to associate the inner class object with the outer class object.
In this section, we provide the implementation of a usable LinkedList generic class.
As in the case of the ArrayList class, our list class will be named MyLinkedList to avoid ambiguities with the library class.
In considering the design, we will need to provide three classes:
The MyLinkedList class itself, which contains links to both ends, the size of the list, and a host of methods.
The Node class, which is likely to be a private nested class.
A node contains the data and links to the previous and next nodes, along with appropriate constructors.
The LinkedListIterator class, which abstracts the notion of a position and is a private inner class, implementing the Iterator interface.
Figure 3.22 A doubly linked list with header and tail nodes.
Figure 3.23 An empty doubly linked list with header and tail nodes.
We can see at line 3 the beginning of the declaration of the private nested Node class.
Figure 3.25 shows the Node class, consisting of the stored item, links to the previous and next Node, and a constructor.
Recall that in a class, the data members are normally private.
However, members in a nested class are visible even in the outer class.
Since the Node class is private, the visibility of the data members in the Node class is irrelevant; the MyLinkedList methods can see all Node data members, and classes outside of MyLinkedList cannot see the Node class at all.
The rest of the MyLinkedList class consists of the constructor, the implementation of the iterator, and a host of methods.
Figure 3.26 clear routine for MyLinkedList class, which invokes private doClear.
The doClear method in Figure 3.26 is invoked by the constructor.
It creates and connects the header and tail nodes and then sets the size to 0
We’ll discuss those details when we see the actual implementations later.
Figure 3.27 illustrates how a new node containing x is spliced in between a node referenced by p and p.prev.
The assignment to the node links can be described as follows:
But then these two lines can also be combined, yielding:
Figure 3.27 Insertion in a doubly linked list by getting new node and then changing pointers in the order indicated.
Figure 3.30 shows the basic private remove routine that contains the two lines of code shown above.
It is mostly error checking (which is why we avoided the error checks in the ArrayListIterator)
The actual remove at line 30 mimics the logic in the ArrayListIterator.
But here, current remains unchanged, because the node that current is viewing is unaffected by the removal of the prior node (in the ArrayListIterator, items shifted, requiring an update of current)
A stack is a list with the restriction that insertions and deletions can be performed in only one position, namely, the end of the list, called the top.
The fundamental operations on a stack are push, which is equivalent to an insert, and pop, which deletes the most recently.
Figure 3.33 Stack model: input to a stack is by push, output is by pop and top.
Figure 3.34 Stack model: Only the top element is accessible.
The most recently inserted element can be examined prior to performing a pop by use of the top routine.
A pop or top on an empty stack is generally considered an error in the stack ADT.
On the other hand, running out of space when performing a push is an implementation limit but not an ADT error.
The general model is that there is some element that is at the top of the stack, and it is the only element that is visible.
Since a stack is a list, any list implementation will do.
Clearly ArrayList and LinkedList support stack operations; 99% of the time they are the most reasonable choice.
Occasionally it can be faster to design a special-purpose implementation (for instance, if the items being placed on the stack are a primitive type)
Because stack operations are constant-time operations, this is unlikely to yield any discernable improvement except under very unique circumstances.
For these special times, we will give two popular implementations.
Notice that these operations are performed in not only constant time, but very fast constant time.
On some machines, pushes and pops (of integers) can be written in one machine instruction, operating on a register with auto-increment and auto-decrement addressing.
The fact that most modern machines have stack operations as part of the instruction set enforces the idea that the stack is probably the most fundamental data structure in computer science, after the array.
It should come as no surprise that if we restrict the operations allowed on a list, those operations can be performed very quickly.
The big surprise, however, is that the small number of operations left are so powerful and important.
The third application gives a deep insight into how programs are organized.
Balancing Symbols Compilers check your programs for syntax errors, but frequently a lack of one symbol (such as a missing brace or comment starter) will cause the compiler to spill out a hundred lines of diagnostics without identifying the real error.
A useful tool in this situation is a program that checks whether everything is balanced.
Thus, every right brace, bracket, and parenthesis must correspond to its left counterpart.
Obviously, it is not worthwhile writing a huge program for this, but it turns out that it is easy to check these things.
For simplicity, we will just check for balancing of parentheses, brackets, and braces and ignore any other character that appears.
The simple algorithm uses a stack and is as follows:
You should be able to convince yourself that this algorithm works.
It is clearly linear and actually makes only one pass through the input.
Extra work can be done to attempt to decide what to do when an error is reported—such as identifying the likely cause.
The correct thing to do is to place operators that have been seen, but not placed on the output, onto the stack.
We will also stack left parentheses when they are encountered.
If we see a right parenthesis, then we pop the stack, writing symbols until we encounter a (corresponding) left parenthesis, which is popped but not output.
Finally, if we read the end of input, we pop the stack until it is empty, writing symbols onto the output.
The idea of this algorithm is that when an operator is seen, it is placed on the stack.
However, some of the operators on the stack that have high precedence are now known to be completed and should be popped, as they will no longer be pending.
Thus prior to placing the operator on the stack, operators that are on the stack and are to be completed prior to the current operator, are popped.
We can view a left parenthesis as a high-precedence operator when it is an input symbol (so that pending operators remain pending), and a low-precedence operator when it is on the stack (so that it is not accidentally removed by an operator)
The next symbol read is a (, which, being of highest precedence, is placed on the stack.
Since open parentheses do not get removed except when a closed parenthesis is being processed, there is no output.
Now we read a ), so the stack is emptied back to the (
We read a * next; it is pushed onto the stack.
The input is now empty, so we pop and output symbols from the stack until it is empty.
As before, this conversion requires only O(N) time and works in one pass through the input.
We can add subtraction and division to this repertoire by assigning subtraction and addition equal priority and multiplication and division equal priority.
A subtle point is that the expression a-b-c will be converted to ab-c- and not abc--
Our algorithm does the right thing, because these operators associate from left to right.
When there is a method call, all the important information that needs to be saved, such as register values (corresponding to variable names) and the return address (which can be obtained from the program counter, which is typically in a register), is saved “on a piece of paper” in an abstract way and put at the top of a pile.
Then the control is transferred to the new method, which is free to replace the registers with its values.
If it makes other method calls, it follows the same procedure.
When the method wants to return, it looks at the “paper” at the top of the pile and restores all the registers.
Since Java is interpreted, rather than compiled, some details in this section may not apply to Java, but the general concepts still do in Java and many other languages.
Figure 3.35 A bad use of recursion: printing a linked list.
Figure 3.36 Printing a list without recursion; a compiler might do this.
With a queue, however, insertion is done at one end, whereas deletion is performed at the other end.
The basic operations on a queue are enqueue, which inserts an element at the end of the list (called the rear), and dequeue, which deletes (and returns) the element at the start of the list (known as the front)
As with stacks, any list implementation is legal for queues.
Like stacks, both the linked list and array implementations give fast O(1) running times for every operation.
The linked list implementation is straightforward and left as an exercise.
For each queue data structure, we keep an array, theArray, and the positions front and back, which represent the ends of the queue.
To enqueue an element x, we increment currentSize and back, then set theArray[back]=x.
To dequeue an element, we set the return value to theArray[front], decrement currentSize, and then increment front.
After 10 enqueues, the queue appears to be full, since back is now at the last array index, and the next enqueue would be in a nonexistent position.
However, there might only be a few elements in the queue, because several elements may have already been dequeued.
Queues, like stacks, frequently stay small even in the presence of a lot of operations.
When jobs are submitted to a printer, they are arranged in order of arrival.
Thus, essentially, jobs sent to a line printer are placed on a queue.2
A whole branch of mathematics, known as queuing theory, deals with computing, probabilistically, how long users expect to wait on a line, how long the line gets, and other such questions.
The answer depends on how frequently users arrive to the line and how long it takes to process a user once the user is served.
Both of these parameters are given as probability distribution functions.
An example of an easy case would be a phone line with one operator.
If the operator is busy, callers are placed on a waiting line (up to some maximum limit)
This problem is important for businesses, because studies have shown that people are quick to hang up the phone.
Additional uses for queues abound, and as with stacks, it is staggering that such a simple data structure can be so important.
This chapter describes the concept of ADTs and illustrates the concept with three of the most common abstract data types.
The primary objective is to separate the implementation of the abstract data types from their function.
The program must know what the operations do, but it is actually better off not knowing how it is done.
Lists, stacks, and queues are perhaps the three fundamental data structures in all of computer science, and their use is documented through a host of examples.
In particular, we saw how stacks are used to keep track of method calls and how recursion is actually implemented.
This is important to understand, not just because it makes procedural languages possible, but because knowing how recursion is implemented removes a good deal of the mystery that surrounds its use.
Although recursion is very powerful, it is not an entirely free operation; misuse and abuse of recursion can result in programs crashing.
After M passes, the person holding the hot potato is eliminated, the circle closes ranks, and the game continues with the person who was sitting after the eliminated person picking up the hot potato.
Why is theSize saved prior to entering the for loop? b.
What is the running time of removeFirstHalf if lst is an ArrayList? c.
What is the running time of removeFirstHalf if lst is a LinkedList? d.
Does using an iterator make removeHalf faster for either type of List?
The ListIterator interface in java.util has more methods than are shown in Section 3.3.5
Notice that you will write a listIterator method to return a newly constructed ListIterator, and further, that the existing iterator method can return a newly constructed ListIterator.
Thus you will change ArrayListIterator so that it implements ListIterator instead of Iterator.
Then you could print a MyArrayList L in reverse by using the code.
Write routines to support the deque that take O(1) time per operation.
This instruction implies that you cannot use recursion, but you may assume that your algorithm is a list member function.
In a self-adjusting list, all insertions are performed at the front.
A self-adjusting list adds a find operation, and when an element is accessed by a find, it is moved to the front of the list without changing the relative order of the other items.
We do not have references to any other nodes (except by following links)
Describe an O(1) algorithm that logically removes the value stored in such a node from the linked list, maintaining the integrity of the linked list.
Insert item x before position p (given by an iterator)
Remove the item stored at position p (given by an iterator)
The data structure that we are referring to is known as a binary search tree.
The binary search tree is the basis for the implementation of two library collections classes, TreeSet and TreeMap, which are used in many applications.
Trees in general are very useful abstractions in computer science, so we will discuss their use in other, more general applications.
For any node ni, the depth of ni is the length of the unique path from the root to ni.
The height of ni is the length of the longest path from ni to a leaf.
The height of a tree is equal to the height of the root.
The depth of a tree is equal to the depth of the deepest leaf; this is always equal to the height of the tree.
One way to implement a tree would be to have in each node, besides its data, a link to each child of the node.
However, since the number of children per node can vary so greatly and is not known in advance, it might be infeasible to make the children direct links in the data.
The solution is simple: Keep the children of each node in a linked list of tree nodes.
Figure 4.4 shows how a tree might be represented in this implementation.
Null links are not drawn, because there are too many.
In the tree of Figure 4.4, node E has both a link to a sibling (F) and a link to a child (I), while some nodes have neither.
The heart of the algorithm is the recursive method listAll.
This routine needs to be started with a depth of 0, to signify no indenting for the root.
This depth is an internal bookkeeping variable and is hardly a parameter that a calling routine should be expected to know about.
Thus the driver routine is used to interface the recursive routine to the outside world.
Figure 4.9 Pseudocode to calculate the size of a directory.
A binary tree is a tree in which no node can have more than two children.
Figure 4.11 shows that a binary tree consists of a root and two subtrees, TL and TR, both of which could possibly be empty.
A property of a binary tree that is sometimes important is that the depth of an average binary tree is considerably smaller than N.
N), and that for a special type of binary tree, namely the binary search tree, the average.
Because a binary tree node has at most two children, we can keep direct links to them.
The declaration of tree nodes is similar in structure to that for doubly linked lists in that a node is a structure consisting of the element information plus two references (left and right) to other nodes (see Figure 4.13)
We could draw the binary trees using the rectangular boxes that are customary for linked lists, but trees are generally drawn as circles connected by lines, because they are.
Friendly data; accessible by other package routines Object element; // The data in the node BinaryNode left; // Left child BinaryNode right; // Right child.
Binary trees have many important uses not associated with searching.
One of the principal uses of binary trees is in the area of compiler design, which we will now explore.
The leaves of an expression tree are operands, such as constants or variable names, and the other nodes contain operators.
This particular tree happens to be binary, because all the operators are binary, and although this is the simplest case, it is possible for nodes to have more than two children.
It is also possible for a node to have only one child, as is the case with the unary minus operator.
We can evaluate an expression tree, T, by applying the operator at the root to the values obtained by recursively evaluating the left and right subtrees.
Next, a + is read, so two trees are popped, a new tree is formed, and it is pushed onto the stack.
Next, c, d, and e are read, and for each a one-node tree is created and the corresponding tree is pushed onto the stack.
For convenience, we will have the stack grow from left to right in the diagrams.
Now a + is read, so two trees are merged.
An important application of binary trees is their use in searching.
Let us assume that each node in the tree stores an item.
In our examples, we will assume for simplicity that these are integers, although arbitrarily complex items are easily handled in Java.
We will also assume that all the items are distinct and deal with duplicates later.
The property that makes a binary tree into a binary search tree is that for every node, X, in the tree, the values of all the items in its left subtree are smaller than the item in X, and the values of all the items in its right subtree are larger than the item in X.
Notice that this implies that all the elements in the tree can be ordered in some consistent manner.
In Figure 4.15, the tree on the left is a binary search tree, but the tree on the right is not.
Figure 4.15 Two binary trees (only the left tree is a search tree)
This operation requires returning true if there is a node in tree T that has item X, or false if there is no such node.
If T is empty, then we can just return false.
Otherwise, if the item stored at T is X, we can return true.
Otherwise, we make a recursive call on a subtree of T, either left or right, depending on the relationship of X to the item stored in T.
The code in Figure 4.18 is an implementation of this strategy.
These private routines return a reference to the node containing the smallest and largest elements in the tree, respectively.
To perform a findMin, start at the root and go left as long as there is a left child.
The findMax routine is the same, except that branching is to the right child.
This is so easy that many programmers do not bother using recursion.
We will code the routines both ways by doing findMin recursively and findMax nonrecursively (see Figure 4.20)
Notice how we carefully handle the degenerate case of an empty tree.
Although this is always important to do, it is especially crucial in recursive programs.
Also notice that it is safe to change t in findMax, since we are only working with a copy of a reference.
Always be extremely careful, however, because a statement such as t.right = t.right.right will make changes.
Figure 4.19 Illustrates use of a function object to implement binary search tree.
To insert X into tree T, proceed down the tree as you would with a contains.
Otherwise, insert X at the last spot on the path traversed.
Figure 4.20 Recursive implementation of findMin and nonrecursive implementation of findMax for binary search trees.
To insert 5, we traverse the tree as though a contains were occurring.
As is common with many data structures, the hardest operation is deletion.
Once we have found the node to be deleted, we need to consider several possibilities.
If the node is a leaf, it can be deleted immediately.
If the node has one child, the node can be deleted after its parent adjusts a link to bypass the node (we will draw the link directions explicitly for clarity)
The complicated case deals with a node with two children.
The general strategy is to replace the data of this node with the smallest data of the right subtree (which is easily found) and recursively delete that node (which is now empty)
Because the smallest node in the right subtree cannot have a left child, the second remove is an easy one.
Figure 4.24 shows an initial tree and the result of a deletion.
The node to be deleted is the left child of the root; the key value is 2
It is replaced with the smallest data in its right subtree (3), and then that node is deleted as before.
If the number of deletions is expected to be small, then a popular strategy to use is lazy deletion: When an element is to be deleted, it is left in the tree and merely marked.
Intuitively, we expect that all of the operations of the previous section should take O(logN) time, because in constant time we descend a level in the tree, thus operating on a tree that is now roughly half as large.
Indeed, the running time of all the operations is O(d), where d is the depth of the node containing the accessed item (in the case of remove this may be the replacement node in the two-child case)
We prove in this section that the average depth over all nodes in a tree is O(logN) on the assumption that all insertion sequences are equally likely.
The sum of the depths of all nodes in a tree is known as the internal path length.
We will now calculate the average internal path length of a binary search tree, where the average is taken over all possible insertion sequences into binary search trees.
It is tempting to say immediately that this result implies that the average running time of all the operations discussed in the previous section is O(logN), but this is not entirely.
We could try to eliminate the problem by randomly choosing between the smallest element in the right subtree and the largest in the left when replacing the deleted element.
This apparently eliminates the bias and should keep the trees balanced, but nobody has actually proved this.
If the input comes into a tree presorted, then a series of inserts will take quadratic time and give a very expensive implementation of a linked list, since the tree will consist only of nodes with no left children.
One solution to the problem is to insist on an extra structural condition called balance: No node is allowed to get too deep.
There are quite a few general algorithms to implement balanced trees.
Most are quite a bit more complicated than a standard binary search tree, and all take longer on average for updates.
They do, however, provide protection against the embarrassingly simple cases.
Below, we will sketch one of the oldest forms of balanced search trees, the AVL tree.
An AVL (Adelson-Velskii and Landis) tree is a binary search tree with a balance condition.
The balance condition must be easy to maintain, and it ensures that the depth of the tree is O(logN)
The simplest idea is to require that the left and right subtrees have the same height.
As Figure 4.28 shows, this idea does not force the tree to be shallow.
S(h) is closely related to the Fibonacci numbers, from which the bound claimed above on the height of an AVL tree follows.
To ideally rebalance the tree, we would like to move X up a level and Z down a level.
Note that this is actually more than the AVL property would require.
As a result of this work, which requires only a few link changes, we have another binary search tree that is an AVL tree.
This happens because X moves up one level, Y stays at the same level, and Z moves down one level.
Furthermore, the new height of the entire subtree is exactly the same as the height of the original subtree prior to the insertion that caused X to grow.
Thus no further updating of heights on the path to the root is needed, and consequently no further rotations are needed.
As we mentioned earlier, case 4 represents a symmetric case.
The next item we insert is 7, which causes another rotation:
The problem is that subtree Y is too deep, and a single rotation does not make it any less deep.
The double rotation that solves the problem is shown in Figure 4.35
The fact that subtree Y in Figure 4.34 has had an item inserted into it guarantees that it is nonempty.
Thus, we may assume that it has a root and two subtrees.
Consequently, the tree may be viewed as four subtrees connected by three nodes.
If 13 is now inserted, there is an imbalance at the root.
We insert 8 without a rotation creating an almost perfectly balanced tree:
Finally, we will insert 9 to show the symmetric case of the double rotation.
With all this, we are ready to write the AVL routines.
We show some of the code here; the rest is online.
Figure 4.38 Method to compute height of an AVL node.
This method is necessary to handle the annoying case of a null reference.
The basic insertion routine is easy to write (see Figure 4.39): It adds only a single line at the end that invokes a balancing method.
The balancing method applies a single or double rotation if needed, updates the height, and returns the resulting tree.
The basic idea of the splay tree is that after a node is accessed, it is pushed to the root by a series of AVL tree rotations.
Notice that if a node is deep, there are many nodes on the path that are also relatively deep, and by restructuring we can make future accesses cheaper on all these nodes.
Thus, if the node is unduly deep, then we want this restructuring to have the side effect of balancing the tree (to some extent)
Besides giving a good time bound in theory, this method is likely to have practical utility, because in many applications, when a node is accessed, it is likely to be accessed again in the near future.
Studies have shown that this happens much more often than one would expect.
Splay trees also do not require the maintenance of height or balance information, thus saving space and simplifying the code to some extent (especially when careful implementations are written)
This means that we rotate every node on the access path with its parent.
As an example, consider what happens after an access (a find) on k1 in the following tree.
First, we would perform a single rotation between k1 and its parent, obtaining the following tree.
Then two more rotations are performed until we reach the root.
The splaying strategy is similar to the rotation idea above, except that we are a little more selective about how rotations are performed.
As an example, consider the tree from the last example, with a contains on k1:
Although it is hard to see from small examples, splaying not only moves the accessed node to the root but also has the effect of roughly halving the depth of most nodes on the access path (some shallow nodes are pushed down at most two levels)
This takes a total of O(N), as before, and yields the same tree as simple rotations.
Thus we do not get the same bad behavior from splay trees that is prevalent in the simple rotation strategy.
Actually, this turns out to be a very good case.
A rather complicated proof shows that for this example, the N accesses take a total of O(N) time.)
The main theorem, which we will prove in Chapter 11, is that we never fall behind a pace of O(logN) per operation: We are always on schedule, even though there are occasionally bad operations.
The recursive method in Figure 4.57 does the real work.
Figure 4.57 Routine to print a binary search tree in order.
Figure 4.58 Routine to compute the height of a tree using a postorder traversal.
The third popular traversal scheme that we have seen is preorder traversal.
This could be useful, for example, if you wanted to label each node with its depth.
The worst case of 1.44 logN is unlikely to occur, and the typical case is very close to logN.
One way to implement this is to use a B-tree.
Many variations and improvements are possible, and an implementation is somewhat complex because there are quite a few cases.
However, it is easy to see that, in principle, a B-tree guarantees only a few disk accesses.
A B-tree of order M is an M-ary tree with the following properties:5
The root is either a leaf or has between two and M children.
The remaining issue is how to add and remove items from the B-tree; the ideas involved are sketched next.
However, the cost of doing this is negligible when compared to that of the disk access, which in this case also includes a disk write.
Of course, that was relatively painless because the leaf was not already full.
The solution is simple, however: Since we now have L+1 items, we split them into two leaves, both guaranteed to have the minimum number of data records needed.
Two disk accesses are required to write these leaves, and a third disk access is required to update the parent.
Note that in the parent, both keys and branches change, but they do so in a controlled way that is easily calculated.
Although splitting nodes is time-consuming because it requires at least two additional disk writes, it is a relatively rare occurrence.
Put another way, for every split, there are roughly L/2 nonsplits.
The node splitting in the previous example worked because the parent did not have its full complement of children.
However, it can adopt from a neighbor because the neighbor has four children.
The Set interface represents a Collection that does not allow duplicates.
A special kind of Set, given by the SortedSet interface, guarantees that the items are maintained in sorted order.
Because a Set IS-A Collection, the idioms used to access items in a List, which are inherited from Collection, also work for a Set.
The print method described in Figure 3.6 will work if passed a Set.
A Map is an interface that represents a collection of entries that consists of keys and their values.
Keys must be unique, but several keys can map to the same values.
In a SortedMap, the keys in the map are maintained in logically sorted order.
The basic operations for a Map include methods such as isEmpty, clear, size, and most importantly, the following:
If there are no null values in the Map, the value returned by get can be used to determine if key is in the Map.
However, if there are null values, you have to use containsKey.
Method put places a key/value pair into the Map, returning either null or the old value associated with key.
Iterating through a Map is trickier than a Collection because the Map does not provide an iterator.
Instead, three methods are provided that return the view of a Map as a Collection.
Since the views are themselves Collections, the views can be iterated.
Methods keySet and values return simple collections (the keys contain no duplicates, thus the keys are returned in a Set)
The entrySet is returned as a Set of entries (there are no duplicate entries, since the keys are unique)
For an object of type Map.Entry, the available methods include accessing the key, the value, and changing the value:
KeyType getKey( ) ValueType getValue( ) ValueType setValue( ValueType newValue )
Java requires that TreeSet and TreeMap support the basic add, remove, and contains operations in logarithmic worst-case time.
Consequently, the underlying implementation is a balanced binary search tree.
Typically, an AVL tree is not used; instead, top-down red-black trees, which are discussed in Section 12.2, are often used.
An important issue in implementing TreeSet and TreeMap is providing support for the iterator classes.
Of course, internally, the iterator maintains a link to the “current” node.
When the iterator is constructed, have each iterator store as its data an array containing the TreeSet items.
This is lame, because we might as well use toArray and have no need for an iterator.
Have the iterator maintain a stack storing nodes on the path to the current node.
With this information, one can deduce the next node in the iteration, which is either the node in the current node’s right subtree that contains the minimum item, or the nearest ancestor that contains the current node in its left subtree.
This makes the iterator somewhat large, and makes the iterator code clumsy.
Have each node in the search tree store its parent in addition to the children.
The iterator is not as large, but there is now extra memory required in each node, and the code to iterate is still clumsy.
Have each node maintain extra links: one to the next smaller, and one to the next larger node.
This takes space, but the iteration is very simple to do, and it is easy to maintain these links.
Boolean variables to allow the routines to tell if a left link is being used as a standard binary search tree left link or a link to the next smaller node, and similarly for the right link (Exercise 4.50)
This idea is called a threaded tree, and is used in many balanced binary search tree implementations.
The most straightforward strategy is to use a Map in which the keys are words and the values are lists containing the words that can be changed from the key with a one-character substitution.
The routine in Figure 4.65 shows how the Map that is eventually produced (we have yet to write code for that part) can be used to print the required answers.
The code obtains the entry set and uses the enhanced for loop to step through the entry set and view entries that are pairs consisting of a word and a list of words.
Figure 4.65 Given a map containing words as keys and a list of words that differ in only one character as values, output words that have minWords or more words obtainable by a one-character substitution.
Figure 4.66 Routine to check if two words differ in only one character.
The main issue is how to construct the Map from an array that contains the 89,000 words.
The routine in Figure 4.66 is a straightforward function to test if two words are identical except for a one-character substitution.
We can use the routine to provide the simplest algorithm for the Map construction, which is a brute-force test of all pairs of words.
Among other things, this avoids repeated calls to cast from Object to String, which occur behind the scenes if generics are used.
Figure 4.67 Function to compute a map containing words as keys and a list of words that differ in only one character as values.
The problem with this algorithm is that it is slow, and takes 75 seconds on our computer.
An obvious improvement is to avoid comparing words of different lengths.
We can do this by grouping words by their length, and then running the previous algorithm on each of the separate groups.
It is interesting to note that although the use of the additional Maps makes the algorithm faster, and the syntax is relatively clean, the code makes no use of the fact that the keys of the Map are maintained in sorted order.
As such, it is possible that a data structure that supports the Map operations but does not guarantee sorted order can perform better, since it is being asked to do less.
Chapter 5 explores this possibility and discusses the ideas behind the alternative Map implementation, known as a HashMap.
A HashMap reduces the running time of the implementation from one second to roughly 0.8 seconds.
Figure 4.68 Function to compute a map containing words as keys and a list of words that differ in only one character as values.
Figure 4.69 Function to compute a map containing words as keys and a list of words that differ in only one character as values.
We have seen uses of trees in operating systems, compiler design, and searching.
Expression trees are a small example of a more general structure known as a parse tree, which is a central data structure in compiler design.
Parse trees are not binary but are relatively simple extensions of expression trees (although the algorithms to build them are not quite so simple)
The operations that do not change the tree, as insertion does, can all use the standard binary search tree code.
This can be somewhat complicated, especially in the case of deletion.
We showed how to restore the tree after insertions in O(logN) time.
Nodes in splay trees can get arbitrarily deep, but after every access the tree is adjusted in a somewhat mysterious manner.
The net effect is that any sequence of M operations takes O(M logN) time, which is the same as a balanced tree would take.
In practice, the running time of all the balanced tree schemes, while slightly faster for searching, is worse (by a constant factor) for insertions and deletions than the simple binary search tree, but this is generally acceptable in view of the protection being given against easily obtained worst-case input.
Chapter 12 discusses some additional search tree data structures and provides detailed implementations.
Add to each node a link to the parent node.
Add to each node a link to the next smallest and next largest node.
To make your code simpler, add a header and tail node which are not part of the binary search tree, but help make the linked list part of the code simpler.
Explain how to generate a random integer between 1 and M that is already in the tree (so a random deletion can be performed)
Replace with the largest node, X, in TL and recursively remove X.
Alternately replace with the largest node in TL and the smallest node in TR, and.
Replace with either the largest node in TL or the smallest node in TR (recursively.
Especially challenging are findMin and findMax, which must now be done recursively.
What is the minimum number of nodes in an AVL tree of height 15?
Show that if all nodes in a splay tree are accessed in sequential order, the resulting tree consists of a chain of left children.
Count the total number of rotations performed over the sequence.
How does the running time compare to AVL trees and unbalanced binary search trees?
Write a routine to do this for each node in the tree.
The y coordinate can be computed by using the negative of the depth of the.
Write a routine to do this for each node in the tree.
In terms of some imaginary unit, what will the dimensions of the picture be?
How can you adjust the units so that the tree is always roughly two-thirds as high as it is wide?
Prove that using this system no lines cross, and that for any node, X, all elements in X’s left subtree appear to the left of X and all elements in X’s right subtree appear to the right of X.
Note that you have to scale the stored coordinates into pixels.)
Write a method to decide whether two binary trees are similar.
For instance, the two trees in Figure 4.74 are isomorphic because they are the same if the children of A, B, and G, but not the other nodes, are swapped.
Give a polynomial time algorithm to decide if two trees are isomorphic.
What is the running time of your program (there is a linear solution)?
Give an algorithm to perform this transformation using O(N logN) rotations on average.
Suppose that if a node has a null left child, we make its left child link to its inorder predecessor, and if a node has a null right child, we make its right child link to its inorder successor.
This is known as a threaded tree, and the extra links are called threads.
How can we distinguish threads from real children links? b.
Write routines to perform insertion and deletion into a tree threaded in the.
Use the results of Exercise 4.6 to determine the average number of leaves in an N node binary search tree.
Several papers deal with the lack of balance caused by biased deletion algorithms in binary search trees.
Simulation results for AVL trees, and variants in which the height imbalance is allowed to be at most k for various values of k, are presented in [21]
Analysis of the average search cost in AVL trees is incomplete, but some results are contained in [24]
In Chapter 4, we discussed the search tree ADT, which allowed various operations on a set of elements.
In this chapter, we discuss the hash table ADT, which supports only a subset of the operations allowed by binary search trees.
The central data structure in this chapter is the hash table.
The only remaining problems deal with choosing a function, deciding what to do when two keys hash to the same value (this is known as a collision), and deciding on the table size.
If the input keys are integers, then simply returning Key mod TableSize is generally a reasonable strategy, unless Key happens to have some undesirable properties.
In this case, the choice of hash function needs to be carefully considered.
For instance, if the table size is 10 and the keys all end in zero, then the standard hash function is a bad choice.
For reasons we shall see later, and to avoid situations like the one above, it is often a good idea to ensure that the table size is prime.
When the input keys are random integers, then this function is not only very simple to compute but also distributes the keys evenly.
Usually, the keys are strings; in this case, the hash function needs to be chosen carefully.
One option is to add up the ASCII (or Unicode) values of the characters in the string.
The hash function depicted in Figure 5.2 is simple to implement and computes an.
Even if none of these combinations collide, only 28 percent of the table can actually be hashed to.
Thus this function, although easily computable, is also not appropriate if the hash table is reasonably large.
If, when an element is inserted, it hashes to the same value as an already inserted element, then we have a collision and need to resolve it.
We will discuss two of the simplest: separate chaining and open addressing; then we will look at some more recently discovered alternatives.
The class skeleton required to implement separate chaining is shown in Figure 5.6
The hash table stores an array of linked lists, which are allocated in the constructor.
Figure 5.8 Example of Employee class that can be in a hash table.
The code to implement contains, insert, and remove is shown in Figure 5.10
Figure 5.9 Constructors and makeEmpty for separate chaining hash table.
In the insertion routine, if the item to be inserted is already present, then we do nothing; otherwise, we place it in the list.
The element can be placed anywhere in the list; using add is most convenient in our case.
Any scheme could be used besides linked lists to resolve the collisions; a binary search tree or even another hash table would work, but we expect that if the table is large and the hash function is good, all the lists should be short, so basic separate chaining makes no attempt to try anything complicated.
Figure 5.10 contains, insert, and remove routines for separate chaining hash table.
The corresponding formulas, if clustering is not a problem, are fairly easy to derive.
We will assume a very large table and that each probe is independent of the previous probes.
Figure 5.11 Hash table with linear probing, after each insertion.
These formulas are clearly better than the corresponding formulas for linear probing.
Clustering is not only a theoretical problem but actually occurs in real implementations.
Figure 5.12 compares the performance of linear probing (dashed curves) with what would be expected from more random collision resolution.
Successful searches are indicated by an S, and unsuccessful searches and insertions are marked with U and I, respectively.
Figure 5.12 Number of probes plotted against load factor for linear probing (dashed) and random strategy (S is successful search, U is unsuccessful search, and I is insertion)
Figure 5.13 Hash table with quadratic probing, after each insertion.
Quadratic probing is a collision resolution method that eliminates the primary clustering problem of linear probing.
Quadratic probing is what you would expect—the collision function is quadratic.
Figure 5.13 shows the resulting hash table with this collision function on the same input used in the linear probing example.
Indeed, we prove now that if the table is half empty and the table size is prime, then we are always guaranteed to be able to insert a new element.
If quadratic probing is used, and the table size is prime, then a new element can always be inserted if the table is at least half empty.
If the table is even one more than half full, the insertion could fail (although this is extremely unlikely)
It is also crucial that the table size be prime.1 If the table size is not prime, the number of alternative locations can be severely reduced.
Standard deletion cannot be performed in a probing hash table, because the cell might have caused a collision to go past it.
For instance, if we remove 89, then virtually all the remaining contains operations will fail.
Thus, probing hash tables require lazy deletion, although in this case there really is no laziness implied.
The class skeleton required to implement probing hash tables is shown in Figure 5.14
Instead of an array of lists, we have an array of hash table entry cells, which are also shown in Figure 5.14
Each entry in the array of HashEntry references is either.
Not null, and the entry is active (isActive is true)
Not null, and the entry is marked deleted (isActive is false)
Constructing the table (Figure 5.15) consists of allocating space and then setting each HashEntry reference to null.
We ensure in the insert routine that the hash table is at least twice as large as the number of elements in the table, so quadratic resolution will always work.
In the implementation in Figure 5.16, elements that are marked as deleted count as being in the table.
This can cause problems, because the table can get too full prematurely.
Although quadratic probing eliminates primary clustering, elements that hash to the same position will probe the same alternative cells.
Simulation results suggest that it generally causes less than an extra half probe per search.
The following technique eliminates this, but does so at the cost of computing an extra hash function.
Figure 5.14 Class skeleton for hash tables using probing strategies, including the nested HashEntry class.
Figure 5.16 contains routine (and private helpers) for hashing with quadratic probing.
Figure 5.17 insert routine for hash tables with quadratic probing.
Figure 5.18 Hash table with double hashing, after each insertion.
If the table gets too full, the running time for the operations will start taking too long and insertions might fail for open addressing hashing with quadratic resolution.
This can happen if there are too many removals intermixed with insertions.
A solution, then, is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table.
This is obviously a very expensive operation; the running time is O(N), since there are N elements to rehash and the table size is roughly 2N, but it is actually not all that bad, because it happens very infrequently.
On the other hand, if the hashing is performed as part of an interactive system, then the unfortunate user whose insertion caused a rehash could see a slowdown.
Rehashing can be implemented in several ways with quadratic probing.
One alternative is to rehash as soon as the table is half full.
This is why the new table is made twice as large as the old table.
A third, middle-of-the-road strategy is to rehash when the table reaches a certain load factor.
Since performance does degrade as the load factor increases, the third strategy, implemented with a good cutoff, could be best.
Figure 5.22 shows that rehashing is simple to implement, and provides an implementation for separate chaining rehashing also.
The items in the HashSet (or the keys in the HashMap) must provide an equals and hashCode method, as described earlier in Section 5.3
The HashSet and HashMap are currently implemented using separate chaining hashing.
These classes can be used if it is not important for the entries to be viewable in sorted order.
For instance, in the word-changing example in Section 4.8, there were three maps:
A map in which the key is a word length, and the value is a collection of all words of that word length.
A map in which the key is a representative, and the value is a collection of all words with that representative.
A map in which the key is a word, and the value is a collection of all words that differ in only one character from that word.
The performance of a HashMap can often be superior to a TreeMap, but it is hard to know for sure without writing the code both ways.
Thus, in cases where either a HashMap or TreeMap is acceptable, it is preferable to declare variables using the interface type Map and then change the instantiation from a TreeMap to a HashMap, and perform timing tests.
Figure 5.22 Rehashing for both separate chaining and probing hash tables.
This technique is called caching the hash code, and represents another classic time-space tradeoff.
Figure 5.23 shows an implementation of the String class that caches the hash code.
Caching the hash code works only because Strings are immutable: If the String were allowed to change, it would invalidate the hashCode, and the hashCode would have to be reset back to 0
Although two String objects with the same state must have their hash codes computed independently, there are many situations in which the same String object keeps having its hash code queried.
One situation where caching the hash code helps occurs during rehashing, because all the Strings involved in the rehashing have already had their hash codes cached.
On the other hand, caching the hash code does not help in the representative map for the word changing example.
Each of the representatives is a different String computed by removing a character from a larger String, and thus each individual String has to have its hash code computed separately.
However, in the third map, caching the hash code does help, because the keys are only Strings that were stored in the original array of Strings.
The hash tables that we have examined so far all have the property that with reasonable load factors, and appropriate hash functions, we can expect O(1) cost on average for insertions, removes, and searching.
But what is the expected worst case for a search assuming a reasonably well-behaved hash function?
Similar types of bounds are observed (or provable) for the length of the longest expected probe sequence in a probing hash table.
In the remainder of this section we describe the earliest solution to this problem, namely perfect hashing, and then two more recent approaches that appear to offer promising alternatives to the classic hashing schemes that have been prevalent for many years.
But there are two fundamental problems with this approach: First, the number of lists might be unreasonably large; second, even with lots of lists, we might still get unlucky.
However, the preceding analysis suggests the following alternative: Use only N bins, but resolve the collisions in each bin by using hash tables instead of linked lists.
The idea is that because the bins are expected to have only a few items each, the hash table that is used for each bin can be quadratic in the bin size.
As with the original idea, each secondary hash table will be constructed using a different hash function until it is collision free.
The primary hash table can also be constructed several times if the number of collisions that are produced is higher than required.
All that remains to be shown is that the total size of the secondary hash tables is indeed expected to be linear.
If N items are placed into a primary hash table containing N bins, then the total size of the secondary hash tables has expected value at most 2N.
Once that is done, each secondary hash table will itself require only an average of two trials to be collision free.
After the tables are built, any lookup can be done in two probes.
Perfect hashing works if the items are all known in advance.
There are dynamic schemes that allow insertions and deletions (dynamic perfect hashing), but instead we will investigate two newer alternatives that are relatively easy to code and appear to be competitive in practice with the classic hashing algorithms.
We maintain two tables each more than half empty, and we have two independent has functions that can assign each item to a position in each table.
Cuckoo hashing maintains the invariant that an item is always stored in one of these two locations.
Immediately, this implies that a search in a cuckoo hash table requires at most two.
At this point there are two options: One is to look in Table 2
It happens that in this case it is not, but the algorithm that the standard cuckoo hash table uses does not bother to look.
Instead, it preemptively places the new item B in Table 1
It is easy to insert C, and this is shown in Figure 5.28
Note also, that the Table 2 location is not already taken, but we don’t look there.
It is not yet hopeless since when G is displaced, we would now try the other hash table, at position 2
However, while that could be successful in general, in this case there is a cycle and the insertion will not terminate.
However, if the table’s load factor is at 0.5 or higher, then the probability of a cycle becomes drastically higher, and this scheme is unlikely to work well at all.
Cuckoo Hash Table Implementation Implementing cuckoo hashing requires a collection of hash functions; simply using hashCode to generate the collection of hash functions makes no sense, since any hashCode collisions will result in collisions in all the hash functions.
Figure 5.36 shows a simple interface that can be used to send families of hash functions to the cuckoo hash table.
Thus our implementation differs from the classic notion of two separately addressable hash tables.
We can implement the classic version by making relatively minor changes to the code; however, this version provided in this section seems to perform better in tests using simple hash functions.
The helper routine for insertion is shown in Figure 5.43
We declare a variable rehashes to keep track of how many attempts have been made to rehash in this insertion.
Our insertion routine is mutually recursive: If needed, insert eventually calls rehash, which eventually calls back into insert.
Thus rehashes is declared in an outer scope for code simplicity.
We have already tested that the item to insert is not already present.
At lines 15–25, we check to see if any of the valid.
Evicting the item purely randomly did not perform well in experiments: In particular, with only two hash functions, it tended to create cycles.
To alleviate the last problem, we maintain the last position that was evicted and if our random item was the last evicted item, we select a new random item.
This will loop forever if used with two hash functions, and both hash functions happen to probe to the same.
The code for expand and rehash is shown in Figure 5.44
The zero-parameter rehash leaves the array size unchanged but creates a new array that is populated with newly chosen hash functions.
Finally, Figure 5.45 shows the StringHashFamily class that provides a set of simple hash functions for strings.
Figure 5.41 Routine to remove from a cuckoo hash table.
The idea of hopscotch hashing is to bound the maximal length of the probe sequence by a predetermined constant that is optimized to the underlying computer’s architecture.
Doing so would give constant-time lookups in the worst case, and like cuckoo hashing, the lookup could be parallelized to simultaneously check the bounded set of possible locations.
Figure 5.43 Insertion routine for cuckoo hashing uses a different algorithm that chooses the item to evict randomly, attempting not to re-evict the last item.
The table will attempt to select new hash functions (rehash) if there are too many evictions and will expand if there are too many rehashes.
Figure 5.44 Rehashing and expanding code for cuckoo hash tables.
Continuing the example, suppose we now insert item H with hash value 9
So instead, we look to evict an item and relocate it to position 13
Since position 11 is now close enough to the hash value of H, we can now insert H.
These steps, along with the changes to the Hop information, are shown in Figure 5.47
Finally, we will attempt to insert I whose hash value is 6
Linear probing suggests position 14, but of course that is too far away.
The hops tell which of the positions in the block are occupied with cells containing this hash value.
The hash function must be computable in constant time (i.e., independent of the number of items in the hash table)
The hash function must distribute its items uniformly among the array slots.
As in Section 5.7, we use M to represent TableSize.
Our universal family H will consist of the following set of functions, where a and b are chosen randomly:
If either probing hashing or separate chaining hashing is used, the major problem is that collisions could cause several blocks to be examined during a search, even for a well-distributed hash table.
Furthermore, when the table gets too full, an extremely expensive rehashing step must be performed, which requires O(N) disk accesses.
A clever alternative, known as extendible hashing, allows a search to be performed in two disk accesses.
Notice that all the leaves not involved in the split are now pointed to by two adjacent directory entries.
Thus, although an entire directory is rewritten, none of the other leaves is actually accessed.
This very simple strategy provides quick access times for insert and search operations on large databases.
There are a few important details we have not considered.
These possibilities suggest that it is important for the bits to be fairly random.
This can be accomplished by hashing the keys into a reasonably long integer—hence the name.
Hash tables can be used to implement the insert and search operations in constant average time.
It is especially important to pay attention to details such as load factor when using hash tables, since otherwise the time bounds are not valid.
It is also important to choose the hash function carefully when the key is not a short string or integer.
On the other hand, the worst case for hashing generally results from an implementation error, whereas sorted input can make binary trees perform poorly.
Balanced search trees are quite expensive to implement, so if no ordering information is required and there is any suspicion that the input might be sorted, then hashing is the data structure of choice.
A third common use of hash tables is in programs that play games.
As the program searches through different lines of play, it keeps track of positions it has seen by computing a hash function based on the position (and storing its move for that position)
If the same position reoccurs, usually by a simple transposition of moves, the program can avoid.
This general feature of all game-playing programs is known as the transposition table.
Yet another use of hashing is in online spelling checkers.
If misspelling detection (as opposed to correction) is important, an entire dictionary can be prehashed and words can be checked in constant time.
Hash tables are well suited for this, because it is not important to alphabetize words; printing out misspellings in the order they occurred in the document is certainly acceptable.
Hash tables are often used to implement caches, both in software (for instance, the cache in your Internet browser) and in hardware (for instance, the memory caches in modern computers)
In this case, we can rehash to a table half as large.
Assume that we rehash to a larger table when there are twice as many elements as the table size.
How empty should the table be before we rehash to a smaller table?
Explain the circumstances under which the revised algorithm is faster than the original algorithm.
Is it worth computing this once prior to entering the loop?
Show that the running time is O(k + N) plus the time spent refuting false matches.
Show that the expected number of false matches is negligible.
Explain how hash tables can be used by the compiler to implement this language addition.
Implement the word puzzle program using the algorithm described at the end of the chapter.
The simplest way to do this is to use a single array and modify the hash function to access either the top half or the bottom half.
Luhn wrote an internal IBM memorandum that used separate chaining hashing.
A wealth of information on the subject, including an analysis of hashing with linear probing under the assumption of totally random and independent hashing, can be found in [25]
Precise analytic and simulation results for separate chaining, linear probing, quadratic probing, and double hashing can be found in [19]
However, due to changes (improvements) in computer architecture and compilers, simulation results tend to quickly become dated.
Yet another collision resolution scheme is coalesced hashing, described in [33]
Yao [37] has shown the uniform hashing, in which no clustering exists, is optimal with respect to cost of a successful search, assuming that items cannot move once placed.
Although jobs sent to a printer are generally placed on a queue, this might not always be the best thing to do.
For instance, one job might be particularly important, so it might be desirable to allow that job to be run as soon as the printer is available.
Unfortunately, most systems do not do this, which can be particularly annoying at times.)
This particular application seems to require a special kind of queue, known as a priority queue.
The data structures we will see are among the most elegant in computer science.
As with most data structures, it is sometimes possible to add other operations, but these are extensions and not part of the basic model depicted in Figure 6.1
In Chapter 7, we will see how priority queues are used for external sorting.
There are several obvious ways to implement a priority queue.
We could use a simple linked list, performing insertions at the front in O(1) and traversing the list, which requires O(N) time, to delete the minimum.
Alternatively, we could insist that the list be kept always sorted; this makes insertions expensive (O(N)) and deleteMins cheap (O(1))
The former is probably the better idea of the two, based on the fact that there are never more deleteMins than insertions.
Another way of implementing priority queues would be to use a binary search tree.
This gives an O(logN) average running time for both operations.
This is true in spite of the fact that although the insertions are random, the deletions are not.
Recall that the only element we ever delete is the minimum.
Repeatedly removing a node that is in the left subtree would seem to hurt the balance of the tree by making the right subtree heavy.
In the worst case, where the deleteMins have depleted the left subtree, the right subtree would have at most twice as many elements as it should.
This adds only a small constant to its expected depth.
Notice that the bound can be made into a worst-case bound by using a balanced tree; this protects one against bad insertion sequences.
In this section, we will refer to binary heaps merely as heaps.
Like binary search trees, heaps have two properties, namely, a structure property and a heaporder property.
As with AVL trees, an operation on a heap can destroy one of the properties, so a heap operation must not terminate until all heap properties are in order.
An important observation is that because a complete binary tree is so regular, it can be represented in an array and no links are necessary.
The only problem with this implementation is that an estimate of the maximum heap size is required in advance, but typically this is not a problem (and we can resize if necessary)
The array has a position 0; more on this later.
A heap data structure will, then, consist of an array (of Comparable objects) and an integer representing the current heap size.
Throughout this chapter, we shall draw the heaps as trees, with the implication that an actual implementation will use simple arrays.
Figure 6.5 Two complete trees (only the left tree is a heap)
In a heap, for every node X, the key in the parent of X is smaller than (or equal to) the key in X, with the exception of the root (which has no parent).1 In Figure 6.5 the tree on the left is a heap, but the tree on the right is not (the dashed line shows the violation of heap order)
By the heap-order property, the minimum element can always be found at the root.
Thus, we get the extra operation, findMin, in constant time.
It is easy (both conceptually and practically) to perform the two required operations.
All the work involves ensuring that the heap-order property is maintained.
If X can be placed in the hole without violating heap order, then we do so and are done.
Otherwise we slide the element that is in the hole’s parent node into the hole, thus bubbling the hole up toward the root.
We continue this process until X can be placed in the hole.
This general strategy is known as a percolate up; the new element is percolated up the heap until the correct location is found.
Insertion is easily implemented with the code shown in Figure 6.8
We could have implemented the percolation in the insert routine by performing repeated swaps until the correct order was established, but a swap requires three assignment statements.
If an element is percolated up d levels, the number of assignments performed by the swaps would be 3d.
If the element to be inserted is the new minimum, it will be pushed all the way to the top.
At some point, hole will be 1 and we will want to break out of the loop.
We could do this with an explicit test, or we can put a reference to the inserted item in position 0 in order to make the loop terminate.
We elect to place x into position 0 in our implementation.
The time to do the insertion could be as much as O(logN), if the element to be inserted is the new minimum and is percolated all the way to the root.
Finding the minimum is easy; the hard part is removing it.
When the minimum is removed, a hole is created at the root.
Since the heap now becomes one smaller, it follows that the last element X in the heap must move somewhere in the heap.
If X can be placed in the hole, then we are done.
This is unlikely, so we slide the smaller of the hole’s children into the hole, thus pushing the hole down one level.
We repeat this step until X can be placed in the hole.
Thus, our action is to place X in its correct spot along a path from the root containing minimum children.
We use the same technique as in the insert routine to avoid the use of swaps in this routine.
A frequent implementation error in heaps occurs when there are an even number of elements in the heap, and the one node that has only one child is encountered.
You must make sure not to assume that there are always two children, so this usually involves an extra test.
One extremely tricky solution is always to ensure that your algorithm thinks every node has two children.
Do this by placing a sentinel, of value higher than any in the heap, at the spot after the heap ends, at the start of each percolate down when the heap size is even.
You should think very carefully before attempting this, and you must put in a prominent comment if you do use this technique.
Although this eliminates the need to test for the presence of a right child, you cannot eliminate the requirement that you test when you reach the bottom, because this would require a sentinel for every leaf.
On average, the element that is placed at the root is percolated almost to the bottom of the heap (which is the level it came from), so the average running time is O(logN)
Figure 6.12 Method to perform deleteMin in a binary heap.
Many schedulers automatically drop the priority of a process that is consuming excessive CPU time.
This constructor takes as input N items and places them into a heap.
Since each insert will take O(1) average and O(logN) worstcase time, the total running time of this algorithm would be O(N) average but O(N logN) worst-case.
Since this is a special instruction and there are no other operations intervening, and we already know that the instruction can be performed in linear average time, it is reasonable to expect that with reasonable care a linear time bound can be guaranteed.
The general algorithm is to place the N items into the tree in any order, maintaining the structure property.
Then, if percolateDown(i) percolates down from node i, the buildHeap routine in Figure 6.14 can be used by the constructor to create a heap-ordered tree.
To bound the running time of buildHeap, we must bound the number of dashed lines.
This can be done by computing the sum of the heights of all the nodes in the heap, which is the maximum number of dashed lines.
What we would like to show is that this sum is O(N)
A complete tree is not a perfect binary tree, but the result we have obtained is an upper bound on the sum of the heights of the nodes in a complete tree.
In Chapter 7, we will see how to solve this problem in O(N) average time.
In Chapter 10, we will see an elegant, albeit impractical, algorithm to solve this problem in O(N) worst-case time.
Recall that we have a system, such as a bank, where customers arrive and wait in a line until one of k tellers is available.
Customer arrival is governed by a probability distribution function, as is the service time (the amount of time to be served once a teller is available)
We are interested in statistics such as how long on average a customer has to wait or how long the line might be.
The two events here are (a) a customer arriving and (b) a customer departing, thus freeing up a teller.
We can use the probability functions to generate an input stream consisting of ordered pairs of arrival time and service time for each customer, sorted by arrival time.
We do not need to use the exact time of day.
Rather, we can use a quantum unit, which we will refer to as a tick.
One way to do this simulation is to start a simulation clock at zero ticks.
We then advance the clock one tick at a time, checking to see if there is an event.
If there is, then we process the event(s) and compile statistics.
When there are no customers left in the input stream and all the tellers are free, then the simulation is over.
The problem with this simulation strategy is that its running time does not depend on the number of customers or events (there are two events per customer), but instead depends on the number of ticks, which is not really part of the input.
To see why this is important, suppose we changed the clock units to milliticks and multiplied all the times in the input by 1,000
If the event is a departure, processing includes gathering statistics for the departing customer and checking the line (queue) to see whether there is another customer waiting.
If so, we add that customer, process whatever statistics are required, compute the time when that customer will leave, and add that departure to the set of events waiting to happen.
It is then straightforward, although possibly time-consuming, to write the simulation routines.
Binary heaps are so simple that they are almost always used when priority queues are needed.
A simple generalization is a d-heap, which is exactly like a binary heap except that all nodes have d children (thus, a binary heap is a 2-heap)
The most glaring weakness of the heap implementation, aside from the inability to perform finds, is that combining two heaps into one is a hard operation.
Like a binary heap, a leftist heap has both a structural property and an ordering property.
Indeed, a leftist heap, like virtually all heaps used, has the same heap-order property we have already seen.
The only difference between a leftist heap and a binary heap is that leftist heaps are not perfectly balanced but actually attempt to be very unbalanced.
Figure 6.20 Null path lengths for two trees; only the left tree is leftist.
Indeed, a tree consisting of a long path of left nodes is possible (and actually preferable to facilitate merging)—hence the name leftist heap.
Because leftist heaps tend to have deep left paths, it follows that the right path ought to be short.
Indeed, the right path down a leftist heap is as short as any in the heap.
Otherwise, there would be a path that goes through some node X and takes the left child.
In addition to space for the data and left and right references, each node will have an entry that indicates the null path length.
If either of the two heaps is empty, then we can return the other heap.
Otherwise, to merge the two heaps, we compare their roots.
First, we recursively merge the heap with the larger root with the right subheap of the heap with the smaller root.
The public merge method merges rhs into the controlling heap.
As mentioned above, we can carry out insertions by making the item to be inserted a one-node heap and performing a merge.
To perform a deleteMin, we merely destroy the root, creating two heaps, which can then be merged.
Finally, we can build a leftist heap in O(N) time by building a binary heap (obviously using a linked implementation)
Although a binary heap is clearly leftist, this is not necessarily the best solution, because the heap we obtain is the worst possible leftist heap.
Furthermore, traversing the tree in reverse-level order is not as easy with links.
The buildHeap effect can be obtained by recursively building the left and right subtrees and then percolating the root down.
A skew heap is a self-adjusting version of a leftist heap that is incredibly simple to implement.
The relationship of skew heaps to leftist heaps is analogous to the relation between splay trees and AVL trees.
Skew heaps are binary trees with heap order, but there is no structural constraint on these trees.
Unlike leftist heaps, no information is maintained about the null path length of any node.
The right path of a skew heap can be arbitrarily long at any time, so the worst-case running time of all operations is O(N)
However, as with splay trees, it can be shown (see Chapter 11) that for any M consecutive operations, the total worst-case running time is O(M logN)
As with leftist heaps, the fundamental operation on skew heaps is merging.
The merge routine is once again recursive, and we perform the exact same operations as before, with.
The difference is that for leftist heaps, we check to see whether the left and right children satisfy the leftist heap structure property and swap them if they do not.
For skew heaps, the swap is unconditional; we always do it, with the one exception that the largest of all the nodes on the right paths does not have its children swapped.
This one exception is what happens in the natural recursive implementation, so it is not really a special case at all.
Furthermore, it is not necessary to prove the bounds, but since this node is guaranteed not to have a right child, it would be silly to perform the swap and give it one.
In our example, there are no children of this node, so we do not worry about it.) Again, suppose our input is the same two heaps as before, Figure 6.31
Again, this is done recursively, so by the third rule of recursion (Section 1.3) we need not worry about how it was obtained.
This heap happens to be leftist, but there is no.
The entire tree is leftist, but it is easy to see that that is not always true: Inserting 15 into this new heap would destroy the leftist property.
We can perform all operations nonrecursively, as with leftist heaps, by merging the right paths and swapping left and right children for every node on the right path, with the exception of the last.
After a few examples, it becomes clear that since all but the last node on the right path have their children swapped, the net effect is that this becomes the new left path (see the preceding example to convince yourself)
This makes it very easy to merge two skew heaps visually.3
This is not exactly the same as the recursive implementation (but yields the same time bounds)
If we only swap children for nodes on the right path that are above the point where the merging of right paths terminated due to exhaustion of one heap’s right path, we get the same result as the recursive version.
Although both leftist and skew heaps support merging, insertion, and deleteMin all effectively in O(logN) time per operation, there is room for improvement because we know that binary heaps support insertion in constant average time per operation.
Binomial queues support all three operations in O(logN) worst-case time per operation, but insertions take constant time on average.
As an example, a priority queue of six elements could be represented as in Figure 6.35
The minimum element can then be found by scanning the roots of all the trees.
Since there are at most logN different trees, the minimum can be found in O(logN) time.
Alternatively, we can maintain knowledge of the minimum and perform the operation in O(1) time, if we remember to update the minimum when it changes during other operations.
Merging two binomial queues is a conceptually easy operation, which we will describe by example.
The merge is performed by essentially adding the two queues together.
We count this as three steps (two tree merges plus the stopping case)
The next insertion after 7 is inserted is another bad case and would require three tree merges.
Figure 6.52 shows the type declarations for a node in the binomial tree, and the binomial queue class skeleton.
In order to merge two binomial queues, we need a routine to merge two binomial trees of the same size.
Figure 6.53 shows how the links change when two binomial trees are merged.
The code to do this is simple and is shown in Figure 6.54
At any point we are dealing with trees of rank i.
This process proceeds from rank 0 to the last rank in the resulting binomial queue.
The deleteMin routine for binomial queues is given in Figure 6.56
We can extend binomial queues to support some of the nonstandard operations that.
Prior to Java 1.5, there was no support in the Java library for priority queues.
However, in Java 1.5, there is a generic PriorityQueue class.
In this class, insert, findMin, and deleteMin are expressed via calls to add, element, and remove.
The PriorityQueue can be constructed either with no parameters, a comparator, or another compatible collection.
We considered the additional merge operation and developed three implementations, each of which is unique in its own way.
The leftist heap is a wonderful example of the power of recursion.
The skew heap represents a remarkable data structure because of the lack of balance criteria.
Its analysis, which we will perform in Chapter 11, is interesting in its own right.
The binomial queue shows how a simple idea can be used to achieve a good time bound.
We have also seen several uses of priority queues, ranging from operating systems scheduling to simulation.
Show the result of using the linear-time algorithm to build a binary heap using the same input.
Suppose we try to use an array representation of a binary tree that is not complete.
Determine how large the array must be for the following: a.
Show that a heap of eight elements can be constructed in eight comparisons between heap elements.
Does your algorithm extend to any of the other heap structures discussed in this chapter?
Compare the running time of both algorithms for sorted, reverse-ordered, and random inputs.
Do the savings in comparisons compensate for the increased complexity of your algorithm?
When a findMin or deleteMin is performed, there is a potential problem if the root is marked deleted, since then the node has to be actually deleted and the real minimum needs to be found, which may involve deleting other marked nodes.
In this strategy, deletes cost one unit, but the cost of a deleteMin or findMin depends on the number of nodes that are marked deleted.
Suppose that after a deleteMin or findMin there are k fewer marked nodes than before the operation.
Prove that this algorithm is O(N) in the worst case.
Why might this algorithm be preferable to the algorithm described in the text?
Show that N inserts into an initially empty binomial queue takes O(N) time in the worst case.
Propose an algorithm to insert M nodes into a binomial queue of N elements in O(M + logN) worst-case time.
Modify the merge routine to terminate merging if there are no trees left in H2 and.
Modify the merge so that the smaller tree is always merged into the larger.
The object is to pack all the items without placing more weight in any box than its capacity and using as few boxes as possible.
The Fibonacci heap allows all operations to be performed in O(1) amortized time, except for deletions, which are O(logN)
Relaxed heaps [13] achieve identical bounds in the worst case (with the exception of merge)
The procedure of [3] achieves optimal worst-case bounds for all operations.
In this chapter we discuss the problem of sorting an array of elements.
To simplify matters, we will assume in our examples that the array contains only integers, although our code will once again allow more general objects.
For most of this chapter, we will also assume that the entire sort can be done in main memory, so that the number of elements is relatively small (less than a few million)
Sorts that cannot be performed in main memory and must be done on disk or tape are also quite important.
This type of sorting, known as external sorting, will be discussed at the end of the chapter.
The rest of this chapter will describe and analyze the various sorting algorithms.
These algorithms contain interesting and important ideas for code optimization as well as algorithm design.
Sorting is also an example where the analysis can be precisely performed.
Be forewarned that where appropriate, we will do as much analysis as possible.
Each will be passed an array containing the elements; we assume all array positions contain data to be sorted.
We will assume that N is the number of elements passed to our sorting routines.
The objects being sorted are of type Comparable, as described in Section 1.4
We thus use the compareTo method to place a consistent ordering on the input.
Besides (reference) assignments, this is the only operation allowed on the input data.
The sorting algorithms are easily rewritten to use Comparators, in the event that the default ordering is unavailable or unacceptable.
Because of the nested loops, each of which can take N iterations, insertion sort is O(N2)
Furthermore, this bound is tight, because input in reverse order can achieve this bound.
Notice that this is exactly the number of swaps that needed to be (implicitly) performed by insertion sort.
This is always the case, because swapping two adjacent elements that are out of place removes exactly one inversion, and a sorted array has no inversions.
Since there is O(N) other work involved in the algorithm, the running time of insertion sort is O(I + N), where I is the number of inversions in the original array.
Thus, insertion sort runs in linear time if the number of inversions is O(N)
Figure 7.4 Shellsort routine using Shell’s increments (better increments are possible)
The program in Figure 7.4 avoids the explicit use of swaps in the same manner as our implementation of insertion sort.
Although Shellsort is simple to code, the analysis of its running time is quite another story.
The running time of Shellsort depends on the choice of increment sequence, and the proofs can be rather involved.
The average-case analysis of Shellsort is a long-standing open problem, except for the most trivial increment sequences.
We will prove tight worst-case bounds for two particular increment sequences.
We will prove only the upper bound and leave the proof of the lower bound as an exercise.
The proof requires some well-known results from additive number theory.
References to these results are provided at the end of the chapter.
For the upper bound, as before, we bound the running time of each pass and sum over all passes.
Although this bound holds for the other increments, it is too large to be useful.
Intuitively, we must take advantage of the fact that this increment sequence is special.
N, and assuming that t is even, the total running time is then.
The performance of Shellsort is quite acceptable in practice, even for N in the tens of thousands.
The simplicity of the code makes it the algorithm of choice for sorting up to moderately large input.
As mentioned in Chapter 6, priority queues can be used to sort in O(N logN) time.
The algorithm based on this idea is known as heapsort and gives the best Big-Oh running time we have seen so far.
Using this strategy, after the last deleteMin the array will contain the elements in decreasing sorted order.
If we want the elements in the more typical increasing sorted order, we can change the ordering property so that the parent has a larger key than the child.
The code to perform heapsort is given in Figure 7.8
Thus the code is a little different from the binary heap code.
Experiments have shown that the performance of heapsort is extremely consistent: On average it uses only slightly fewer comparisons than the worst-case bound suggests.
For many years, nobody had been able to show nontrivial bounds on heapsort’s average running time.
The problem, it seems, is that successive deleteMax operations destroy the heap’s randomness, making the probability arguments very complex.
Consequently, for any sequence D, the number of distinct corresponding deleteMax sequences is at most.
It follows that the number of distinct deleteMax sequences that require cost exactly equal to M is at most the number of cost sequences of total cost M times the number of deleteMax sequences for each of these cost sequences.
The total number of heaps with cost sequence less than M is at most.
The fundamental operation in this algorithm is merging two sorted lists.
Because the lists are sorted, this can be done in one pass through the input, if the output is put in a third list.
The basic merging algorithm takes two input arrays A and B, an output array C, and three counters, Actr, Bctr, and Cctr, which are initially set to the beginning of their respective arrays.
When either input list is exhausted, the remainder of the other list is copied to C.
An example of how the merge routine works is provided for the following input.
The remainder of the B array is then copied to C.
The public mergeSort is just a driver for the private recursive method mergeSort.
If a temporary array is declared locally for each recursive call of merge, then there could be logN temporary arrays active at any point.
A close examination shows that since merge is the last line of mergeSort, there only needs to be one temporary array active at any point, and that the temporary array can be created in the public mergeSort driver.
Further, we can use any part of the temporary array; we will use the same portion as the input array a.
This allows the improvement described at the end of this section.
Mergesort is a classic example of the techniques used to analyze recursive routines: we have to write a recurrence relation for the running time.
We will assume that N is a power of 2, so that we always split into even halves.
Otherwise, the time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge, which is linear.
This equation is valid for any N that is a power of 2, so we may also write.
Internal method that merges two sorted halves of a subarray.
This is why it was necessary to divide through by N.
An alternative method is to substitute the recurrence relation continually on the righthand side.
It is theoretically possible to use less extra memory, but the resulting algorithm is complex and impractical.
The running time of mergesort, when compared with other O(N logN) alternatives, depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array)
For instance, in Java, when performing a generic sort (using a Comparator), an element comparison can be expensive (because comparisons might not be easily inlined, and thus the overhead of dynamic dispatch could slow things down), but moving elements is cheap (because they are reference assignments, rather than copies of large objects)
Mergesort uses the lowest number of comparisons of all the popular sorting algorithms, and thus is a good candidate for general-purpose sorting in Java.
In fact, it is the algorithm used in the standard Java library for generic sorting.
As its name implies, quicksort is a fast sorting algorithm in practice and is especially useful in C++, or for sorting primitive types in Java.
It is very fast, mainly due to a very tight and highly optimized inner loop.
It has O(N2) worst-case performance, but this can be made exponentially unlikely with a little effort.
By combining quicksort with heapsort, we can achieve quicksort’s fast running time on almost all inputs, with heapsort’s O(N logN) worst-case running time.
The quicksort algorithm is simple to understand and prove correct, although for many years it had the reputation of being an algorithm that could in theory be highly optimized but in practice was impossible to code correctly.
The algorithm we have described forms the basis of the quicksort.
However, by making the extra lists, and doing so recursively, it is hard to see how we have improved upon.
The classic quicksort algorithm to sort an array S consists of the following four easy steps:
Figure 7.12 shows the action of quicksort on a set of numbers.
The remaining elements in the set are partitioned into two smaller sets.
The sorted arrangement of the entire set is then trivially obtained.
Although the algorithm as described works no matter which element is chosen as the pivot, some choices are obviously better than others.
For now we will assume that all the elements are distinct.
Later on we will worry about what to do in the presence of duplicates.
As a limiting case, our algorithm must do the proper thing if all of the elements are identical.
It is surprising how easy it is to do the wrong thing.
What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part.
While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot.
We move j left, skipping over elements that are larger than the pivot.
When i and j have stopped, i is pointing at a large element and j is pointing at a small element.
If i is to the left of j, those elements are swapped.
The effect is to push a large element to the right and a small element to the left.
In the example above, i would not move and j would slide over one place.
We then swap the elements pointed to by i and j and repeat the process until i and j cross.
When the pivot is swapped with i in the last step, we know that every element in a position p < i must be small.
This is because either position p contained a small element to start with, or the large element originally in position p was replaced during a swap.
A similar argument shows that elements in positions p > i must be large.
One important detail we must consider is how to handle elements that are equal to the pivot.
The questions are whether or not i should stop when it sees an element equal to the pivot and whether or not j should stop when it sees an element equal to the pivot.
Intuitively, i and j ought to do the same thing, since otherwise the partitioning step is biased.
For instance, if i stops and j does not, then all elements that are equal to the pivot will wind up in S2
To get an idea of what might be good, we consider the case where all the elements in the array are identical.
If both i and j stop, there will be many swaps between identical elements.
Although this seems useless, the positive effect is that i and j will cross in the middle, so when the pivot is replaced, the partition creates two nearly equal subarrays.
The mergesort analysis tells us that the total running time would then be O(N logN)
The real heart of the quicksort routine is in Figure 7.15
This initialization depends on the fact that median-of-three partitioning has some side.
Figure 7.16 A small change to quicksort, which breaks the algorithm.
Like mergesort, quicksort is recursive, and hence, its analysis requires solving a recurrence formula.
We will do the analysis for a quicksort, assuming a random pivot (no medianof-three partitioning) and no cutoff for small arrays.
The running time of quicksort is equal to the running time of the two recursive calls plus the linear time spent in the partition (the pivot selection takes only constant time)
To see that this is the worst possible case, note that the total cost of all the partitions in recursive calls at depth d must be at most N.
Since the recursion depth is at most N, this gives as O(N2) worst-case bound for quicksort.
Best-Case Analysis In the best case, the pivot is in the middle.
To simplify the math, we assume that the two subarrays are each exactly half the size of the original, and although this gives a slight overestimate, this is acceptable because we are only interested in a Big-Oh answer.
We need to remove the summation sign to simplify matters.
We note that we can telescope with one more equation.
In contrast to quicksort, quickselect makes only one recursive call instead of two.
The worst case of quickselect is identical to that of quicksort and is O(N2)
The analysis is similar to quicksort’s and is left as an exercise.
Using a median-of-three pivoting strategy makes the chance of the worst case occurring almost negligible.
By carefully choosing the pivot, however, we can eliminate the quadratic worst case and ensure an O(N) algorithm.
The overhead involved in doing this is considerable, so the resulting algorithm is mostly of theoretical interest.
In Chapter 10, we will examine the linear-time worst-case algorithm for selection, and we shall also see an interesting technique of choosing the pivot that results in a somewhat faster selection algorithm in practice.
A decision tree is an abstraction used to prove lower bounds.
In our context, a decision tree is a binary tree.
Each node represents a set of possible orderings, consistent with comparisons that have been made, among the elements.
Every algorithm that sorts by using only comparisons can be represented by a decision tree.
Of course, it is only feasible to draw the tree for extremely small input sizes.
The number of comparisons used by the sorting algorithm is equal to the depth of the deepest leaf.
In our case, this algorithm uses three comparisons in the worst case.
Since a decision tree is large, it follows that there must be some long paths.
To prove the lower bounds, all that needs to be shown are some basic tree properties.
If all the leaves in a decision tree are at depth d or higher, the decision tree must have at least 2d leaves.
Note that all nonleaf nodes in a decision tree have two children.
Every element, x, except the smallest element, must be involved in a comparison with some other element y, in which x is declared larger than y.
Otherwise, if there were two different elements that had not been declared larger than any other elements, then either could be the smallest.
The bound for selection is a little more complicated and requires looking at the structure of the decision tree.
Although decision-tree arguments allowed us to show lower bounds for sorting and some selection problems, generally the bounds that result are not that tight, and sometimes are trivial.
Every element, x, except the smallest element, must be involved in a comparison with some other element y, in which x is declared larger than y.
Otherwise, if there were two different elements that had not been declared larger than any other elements, then either could be the smallest.
This is the underlying idea of an adversary argument which has some basic steps:
Establish that some basic amount of information must be obtained by any algorithm that solves a problem.
In each step of the algorithm, the adversary will maintain an input that is consistent with all the answers that have been provided by the algorithm thus far.
When an item is declared larger than another item, we will change its marking to E (for eliminated)
Initially each unknown item has a value of 0, but there have been no comparisons, so this ordering is consistent with prior answers.
A comparison between two items is either between two unknowns, or it involves at least one item eliminated from being the minimum.
Figure 7.20 shows how our adversary will construct the input values, based on the questioning.
It remains to show that the adversary can maintain values that are consistent with its answers.
If both items are unmarked, then obviously they can be safely assigned values consistent with the comparison answer; this case yields two units of information.
If both are WL, then we can answer consistently with the current assignment, yielding no information.2
Otherwise at least one of the items has only an L or only a W.
We will allow that item to compare redundantly (if it is an L then it loses again, if it is a W then it wins again), and its value can be easily adjusted if needed, based on the other item in the comparison (an L can be lowered as needed; a W can be raised as needed)
This yields at most one unit of information for the other item in the comparison, possibly zero.
Figure 7.21 summarizes the action of the adversary, making y the primary element whose value changes in all cases.
For bucket sort to work, extra information must be available.
Obviously extensions to this are possible.) If this is the case, then the algorithm is simple: Keep an array called count, of size M, which is initialized to all 0’s.
Thus, count has M cells, or buckets, which are initially empty.
After all the input is read, scan the count array, printing out a representation of the.
Although this algorithm seems to violate the lower bound, it turns out that it does not because it uses a more powerful operation than simple comparisons.
By incrementing the appropriate bucket, the algorithm essentially performs an M-way comparison in unit time.
This is similar to the strategy used in extendible hashing (Section 5.9)
This is clearly not in the model for which the lower bound was proven.
Although bucket sort seems like much too trivial an algorithm to be useful, it turns out that there are many cases where the input is only small integers, so that using a method like quicksort is really overkill.
If all the strings have the same length L, then by using buckets for each character, we can implement a radix sort in O (NL)
Figure 7.23 Simple implementation of radix sort for strings, using an ArrayList of buckets.
Initially, in represents arr and out represents the temporary array, buffer.
After each pass, we switch the roles of in and out.
If there are an even number of passes, then at the end, out is referencing arr, so the sort is complete.
Otherwise, we have to copy from the buffer back into arr.
Radix sort for strings will perform especially well when the characters in the string are drawn from a reasonably small alphabet, and when the strings either are relatively short or are very similar.
Because the O(N logN) comparison-based sorting algorithms will generally look only at a small number of characters in each string comparison, once the average string length starts getting large, radix sort’s advantage is minimized or evaporates completely.
If we have extra tapes, then we can expect to reduce the number of passes required to sort our input.
We do this by extending the basic (two-way) merge to a k-way merge.
Merging two runs is done by winding each input tape to the beginning of each run.
Then the smaller element is found, placed on an output tape, and the appropriate input.
We then need two more passes of three-way merging to complete the sort.
The k-way merging strategy developed in the last section requires the use of 2k tapes.
As an example, we will show how to perform two-way merging using only three tapes.
An alternative method is to split the original 34 runs unevenly.
The following table shows the number of runs on each tape after each pass.
If the next record on the input tape is larger than the record we have just output, then it can be included in the run.
Using this observation, we can give an algorithm for producing runs.
Initially, M records are read into memory and placed in a priority queue.
We perform a deleteMin, writing the smallest (valued) record to the output tape.
If it is larger than the record we have just written, we can add it to the priority queue.
Since the priority queue is smaller by one element, we can store this new element in the dead space of the priority queue until the run is completed and use the element for the next run.
Storing an element in the dead space is similar to what is done in heapsort.
We continue doing this until the size of the priority queue is zero, at which point the run is over.
We start a new run by building a new priority queue, using all the elements in the dead space.
As we have seen, it is possible for replacement selection to do no better than the standard algorithm.
However, the input is frequently sorted or nearly sorted to start with, in which case replacement selection produces only a few very long runs.
This kind of input is common for external sorts and makes replacement selection extremely valuable.
Show that for any N, there exists a six-increment sequence such that Shellsort.
For the quicksort implementation in this chapter, what is the running time when all keys are equal?
Does this make it unlikely that quicksort will require quadratic time?
Rewrite the code so that the second recursive call is unconditionally the last.
Do this by reversing the if/else and returning after the call to insertionSort.
Remove the tail recursion by writing a while loop and altering left.
Remove the tail recursion by writing a while loop and altering left or right, as.
Prove that the number of recursive calls is logarithmic in the worst case.
Modify the recursive quicksort to call heapsort on its current subarray if the level.
Hint: Decrement depth as you make recursive calls; when it is 0, switch to heapsort.)
Prove that the worst-case running time of this algorithm is O(N logN)
Explain why the technique in Exercise 7.26 would no longer be needed.
Give an algorithm that performs a three-way in-place partition of an N-element.
Prove that using the algorithm above, sorting an N-element array that contains only d different values, takes O(dN) time.
How large can f(N) be for the entire list still to be sortable in O(N) time?
Give a nontrivial lower bound on the number of comparisons required to merge two sorted lists of N elements, by taking the logarithm of your answer in part (a)
Show that this algorithm is suboptimal, regardless of the choices for Algorithms A, B, and C.
However, there is a better algorithm that makes use of sorting and runs in O(N2 logN) time.
Modify the algorithm as follows: When N is even, but not divisible by four, split.
Give an O(N) algorithm to rearrange the list so that all false elements precede the true elements.
Give an O(N) algorithm to rearrange the list so that all false elements precede maybe elements, which in turn precede true elements.
After that is done, you can solve the problem in linear time.) c.
Code both solutions and compare the running times of your algorithms.
The normal routine is to compare X’s children and then move the child up to X if it is larger (in the case of a (max)heap) than the element we are trying to place, thereby pushing the hole down; we stop when it is safe to place the new element in the hole.
The alternative strategy is to move elements up and the hole down as far as possible, without testing whether the new cell can be inserted.
Gonnet and Baeza-Yates [5] has some more results, as well as a huge bibliography.
An exact average-case analysis of mergesort has been described in [7]
An algorithm to perform merging in linear time without extra space is described in [12]
This paper analyzes the basic algorithm, describes most of the improvements, and includes the selection algorithm.
A detailed analysis and empirical study was the subject of Sedgewick’s dissertation [27]
Decision trees and sorting optimality are discussed in Ford and Johnson [5]
This paper also provides an algorithm that almost meets the lower bound in terms of number of comparisons (but not other operations)
This algorithm was eventually shown to be slightly suboptimal by Manacher [17]
Transitive) a R b and b R c implies that a R c.
This algorithm is dynamic because, during the course of the algorithm, the sets can change via the union operation.
The algorithm must also operate online: When a find is performed, it must give an answer before continuing.
Such an algorithm would be allowed to see the entire sequence of unions and finds.
The answer it provides for each find must still be consistent with all the unions that were performed up until the find, but the algorithm can give all its answers after it has seen all the questions.
The difference is similar to taking a written exam (which is generally off-line—you only have to give the answers before time expires), and an oral exam (which is online, because you must answer the current question before proceeding to the next question)
Our second observation is that the name of the set returned by find is actually fairly arbitrary.
All that really matters is that find(a)==find(b) is true if and only if a and b are in the same set.
These operations are important in many graph theory problems and also in compilers that process equivalence (or type) declarations.
One ensures that the find instruction can be executed in constant worst-case time, and the other ensures that the union instruction can be executed in constant worst-case time.
It has been shown that both cannot be done simultaneously in constant worst-case time.
To perform a union of two sets, we merge the two trees by making the parent link of one tree’s root link to the root node of the other tree.
It should be clear that this operation takes constant time.
The implicit representation of the last forest is shown in Figure 8.5
In our routine, unions are performed on the roots of the trees.
Sometimes the operation is performed by passing any two elements, and having the union perform two finds to determine the roots.
Quadratic running time for a sequence of operations is generally unacceptable.
Fortunately, there are several ways of easily ensuring that this running time does not occur.
To implement this strategy, we need to keep track of the size of each tree.
Since we are really just using an array, we can have the array entry of each root contain the negative of.
Figure 8.13 show a forest and its implicit representation for both union-by-size and union-by-height.
Figure 8.13 Forest with implicit representation for union-by-size and union-by-height.
Path compression is performed during a find operation and is independent of the strategy used to perform unions.
Then the effect of path compression is that every node on the path from x to the root has its parent changed to the root.
Thus, the fast future accesses on these nodes will pay (we hope) for the extra work to do the path compression.
As the code in Figure 8.16 shows, path compression is a trivial change to the basic find algorithm.
The only change to the find routine is that s[x] is made equal to the value returned by find; thus after the root of the set is found recursively, x’s parent link references it.
This occurs recursively to every node on the path to the root, so this implements path compression.
When unions are done arbitrarily, path compression is a good idea, because there is an abundance of deep nodes and these are brought near the root by path compression.
It has been proven that when path compression is done in this case, a sequence of M.
Figure 8.16 Code for the disjoint set find with path compression.
It is still an open problem to determine what the average-case behavior is in this situation.
We begin by establishing two lemmas concerning the properties of the ranks.
The lemma is obvious if there is no path compression.
If, after path compression, some node v is a descendant of w, then clearly v must have been a descendant of w when only unions were considered.
Hence the rank of v is less than the rank of w.
Algorithm A works and computes all the answers correctly, but algorithm B does not compute correctly, or even produce useful answers.
Suppose, however, that every step in algorithm A can be mapped to an equivalent step in algorithm B.
Then it is easy to see that the running time for algorithm B describes the running time for algorithm A, exactly.
Figure 8.18 A large disjoint set tree (numbers below nodes are ranks)
We can use this idea to analyze the running time of the disjoint sets data structure.
We will describe an algorithm B whose running time is exactly the same as the disjoint sets structure, and then algorithm C, whose running time is exactly the same as algorithm B.
Thus any bound for algorithm C will be a bound for the disjoint sets data structure.
However, we run into lots of trouble when we reach Figure 8.22
Here x is in the bottom half, and y is in the top half.
The path compression would require that all nodes from x to y’s child acquire y as its parent.
For nodes in the top half, that is no problem, but for nodes in the bottom half this is a deal breaker: Any recursive charges to the bottom.
So as Figure 8.23 shows, we can perform the path compression on the top, but while some nodes in the bottom will need new parents, it is not clear what to do, because the new parents for those bottom nodes cannot be top nodes, and the new parents cannot be other bottom nodes.
Our basic idea is that we are going to partition the nodes so that all nodes with rank s or lower are in the bottom, and the remaining nodes are in the top.
The choice of s will be made later in the proof.
The next lemma shows that we can provide a recursive formula for the number of parent changes, by splitting the charges into the top and bottom groups.
One of the key ideas is that a recursive formula is written not only in terms of M and N, which would be obvious, but also in terms of the maximum rank in the group.
The path compression that is performed in each of the three cases is covered by C(Mt,Nt, r) + C(Mb,Nb, s)
Node w in case 3 is accounted for by Mt.
Finally, all the other bottom nodes on the path are non-root nodes that can have their parent set to themselves at most once in the entire sequence of compressions.
The bound in Theorem 8.2 is pretty good, but with a little work, we can do even better.
Recall, that a central idea of the recursive decomposition is choosing s to be as small as possible.
But to do this, the other terms must also be small, and as s gets smaller, we would expect C(Mt,Nt, r) to get larger.
But the bound for C(Mt,Nt, r) used a primitive estimate, and Theorem 8.1 itself can now be used to give a better estimate for this term.
Since the C(Mt,Nt, r) estimate will now be lower, we will be able to use a lower s.
A simple algorithm to generate the maze is to start with walls everywhere (except for the entrance and exit)
We then continually choose a wall randomly, and knock it down if the cells that the wall separates are not already connected to each other.
If we repeat this process until the starting and ending cells are connected, then we have a maze.
Figure 8.26 Initial state: all walls up, all cells in their own set.
Figure 8.27 shows a later stage of the algorithm, after a few walls have been knocked down.
By performing two find operations, we see that these are in different sets;
Therefore, we knock down the wall that separates them, as shown in Figure 8.28
At the end of the algorithm, depicted in Figure 8.29, everything is connected, and we are done.
Your program should process a long sequence of equivalence operations using all six of the possible strategies.
Show that if we do union-by-height and finds without path compression, then.
Prove that if path halving is performed on the finds and either union-by-height.
In this chapter we discuss several common problems in graph theory.
Not only are these algorithms useful in practice, they are also interesting because in many real-life applications they are too slow unless careful attention is paid to the choice of data structures.
For undirected graphs, we require that the edges be distinct.
The logic of these requirements is that the path u, v, u in an undirected graph should not be considered a cycle, because (u, v) and (v, u) are the same edge.
In a directed graph, these are different edges, so it makes sense to call this a cycle.
A directed graph is acyclic if it has no cycles.
A directed acyclic graph is sometimes referred to by its abbreviation, DAG.
An undirected graph is connected if there is a path from every vertex to every other vertex.
A directed graph with this property is called strongly connected.
If a directed graph is not strongly connected, but the underlying graph (without direction to the arcs) is connected, then the graph is said to be weakly connected.
A complete graph is a graph in which there is an edge between every pair of vertices.
We will consider directed graphs (undirected graphs are similarly represented)
Suppose, for now, that we can number the vertices, starting at 1
If the graph is not dense, in other words, if the graph is sparse, a better solution is an adjacency list representation.
For each vertex, we keep a list of all adjacent vertices.
If the edges have weights, then this additional information is also stored in the adjacency lists.
In the second scenario, if the vertex is a String (for instance, an airport name, or the name of a street intersection), then a map can be used in which the key is the vertex name and the value is a Vertex and each Vertex object keeps a list of adjacent vertices, and perhaps also the original String name.
In most of the chapter, we present the graph algorithms using pseudocode.
We will do this to save space and, of course, to make the presentation of the algorithms much clearer.
At the end of Section 9.3, we provide a working Java implementation of a routine that makes underlying use of a shortest-path algorithm to obtain its answers.
A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from vi to vj, then vj appears after vi in the ordering.
The graph in Figure 9.3 represents the course prerequisite structure at a state university in Miami.
A directed edge (v,w) indicates that course v must be completed before course w may be attempted.
A topological ordering of these courses is any course sequence that does not violate the prerequisite requirement.
It is clear that a topological ordering is not possible if the graph has a cycle, since for two vertices v and w on the cycle, v precedes w and w precedes v.
Furthermore, the ordering is not necessarily unique; any legal ordering will do.
It returns null if no such vertex exists; this indicates that the graph has a cycle.
By paying more careful attention to the data structures, it is possible to do better.
The cause of the poor running time is the sequential scan through the array of vertices.
However, in the search for a vertex of indegree 0, we look at (potentially) all the vertices, even though only a few have changed.
To implement the box, we can use either a stack or a queue; we will use a queue.
Then all vertices of indegree 0 are placed on an initially empty queue.
While the queue is not empty, a vertex v is removed, and all vertices adjacent to v have their indegrees decremented.
A vertex is put on the queue as soon as its indegree falls to 0
The topological ordering then is the order in which the vertices dequeue.
This is apparent when one realizes that the body of the for loop is executed at most once per edge.
The queue operations are done at most once per vertex, and the other initialization steps, including the computation of indegrees, also take time proportional to the size of the graph.
The input is a weighted graph: associated with each edge (vi, vj) is a cost ci,j to traverse the edge.
There are many examples where we might want to solve the shortest-path problem.
If the vertices represent computers; the edges represent a link between computers; and the costs represent communication costs (phone bill per megabyte of data), delay costs (number of seconds required to transmit a megabyte), or a combination of these and other.
We will examine algorithms to solve four versions of this problem.
First, we will consider the unweighted shortest-path problem and show how to solve it in O(|E|+|V|)
Next, we will show how to solve the weighted shortest-path problem if we assume that there are no negative edges.
For now, suppose we are interested only in the length of the shortest paths, not in the actual paths themselves.
Keeping track of the actual paths will turn out to be a matter of simple bookkeeping.
We can mark this information, obtaining the graph in Figure 9.11
Figure 9.11 Graph after marking the start node as reachable in zero edges.
Now we can start looking for all vertices that are a distance 1 away from s.
These can be found by looking at the vertices that are adjacent to s.
For each vertex, we will keep track of three pieces of information.
First, we will keep its distance from s in the entry dv.
Initially all vertices are unreachable except for s, whose path length is 0
The entry in pv is the bookkeeping variable, which will allow us to print the actual paths.
The entry known is set to true after a vertex is processed.
Initially, all entries are not known, including the start vertex.
When a vertex is marked known, we have a guarantee that no cheaper path will ever be found, and so processing for that vertex is essentially complete.
By tracing back through the pv variable, the actual path can be printed.
We will see how when we discuss the weighted case.
We merely need to begin the process by placing the start node on the queue by itself.
If the graph is weighted, the problem (apparently) becomes harder, but we can still use the ideas from the unweighted case.
Thus, each vertex is marked as either known or unknown.
A tentative distance dv is kept for each vertex, as before.
This distance turns out to be the shortest path length from s to v using only known vertices as intermediates.
As before, we record pv, which is the last vertex to cause a change to dv.
Dijkstra’s algorithm proceeds in stages, just like the unweighted shortest-path algorithm.
At each stage, Dijkstra’s algorithm selects a vertex v, which has the smallest dv.
Figure 9.19 How the data change during the unweighted shortest-path algorithm.
The remainder of a stage consists of updating the values of dw.
Figure 9.24 shows the table after these vertices are selected.
To print out the actual path from a start vertex to some vertex v, we can write a recursive routine to follow the trail left in the p variables.
The path can be printed out using the recursive routine in Figure 9.30
The routine recursively prints the path all the way up to the vertex before v on the path and then just prints v.
Selection of the vertex v is a deleteMin operation, since once the unknown minimum vertex is found, it is no longer unknown and must be removed from future consideration.
The update of w’s distance can be implemented two ways.
DistType cvw = cost of edge from v to w;
Notice that for the typical problems, such as computer mail and mass transit commutes, the graphs are typically very sparse because most vertices have only a couple of edges, so it is important in many applications to use a priority queue to solve this problem.
There are better time bounds possible using Dijkstra’s algorithm if different data structures are used.
In Chapter 11, we will see another priority queue data structure called the Fibonacci heap.
Fibonacci heaps have good theoretical time bounds but a fair amount of overhead, so it is not clear whether using Fibonacci heaps is actually better in practice than Dijkstra’s algorithm with binary heaps.
To date, there are no meaningful average-case results for this problem.
If the graph has negative edge costs, then Dijkstra’s algorithm does not work.
The problem is that once a vertex u is declared known, it is possible that from some other, unknown vertex v there is a path back to u that is very negative.
In such a case, taking a path from s to v back to u is better than going from s to u without using v.
A tempting solution is to add a constant to each edge cost, thus removing negative edges, calculate a shortest path on the new graph, and then use that result on the original.
The naive implementation of this strategy does not work because paths with many edges become more weighty than paths with few edges.
If the graph is known to be acyclic, we can improve Dijkstra’s algorithm by changing the order in which vertices are declared known, otherwise known as the vertex selection rule.
The new rule is to select vertices in topological order.
The algorithm can be done in one pass, since the selections and updates can take place as the topological sort is being performed.
This selection rule works because when a vertex v is selected, its distance, dv, can no longer be lowered, since by the topological ordering rule it has no incoming edges emanating from unknown nodes.
An acyclic graph could model some downhill skiing problem—we want to get from point a to b but can only go downhill, so clearly there are no cycles.
Another possible application might be the modeling of (nonreversible) chemical reactions.
We could have each vertex represent a particular state of an experiment.
Edges would represent a transition from one state to another, and the edge weights might represent the energy released.
If only transitions from a higher energy state to a lower are allowed, the graph is acyclic.
A more important use of acyclic graphs is critical path analysis.
The graph in Figure 9.33 will serve as our example.
Figure 9.32 Pseudocode for weighted shortest-path algorithm with negative edge costs.
The edges represent precedence relationships: An edge (v,w) means that activity v must be completed before activity w may begin.
Of course, this implies that the graph must be acyclic.
We assume that any activities that do not depend (either directly or indirectly) on each other can be performed in parallel by different servers.
To perform these calculations, we convert the activity-node graph to an event-node graph.
Each event corresponds to the completion of an activity and all its dependent activities.
Events reachable from a node v in the event-node graph may not commence until after the event v is completed.
Dummy edges and nodes may need to be inserted in the case where an activity depends on.
This is necessary in order to avoid introducing false dependencies (or false lack of dependencies)
Figure 9.35 shows the earliest completion time for each event in our example event-node graph.
These values can be computed in linear time by maintaining, for each vertex, a list of all adjacent and preceding vertices.
The earliest completion times are computed for vertices by their topological order, and the latest completion times are computed by reverse topological order.
The slack time for each edge in the event-node graph represents the amount of time that the completion of the corresponding activity can be delayed without delaying the overall completion.
Figure 9.37 Earliest completion time, latest completion time, and slack.
On sparse graphs, of course, it is faster to run |V| Dijkstra’s algorithms coded with priority queues.
In this section we write some Java routines to compute word ladders.
In a word ladder each word is formed by changing one character in the ladder’s previous word.
For instance, we can convert zero to five by a sequence of one-character substitutions as follows: zero hero here hire fire five.
This is an unweighted shortest problem in which each word is a vertex, and two vertices have edges (in both directions) between them if they can be converted to each other with a one-character substitution.
In Section 4.8, we described and wrote a Java routine that would create a Map in which the keys are words, and the values are Lists containing the words that can result from a one-character transformation.
As such, this Map represents the graph, in adjacency list format, and we only need to write one routine to run the single-source unweighted shortest-path algorithm and a second routine to output the sequence of words, after the single-source shortest-path algorithm has completed.
It assumes that first is a valid word, which is an easily testable condition prior to the call.
The basic loop incorrectly assigns a previous entry for first (when the initial word adjacent to first is processed), so at line 25 that entry is repaired.
By using a LinkedList and inserting at the front, we obtain the word ladder in the correct order.
Figure 9.48 The vertices reachable from s in the residual graph form one side of a cut; the unreachables form the other side of the cut.
This means that each time (u, v) reappears, u’s distance goes up by 2
This means that any edge can reappear at most |V|/2 times.
Each augmentation causes some edge to reappear so the number of augmentations is O(|E||V|)
For any spanning tree T, if an edge e that is not in T is added, a cycle is created.
The removal of any edge on the cycle reinstates the spanning tree property.
The cost of the spanning tree is lowered if e has lower cost than the edge that was removed.
If, as a spanning tree is created, the edge that is added is the one of minimum cost that avoids creation of a cycle, then the cost of the resulting spanning tree cannot be improved, because any replacement edge would have cost at least as much as an edge already in the spanning tree.
Figure 9.50 A graph G and its minimum spanning tree.
This shows that greed works for the minimum spanning tree problem.
The two algorithms we present differ in how a minimum edge is selected.
One way to compute a minimum spanning tree is to grow the tree in successive stages.
In each stage, one node is picked as the root, and we add an edge, and thus an associated vertex, to the tree.
For this problem, the update rule is even simpler than before: After a vertex v is selected, for each unknown w adjacent to v, dw = min(dw, cw,v)
The entire implementation of this algorithm is virtually identical to that of Dijkstra’s algorithm, and everything that was said about the analysis of Dijkstra’s algorithm applies here.
Be aware that Prim’s algorithm runs on undirected graphs, so when coding it,
A second greedy strategy is continually to select the edges in order of smallest weight and accept an edge if it does not cause a cycle.
The action of the algorithm on the graph in the preceding example is shown in Figure 9.58
When the algorithm terminates, there is only one tree, and this is the minimum spanning tree.
Figure 9.59 shows the order in which edges are added to the forest.
The invariant we will use is that at any point in the process, two vertices belong to the same set if and only if they are connected in the current spanning forest.
If u and v are in the same set, the edge is rejected, because since they are already connected, adding (u, v) would form a cycle.
Otherwise, the edge is accepted, and a union is performed on the two sets containing u and v.
It is easy to see that this maintains the set invariant, because once the edge (u, v) is added to the spanning forest, if w was connected to u and x was connected to v, then x and w must now be connected, and thus belong in the same set.
The edges could be sorted to facilitate the selection, but building a heap in linear time is a much better idea.
Then deleteMins give the edges to be tested in order.
Typically, only a small fraction of the edges need to be tested before the algorithm can terminate, although.
In practice, the algorithm is much faster than this time bound would indicate.
A connected undirected graph is biconnected if there are no vertices whose removal disconnects the rest of the graph.
If the nodes are computers and the edges are links, then if any computer goes down, network mail is unaffected, except, of course, at the down computer.
Similarly, if a mass transit system is biconnected, users always have an alternate route should some terminal be disrupted.
If a graph is not biconnected, the vertices whose removal would disconnect the graph are known as articulation points.
The graph in Figure 9.64 is not biconnected: C and D are articulation points.
The removal of C would disconnect G, and the removal of D would disconnect E and F, from the rest of the graph.
Figure 9.64 A graph with articulation points C and D.
There is no rule that a traversal must be either preorder or postorder.
It is possible to do processing both before and after the recursive calls.
The procedure in Figure 9.69 combines the two routines assignNum and assignLow in a straightforward manner to produce the procedure findArt.
Figure 9.68 Pseudocode to compute Low and to test for articulation points (test for the root is omitted)
We can convert this problem to a graph theory problem by assigning a vertex to each intersection.
Then the edges can be assigned in the natural manner, as in Figure 9.71
The Euler tour and Euler circuit problems, though slightly different, have the same basic solution.
Thus, we will consider the Euler circuit problem in this section.
It is easily seen that this graph has an Euler circuit.
Then we are stuck, and most of the graph is still untraversed.
The single-source unweighted shortest-path problem for directed graphs is also solvable in linear time.
No linear-time algorithm is known for the corresponding longestsimple-path problem.
The situation for these problem variations is actually much worse than we have described.
Not only are no linear algorithms known for these variations, but there are no known algorithms that are guaranteed to run in polynomial time.
The best known algorithms for these problems could take exponential time on some inputs.
In this section we will take a brief look at this problem.
This topic is rather complex, so we will only take a quick and informal look at it.
Because of this, the discussion may be (necessarily) somewhat imprecise in places.
We will see that there are a host of important problems that are roughly equivalent in complexity.
The exact complexity of these NP-complete problems has yet to be determined and remains the foremost open problem in theoretical computer science.
Either all these problems have polynomial-time solutions or none of them do.
The intuitive reason that this problem is undecidable is that such a program might have a hard time checking itself.
For this reason, these problems are sometimes called recursively undecidable.
What happens when LOOP is given itself as input? Either LOOP halts, or it does not halt.
A few steps down from the horrors of undecidable problems lies the class NP.
A deterministic machine, at each point in time, is executing an instruction.
Depending on the instruction, it then goes to some next instruction, which is unique.
It is free to choose any that it wishes, and if one of these steps leads to a solution, it will always choose the correct one.
A nondeterministic machine thus has the power of extremely good (optimal) guessing.
This probably seems like a ridiculous model, since nobody could possibly build a nondeterministic computer, and because it would seem to be an incredible upgrade to your standard computer (every problem might now seem trivial)
We will see that nondeterminism is a very useful theoretical construct.
Furthermore, nondeterminism is not as powerful as one might think.
For instance, undecidable problems are still undecidable, even if nondeterminism is allowed.
Notice also that not all decidable problems are in NP.
Consider the problem of determining whether a graph does not have a Hamiltonian cycle.
To prove that a graph has a Hamiltonian cycle is a relatively simple matter—we just need to exhibit one.
Nobody knows how to show, in polynomial time, that a graph does not have a Hamiltonian cycle.
It seems that one must enumerate all the cycles and check them one by one.
Thus the Non–Hamiltonian cycle problem is not known to be in NP.
Among all the problems known to be in NP, there is a subset, known as the NP-complete problems, which contains the hardest.
An NP-complete problem has the property that any problem in NP can be polynomially reduced to it.
The reason that NP-complete problems are the hardest NP problems is that a problem that is NP-complete can essentially be used as a subroutine for any problem in NP, with only a polynomial amount of overhead.
Thus, if any NP-complete problem has a polynomial-time solution, then every problem in NP must have a polynomial-time solution.
This makes the NP-complete problems the hardest of all NP problems.
As an example, suppose that we already know that the Hamiltonian cycle problem is NP-complete.
For instance, printed circuit boards need to have holes punched so that chips, resistors, and other electronic components can be placed.
Punching the hole is a quick operation; the time-consuming step is positioning the hole puncher.
The time required for positioning depends on the distance traveled from hole to hole.
Since we would like to punch every hole (and then return to the start for the next.
Figure 9.80 Hamiltonian cycle problem transformed to traveling salesman problem.
There is now a long list of problems known to be NP-complete.
To prove that some new problem is NP-complete, it must be shown to be in NP, and then an appropriate NP-complete problem must be transformed into it.
Although the transformation to a traveling salesman problem was rather straightforward, most transformations are actually quite involved and require some tricky constructions.
Generally, several different NP-complete problems are considered before the problem that actually provides the reduction.
As we are only interested in the general ideas, we will not show any more transformations; the interested reader can consult the references.
The formal model for a computer is known as a Turing machine.
Cook showed how the actions of this machine could be simulated by an extremely complicated and long, but still polynomial, Boolean formula.
This Boolean formula would be true if and only if the program which was being run by the Turing machine produced a “yes” answer for its input.
In this chapter we have seen how graphs can be used to model many real-life problems.
Many of the graphs that occur are typically very sparse, so it is important to pay attention to the data structures that are used to implement them.
Propose a method that stores a graph in an adjacency matrix (so that testing for the existence of an edge is O(1)) but avoids the quadratic running time.
Find the shortest path from A to all other vertices for the graph in Figure 9.82
Find the shortest unweighted path from B to all other vertices for the graph in.
Give an example where Dijkstra’s algorithm gives the wrong answer in the presence of a negative edge but no negative-cost cycle.
Explain how to modify Dijkstra’s algorithm to produce a count of the number of different minimum paths from v to w.
Explain how to modify Dijkstra’s algorithm so that if there is more than one minimum path from v to w, a path with the fewest number of edges is chosen.
What is the time complexity of your solution to part (b)?
Prove that a directed graph has an Euler circuit if and only if it is strongly connected and every vertex has equal indegree and outdegree.
Suppose that when taking back edges, we take the back edge to the nearest ancestor.
Show that neither of the graphs in Figure 9.87 is planar.
Show that in a planar graph, there must exist some vertex which is connected to.
Give an algorithm that determines whether it is possible to pick up all the sticks,
Give a linear-time algorithm to test a graph for two-colorability.
Assume graphs are stored in adjacency list format; you must specify any additional data structures that are needed.
Two squares belong to the same group if they share a common edge.
In Figure 9.88, there is one group of four occupied squares, three groups of two occupied squares, and two individual occupied squares.
Assume that the grid is represented by a two-dimensional array.
Computes the size of a group when a square in the group is given.
Suppose we want to output the path in the maze.
Assume that the maze is represented as a matrix; each cell in the matrix stores information about what walls are present (or absent)
Write a program that computes enough information to output a path in the.
Write a program that draws the maze and, at the press of a button, draws the path.
Describe a linear-time algorithm that determines the minimum number of walls.
Explain how each of the following problems (Exercises 9.50–9.53) can be solved by applying a shortest-path algorithm.
Then design a mechanism for representing an input, and write a program that solves the problem.
Assume that all courses are offered every semester and that the student can take an unlimited number of courses.
Given a list of courses and their prerequisites, compute a schedule that requires the minimum number of semesters.
The minimum number of links is an actor’s Bacon number.
Sally Field has a Bacon number of 2, because she was in Forrest Gump with.
Prove that the Hamiltonian cycle problem is NP-complete for directed graphs.
Prove that the unweighted simple longest-path problem is NP-complete for.
An early minimum spanning tree algorithm can be found in [4]
An empirical study of these algorithms suggests that Prim’s algorithm, implemented with decreaseKey, is best in practice on most graphs [42]
Discuss, in general terms, the time and space complexity, where appropriate.
What is the best way to schedule these jobs in order to minimize the average completion time? In this entire section, we will assume nonpreemptive scheduling: Once a job is started, it must run to completion.
This result indicates the reason the operating system scheduler generally gives precedence to shorter jobs.
Figure 10.5 shows an optimal arrangement to minimize mean completion time.
Even if P does not divide N exactly, there can still be many optimal solutions, even if all the job times are distinct.
Although this schedule does not have minimum mean completion time, it has merit in that the completion time of the entire sequence is earlier.
If the same user owns all these jobs, then this is the preferable method of scheduling.
Although these problems are very similar, this new problem turns out to be NP-complete; it is just another way of phrasing the knapsack or bin-packing problems, which we will encounter later in this section.
Figure 10.6 A second optimal solution for the multiprocessor case.
Figure 10.9 Representation of the original code in a tree.
The binary code that represents the alphabet can be represented by the binary tree shown in Figure 10.9
The tree in Figure 10.9 has data only at the leaves.
A better code than the one given in Figure 10.9 can be obtained by noticing that the.
By placing the newline symbol one level higher at its parent, we obtain the new tree in Figure 10.10
This new tree has cost of 173, but is still far from optimal.
Notice that the tree in Figure 10.10 is a full tree: All nodes either are leaves or have two children.
An optimal code will always have this property, since otherwise, as we have already seen, nodes with only one child could move up a level.
These can be obtained by swapping children in the encoding tree.
The main unresolved question, then, is how the coding tree is constructed.
The algorithm to do this was given by Huffman in 1952
Thus, this coding system is commonly referred to as a Huffman code.
At the beginning of the algorithm, there are C single-node trees—one for each character.
At the end of the algorithm there is one tree, and this is the optimal Huffman coding tree.
A worked example will make the operation of the algorithm clear.
Figure 10.13 shows the initial forest; the weight of each tree is shown in small type at the root.
The two trees of lowest weight are merged together, creating the forest shown in Figure 10.14
We will name the new root T1, so that future merges can be stated unambiguously.
We have made s the left child arbitrarily; any tiebreaking procedure can be used.
The total weight of the new tree is just the sum of the weights of the old trees, and can thus be easily computed.
It is also a simple matter to create the new tree, since we merely need to get a new node, set the left and right links, and record the weight.
Now there are six trees, and we again select the two trees of smallest weight.
Finally, the optimal tree, which was shown in Figure 10.11, is obtained by merging the two remaining trees.
We will sketch the ideas involved in proving that Huffman’s algorithm yields an optimal code; we will leave the details as an exercise.
First, it is not hard to show by contradiction that the tree must be full, since we have already seen how a tree that is not full is improved.
The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations.
In this section, we will consider some algorithms to solve the bin-packing problem.
These algorithms will run quickly but will not necessarily produce optimal solutions.
We will prove, however, that the solutions that are produced are not too far from optimal.
What the argument above shows is that an online algorithm never knows when the input might end, so any performance guarantees it provides must hold at every instant throughout the algorithm.
If we follow the foregoing strategy, we can prove the following.
There are inputs that force any online bin-packing algorithm to use at least 43 the optimal number of bins.
Suppose otherwise, and suppose for simplicity that M is even.
Consider any online algorithm A running on the input sequence I1, above.
Recall that this sequence consists of M small items followed by M large items.
Let us consider what the algorithm A has done after processing the Mth item.
At this point in the algorithm, the optimal number of bins is M/2, because we can place two elements in each bin.
Now consider the performance of algorithm A after all items have been packed.
There are three simple algorithms that guarantee that the number of bins used is no more than twice optimal.
There are also quite a few more complicated algorithms with better guarantees.
To prove the lemma we will show that there is no way to place all the items in M bins, which contradicts the premise of the lemma.
Another common technique used to design algorithms is divide and conquer.
Divide: Smaller problems are solved recursively (except, of course, base cases)
Conquer: The solution to the original problem is then formed from the solutions to the subproblems.
Traditionally, routines in which the text contains at least two recursive calls are called divide-and-conquer algorithms, while routines whose text contains only one recursive call are not.
We generally insist that the subproblems be disjoint (that is, essentially nonoverlapping)
Let us review some of the recursive algorithms that have been covered in this text.
In Section 2.4.3, we saw an O(N logN) solution to the maximum subsequence sum problem.
In Chapter 7, we saw the classic examples of divide and conquer, namely mergesort and quicksort, which have O(N logN) worst-case and averagecase bounds, respectively.
The following theorem can be used to determine the running time of most divide-and-conquer algorithms.
If we divide through by am, we obtain the equation.
We can apply this equation for other values of m, obtaining.
Virtually all the terms on the left cancel the leading terms on the right, yielding.
There are two important cases that are not covered by Theorem 10.6
We state two more theorems, leaving the proofs as exercises.
Since the points are sorted by x coordinate, we can draw an imaginary vertical line that partitions the point set into two halves, PL and PR.
Now we have almost exactly the same situation as we saw in the maximum subsequence sum problem in Section 2.4.3
Either the closest points are both in PL, or they are both in PR, or one is in PL and the other is in PR.
Figure 10.30 shows the partition of the point set and these three distances.
Figure 10.30 P partitioned into PL and PR; shortest distances are shown.
Since we would like an O(N logN) solution, we must be able to compute dC with only O(N) additional work.
We have already seen that if a procedure consists of two half-sized recursive calls and O(N) additional work, then the total time will be O(N logN)
Figure 10.31 Two-lane strip, containing all points considered for dC strip.
There are two strategies that can be tried to compute dC.
For large point sets that are uniformly distributed, the number of points that are expected to be in the strip is very small.
Points are all in the strip and sorted by y-coordinate.
The problem is that we have assumed that a list of points sorted by y coordinate is available.
If we perform this sort for each recursive call, then we have O(N logN) extra work:
This is not all that bad, especially when compared to the brute force O(N2)
However, it is not hard to reduce the work for each recursive call to O(N), thus ensuring an O(N logN) algorithm.
One is the point list sorted by x coordinate, and the other is the point list sorted by y coordinate.
These can be obtained by a preprocessing sorting step at cost O(N logN) and thus does not affect the time bound.
We have already seen that P is easily split in the middle.
Once the dividing line is known, we step through Q sequentially, placing each element in QL or QR as appropriate.
It is easy to see that QL and QR will be automatically sorted by y coordinate.
When the recursive calls return, we scan through the Q list and discard all the points whose x coordinates are not within the strip.
Then Q contains only points in the strip, and these points are guaranteed to be sorted by their y coordinates.
This strategy ensures that the entire algorithm is O(N logN), because only O(N) extra work is performed.
The solution in Chapter 7 uses a variation of quicksort and runs in O(N) average time.
Indeed, it is described in Hoare’s original paper on quicksort.
For quicksort, we saw that a good choice for pivot was to pick three elements and use their median.
This gives some expectation that the pivot is not too bad but does not.
Let us assume for the moment that N is divisible by 5, so there are no extra elements.
Suppose also that N/5 is odd, so that the set M contains an odd number of elements.
We will also assume that all the elements are distinct.
The actual algorithm must make sure to handle the case where this is not true.
In this case, there are k elements of type L and k elements of type S.
In this section we describe a divide-and-conquer algorithm that multiplies two N-digit numbers.
Our previous model of computation assumed that multiplication was done in constant time, because the numbers were small.
If we measure multiplication in terms of the size of numbers being multiplied, then the natural multiplication algorithm takes quadratic time.
We also present the classic divide-and-conquer algorithm that multiplies two N by N matrices in subcubic time.
This and the subsequent additions add only O(N) additional work.
If we perform these four multiplications recursively using this algorithm, stopping at an appropriate base case, then we obtain the recurrence.
To achieve a subquadratic algorithm, we must use less than four recursive calls.
To complete the algorithm, we must have a base case, which can be solved without recursion.
When both numbers are one-digit, we can do the multiplication by table lookup.
If one number has zero digits, then we return zero.
In practice, if we were to use this algorithm, we would choose the base case to be that which is most convenient for the machine.
Strassen used a strategy similar to the integer multiplication divide-and-conquer algorithm and showed how to use only seven recursive calls by carefully arranging the computations.
Figure 10.42 Trace of the recursive calculation of Fibonacci numbers.
We could then write the simple program in Figure 10.43 to evaluate the recursion.
Returning to the algorithmic issues, this program contains a triply nested loop and is easily seen to run in O(N3) time.
The references describe a faster algorithm, but since the time to perform the actual matrix multiplication is still likely to be much larger than the time to compute the optimal ordering, this algorithm is still quite practical.
As an example, Figure 10.47 shows seven words along with their probability of occurrence in some context.
This is initially surprising, since the problem appears to be very similar to the construction of a Huffman encoding tree, which, as we have already seen, can be solved by a greedy algorithm.
Construction of an optimal binary search tree is harder, because the data are not constrained to appear only at the leaves, and also because the tree must satisfy the binary search tree property.
Figure 10.47 Sample input for optimal binary search tree problem.
Figure 10.48 Three possible binary search trees for data in previous table.
From this equation, it is straightforward to write a program to compute the cost of the optimal binary search tree.
As usual, the actual search tree can be maintained by saving the value of i that minimizes CLeft,Right.
The standard recursive routine can be used to print the actual tree.
Figure 10.51 shows the table that will be produced by the algorithm.
For each subrange of words, the cost and root of the optimal binary search tree are maintained.
The bottommost entry computes the optimal binary search tree for the entire set of words in the input.
The optimal tree is the third tree shown in Figure 10.48
The precise computation for the optimal binary search tree for a particular subrange, namely, am..if, is shown in Figure 10.52
It is obtained by computing the minimum-cost tree obtained by placing am, and, egg, and if at the root.
Figure 10.51 Computation of the optimal binary search tree for sample input.
The running time of this algorithm is O(N3), because when it is implemented, we obtain a triple loop.
An O(N2) algorithm for the problem is sketched in the exercises.
On a complete graph, where every pair of vertices is connected (in both directions), this algorithm is almost certain to be faster than |V| iterations of Dijkstra’s algorithm, because the loops are so tight.
Thus, this algorithm seems to be well suited for parallel computation.
In some sense, if you have seen one dynamic programming problem, you have seen them all.
More examples of dynamic programming can be found in the exercises and references.
Suppose you are a professor who is giving weekly programming assignments.
You want to make sure that the students are doing their own programs or, at the very least, understand the code they are submitting.
One solution is to give a quiz on the day that each program is due.
On the other hand, these quizzes take time out of class, so it might only be practical to do this for roughly half of the programs.
Your problem is to decide when to give the quizzes.
At least once during the algorithm, a random number is used to make a decision.
The running time of the algorithm depends not only on the particular input, but also on the random numbers that occur.
The worst-case running time of a randomized algorithm is often the same as the worstcase running time of the nonrandomized algorithm.
The important difference is that a good randomized algorithm has no bad inputs, but only bad random numbers (relative to the particular input)
This may seem like only a philosophical difference, but actually it is quite important, as the following example shows.
Throughout the text, in our calculations of running times, we have assumed that all inputs are equally likely.
This is not true, because nearly sorted input, for instance, occurs much more often than is statistically expected, and this causes problems, particularly for quicksort and binary search trees.
By using a randomized algorithm, the particular input is no longer important.
The random numbers are important, and we can get an expected running time, where we now average over all possible random numbers instead of over all possible inputs.
Using quicksort with a random pivot gives an O(N logN)-expectedtime algorithm.
This means that for any input, including already-sorted input, the running time is expected to be O(N logN), based on the statistics of random numbers.
An expected running-time bound is somewhat stronger than an average-case bound but, of course, is weaker than the corresponding worst-case bound.
On the other hand, as we saw in the selection problem, solutions that obtain the worst-case bound are frequently not as practical as their average-case counterparts.
In this section we will examine two additional uses of randomization.
First, we will see a novel scheme for supporting the binary search tree operations in O(logN) expected time.
Once again, this means that there are no bad inputs, just bad random numbers.
From a theoretical point of view, this is not terribly exciting, since balanced.
Nevertheless, the use of randomization leads to relatively simple algorithms for searching, inserting, and especially deleting.
Our second application is a randomized algorithm to test the primality of large numbers.
The algorithm we present runs quickly but occasionally makes an error.
The probability of error can, however, be made negligibly small.
We will use random in place of pseudorandom in the rest of this section.
Generally, a class variable is used to hold the current value in the sequence of x’s.
When the program seems to work, either the system clock can be used or the user can be asked to input a value for the seed.
One might be tempted to assume that all machines have a random number generator at least as good as the one in Figure 10.55 in their standard library.
Unfortunately, these generators always produce values of xi that alternate between even and odd—hardly a desirable property.
Indeed, the lower k bits cycle with period 2k (at best)
Many other random number generators have much smaller cycles than the one provided in Figure 10.55
These are not suitable for the case where long sequences of random numbers are needed.
The Java library and the UNIX drand48 function use a generator of this form.
However, they use a 48-bit linear congruential generator and return only the.
Lines 7–10 show the basic constants of the random number generator.
Because M is a power of 2, we can use bitwise operators.
However, linear congruential generators are unsuitable for some applications, such as cryptography or in simulations that require large numbers of highly independent and uncorrelated random numbers.
The simplest possible data structure to support searching is the linked list.
The time to perform a search is proportional to the number of nodes that have to be examined, which is at most N.
Figure 10.58 Linked list with links to two cells ahead.
Figure 10.59 Linked list with links to four cells ahead.
Each of these steps consumes at most O(logN) total time during a search.
Notice that the search in this data structure is essentially a binary search.
A cursory analysis shows that since the expected number of nodes at each level is unchanged from the original (nonrandomized) algorithm, the total amount of work that is expected to be performed traversing to nodes on the same level is unchanged.
This tells us that these operations have O(logN) expected costs.
Of course, a more formal proof is required, but it is not much different from this.
In this section, we will give a polynomial-time algorithm that can test for primality.
If the algorithm declares that the number is not prime, we can be certain that the number is not prime.
If the algorithm declares that the number is prime, then, with high probability but not 100 percent certainty, the number is prime.
The error probability does not depend on the particular number that is being tested but instead depends on random choices made by the algorithm.
Thus, this algorithm occasionally makes a mistake, but we will see that the error ratio can be made arbitrarily negligible.
The key to the algorithm is a well-known theorem due to Fermat.
In Chapter 7, we proved a theorem related to quadratic probing.
If this also fails, we give up and report no solution.
Figure 10.64 shows a decision tree representing the actions taken to arrive at the solution.
Instead of labeling the branches, we have placed the labels in the branches’ destination nodes.
A node with an asterisk indicates that the points chosen are inconsistent with the given distances; nodes with two asterisks have only impossible nodes as children, and thus represent an incorrect path.
Figure 10.64 Decision tree for the worked turnpike reconstruction example.
We have used one-letter variable names, which is generally poor style, for consistency with the worked example.
We also, for simplicity, do not give the type of variables.
As our last application, we will consider the strategy that a computer might use to play a strategic game, such as checkers or chess.
We will use, as an example, the much simpler game of tic-tac-toe, because it makes the points easier to illustrate.
If a position is not terminal, the value of the position is determined by recursively assuming optimal play by both sides.
This is known as a minimax strategy, because one player (the human) is trying to minimize the value of the position, while the other player (the computer) is trying to maximize it.
A successor position of P is any position Ps that is reachable from P by playing one move.
If the computer is to move when in some position P, it recursively evaluates the value of all the successor positions.
The computer chooses the move with the largest value; this is the value of P.
To evaluate any successor position Ps, all of Ps’s successors are recursively evaluated, and the smallest value is chosen.
This smallest value represents the most favorable reply for the human player.
Since these routines must pass back both the value of the position and the best move, we pass these two variables in a MoveInfo object.
The most costly computation is the case where the computer is asked to pick the opening move.
For more complex games, such as checkers and chess, it is obviously infeasible to search all the way to the terminal nodes.6 In this case, we have to stop the search after a certain depth of recursion is reached.
The nodes where the recursion is stopped become terminal nodes.
These terminal nodes are evaluated with a function that estimates the.
We numbered the squares starting from the top left and moving right.
For instance, in a chess program, the evaluation function measures such variables as the relative amount and strength of pieces and positional factors.
The evaluation function is crucial for success, because the computer’s move selection is based on maximizing this function.
The best computer chess programs have surprisingly sophisticated evaluation functions.
Nevertheless, for computer chess, the single most important factor seems to be number of moves of look-ahead the program is capable of.
To implement this, an extra parameter is given to the search routines.
Figure 10.71 shows the evaluation of the same game tree, with several (but not all possible) unevaluated nodes.
Almost half of the terminal nodes have not been checked.
We show that evaluating them would not change the value at the root.
Figure 10.72 shows the information that has been gathered when it is time to evaluate D.
At this point, we are still in findHumanMove and are contemplating a call to findCompMove on D.
However, we already know that findHumanMove will return at most 40, since it is a min node.
On the other hand, its max node parent has already found a sequence that guarantees 44
In many games, computers are among the best players in the world.
The techniques used are very interesting and can be applied to more serious problems.
Each job ji earns di dollars if it is completed by the time limit ti, but no money if completed after the time limit.
Compute the multiplication between the two matrices Mi and Mi+1, such that.
A Voronoi diagram is a partition of the plane into N regions Ri such that all points in Ri are closer to pi than any other point in P.
Figure 10.78 shows a sample Voronoi diagram for seven (nicely arranged) points.
Give an O(N logN) algorithm to construct the Voronoi diagram.
Give the time and space complexities for your algorithm (as a function of the number of words, N)
Give an algorithm to solve the longest common subsequence problem.
A character can be in S that is not in P.
A character can be in P that is not in S.
Is there a subset of A whose sum is exactly K? a.
Give an algorithm that solves the knapsack problem in O(NK) time.
Give an algorithm that computes the minimum number of coins required to.
Give an algorithm that computes the number of different ways to give K cents.
Two queens are said to attack each other if they are on the same row, column, or (not necessarily main) diagonal.
Give a randomized algorithm to place eight nonattacking queens on the board.
Why does this algorithm not work for general graphs? b.
A submatrix S of A is any group of contiguous entries that forms a square.
Design an O(N2) algorithm that determines the size of the largest submatrix of.
For instance, in the matrix that follows, the largest submatrix is a 4-by-4 square.
Some early chess programs were problematic in that they would get into a repetition of position when a forced win was detected, thereby allowing the opponent to claim a draw.
In tic-tac-toe, this is not a problem, because the program eventually will win.
Modify the tic-tactoe algorithm so that when a winning position is found, the move that leads to the shortest win is always taken.
The board is represented as an N-by-N grid of numbers randomly placed at the start of the game.
At each turn, a player must select a grid element in the current row or column.
The value of the selected position is added to the player’s score, and that position becomes the current position and cannot be selected again.
Players alternate until all grid elements in the current row and column are already selected, at which point the game ends and the player with the higher score wins.
In particular, we will consider the worst-case running time for any sequence of M operations.
This contrasts with the more typical analysis, in which a worst-case bound is given for any single operation.
Because deriving an amortized bound requires us to look at an entire sequence of operations instead of just one, we expect that the analysis will be more tricky.
The rank of a node in a binomial tree is equal to the number of children; in particular, the rank of the root of Bk is k.
A binomial queue is a collection of heap-ordered binomial trees, in which there can be at most one binomial tree Bk for any k.
To merge two binomial queues, an operation similar to addition of binary integers is performed: At any stage we may have zero, one, two, or possibly three Bk trees, depending on whether or not the two priority queues contain a Bk tree and whether or not a Bk tree is carried over from the previous step.
If there is zero or one Bk tree, it is placed as a tree in the resultant binomial queue.
If there are two Bk trees, they are melded into a Bk+1 tree and carried over; if there are three Bk trees, one is placed as a tree in the binomial queue and the other two are melded and carried over.
Insertion is performed by creating a one-node binomial queue and performing a merge.
A less terse explanation of these operations is given in Chapter 6
A binomial queue of N elements can be built by N successive insertions in O(N) time.
The claim, if true, would give an extremely simple algorithm.
Since the worst-case time for each insertion is O(logN), it is not obvious that the claim is true.
Recall that if this algorithm were applied to binary heaps, the running time would be O(N logN)
Let Ti be the number of trees after the ith insertion.
If we add all these equations, most of the Ti terms cancel, leaving.
During the buildBinomialQueue routine, each insertion had a worst-case time of.
O(logN), but since the entire routine used at most 2N units of time, the insertions behaved as though each used no more than two units each.
The state of the data structure at any time is given by a function known as the potential.
The potential function is not maintained by the program but rather is an accounting device that will help with the analysis.
When operations take less time than we have allocated for them, the unused time is “saved” in the form of a higher potential.
In our example, the potential of the data structure is simply the number of trees.
In the analysis above, when we have insertions that use only one unit instead of the two units that are allocated, the extra unit is saved for later by an increase in potential.
When operations occur that exceed the allotted time, then the excess time is accounted for by a decrease in potential.
One may view the potential as representing a savings account.
If an operation uses less than its allotted time, the difference is saved for use later on by more expensive operations.
Figure 11.4 shows the cumulative running time used by buildBinomialQueue over a sequence of insertions.
Observe that the running time never exceeds 2N and that the potential in the binomial queue after any insertion measures the amount of savings.
Once a potential function is chosen, we write the main equation:
Picking a potential function that proves a meaningful bound is a very tricky task; there is no one method that is used.
Nevertheless, the discussion above suggests a few rules, which tell us the properties that good potential functions have.
Always assume its minimum at the start of the sequence.
A popular method of choosing potential functions is to ensure that the potential function is initially 0, and always nonnegative.
All the examples that we will encounter use this strategy.
We can now perform a complete analysis of binomial queue operations.
The amortized running times of insert, deleteMin, and merge are O(1), O(logN), and O(logN), respectively, for binomial queues.
The initial potential is 0, and the potential is always nonnegative, so the amortized time is an upper bound on the actual time.
After the merge, there can be at most logN trees, so the potential can increase by at most O(logN)
Figure 11.5 The insertion cost and potential change for each operation in a sequence.
Recall that for skew heaps, the key operation is merging.
To merge two skew heaps, we merge their right paths and make this the new left path.
For each node on the new path, except the last, the old left subtree is attached as the right subtree.
The last node on the new left path is known to not have a right subtree, so it is silly to give it one.
The bound does not depend on this exception, and if the routine is coded recursively, this is what will happen naturally.
Figure 11.6 shows the result of merging two skew heaps.
What is needed is some sort of a potential function that captures the effect of skew heap operations.
Recall that the effect of a merge is that every node on the right path is moved to the left path, and its old left child becomes the new right child.
One idea might be to classify each node as a right node or left node, depending on whether or not it is a right child, and use the number of right nodes as a potential function.
A similar idea is to classify nodes as either heavy or light, depending on whether or not the right subtree of any node has more nodes than the left subtree.
The potential function we will use is the number of heavy nodes in the (collection of) heaps.
This seems like a good choice, because a long right path will contain an inordinate.
Because nodes on this path have their children swapped, these nodes will be converted to light nodes as a result of the merge.
The amortized time to merge two skew heaps is O(logN)
Now the only nodes whose heavy/light status can change are nodes that are initially on the right path (and wind up on the left path), since no other nodes have their subtrees altered.
The proof is completed by noting that the initial potential is 0 and that the potential is always nonnegative.
It is important to verify this, since otherwise the amortized time does not bound the actual time and is meaningless.
Since the insert and deleteMin operations are basically just merges, they also have O(logN) amortized bounds.
In order to lower this time bound, the time required to perform the decreaseKey operation must be improved.
Fibonacci heaps1 generalize binomial queues by adding two new concepts:
A different implementation of decreaseKey: The method we have seen before is to percolate the element up toward the root.
It does not seem reasonable to expect an O(1) amortized bound for this strategy, so a new method is needed.
Lazy merging: Two heaps are merged only when it is required to do so.
For lazy merging, merges are cheap, but because lazy merging does not actually combine trees, the deleteMin operation could encounter lots of trees, making that operation expensive.
Any one deleteMin could take linear time, but it is always possible to charge the time to previous merge operations.
In particular, an expensive deleteMin must have been preceded by a large number of unduly cheap merges, which were able to store up extra potential.
In binary heaps, the decreaseKey operation is implemented by lowering the value at a node and then percolating it up toward the root until heap order is established.
The name comes from a property of this data structure, which we will prove later in the section.
We do not want to percolate the 0 to the root, because, as we have seen, there are cases where this could be expensive.
The solution is to cut the heap along the dashed line, thus creating two trees, and then merge the two trees back into one.
Let X be the node to which the decreaseKey operation is being applied, and let P be its parent.
Nevertheless, it seems that this scheme will not work, because T2 is not necessarily leftist.
However, it is easy to reinstate the leftist heap property by using two observations:
The heap that results in our example is shown in Figure 11.14
The second idea that is used by Fibonacci heaps is lazy merging.
We will apply this idea to binomial queues and show that the amortized time to perform a merge operation (as well as insertion, which is a special case) is O(1)
The idea is as follows: To merge two binomial queues, merely concatenate the two lists of binomial trees, creating a new binomial queue.
This new queue may have several trees of the same size, so it violates the binomial queue property.
We will call this a lazy binomial queue in order to maintain consistency.
This is a fast operation that always takes constant (worst-case) time.
As before, an insertion is done by creating a one-node binomial queue and merging.
As an example, Figure 11.15 shows a lazy binomial queue.
In a lazy binomial queue, there can be more than one tree of the same size.
To perform the deleteMin, we remove the smallest element, as before, and obtain the tree in Figure 11.16
Figure 11.18 Combining the binomial trees into a binomial queue.
Amortized Analysis of Lazy Binomial Queues To carry out the amortized analysis of lazy binomial queues, we will use the same potential function that was used for standard binomial queues.
Thus, the potential of a lazy binomial queue is the number of trees.
The amortized running times of merge and insert are both O(1) for lazy binomial queues.
The potential function is the number of trees in the collection of binomial queues.
The initial potential is 0, and the potential is always nonnegative.
Thus, over a sequence of operations, the total amortized time is an upper bound on the total actual time.
Figure 11.19 shows one tree in a Fibonacci heap prior to a decreaseKey operation.
Therefore, the node is cut from its parent, becoming the root of a new tree.
We can do this because we can place the constant implied by the Big-Oh notation in the potential function and still get the cancellation of terms, which is needed in the proof.
Figure 11.20 The resulting segment of the Fibonacci heap after the decreaseKey operation.
This will be a crucial observation in our proof of the time bound.
Recall that the reason for marking nodes is that we needed to bound the rank (number of children) R of any node.
We will now show that any node with N descendants has rank O(logN)
Because it is well known that the Fibonacci numbers grow exponentially, it immediately follows that any node with s descendants has rank at most O(log s)
The rank of any node in a Fibonacci heap is O(logN)
If all we were concerned about were the time bounds for the merge, insert, and deleteMin operations, then we could stop here and prove the desired amortized time bounds.
Of course, the whole point of Fibonacci heaps is to obtain an O(1) time bound for decreaseKey as well.
The actual time required for a decreaseKey operation is 1 plus the number of cascading cuts that are performed during the operation.
Since the number of cascading cuts could be much more than O(1), we will need to pay for this with a loss in potential.
If we look at Figure 11.20, we see that the number of trees actually increases with each cascading cut, so we will have to enhance the potential function to include something that decreases during cascading cuts.
Notice that we cannot just throw out the number of trees from the potential function, since then we will not be able to prove the time bound for the merge operation.
Looking at Figure 11.20 again, we see that a cascading cut causes a decrease in the number of marked nodes, because each node that is the victim of a cascading cut becomes an unmarked root.
This way, we have a chance of canceling out the number of cascading cuts.
The amortized time bounds for Fibonacci heaps are O(1) for insert, merge, and decreaseKey and O(logN) for deleteMin.
The potential is the number of trees in the collection of Fibonacci heaps plus twice the number of marked nodes.
As usual, the initial potential is 0 and is always nonnegative.
Thus, over a sequence of operations, the total amortized time is an upper bound on the total actual time.
For the insert operation, the actual time is constant, the number of trees increases by 1, and the number of marked nodes is unchanged.
Recall that the time required for any tree operation on node X is proportional to the number of nodes on the path from the root to X.
If we count each zig operation as one rotation and each zig-zig or zig-zag as two rotations, then the cost of any access is equal to 1 plus the number of rotations.
Figure 11.21 zig, zig-zag, and zig-zig operations; each has a symmetric case (not shown)
The potential function is the sum, over all nodes i in the tree T, of the logarithm of S(i)
The terminology is similar to what we used in the analysis of the disjoint set algorithm, binomial queues, and Fibonacci heaps.
In all these data structures, the meaning of rank is somewhat different, but the rank is generally meant to be on the order (magnitude) of the logarithm of the size of the tree.
Using the sum of ranks as a potential function is similar to using the sum of heights as a potential function.
The important difference is that while a rotation can change the heights of many nodes in the tree, only X, P, and G can have their ranks changed.
Before proving the main theorem, we need the following lemma.
With the preliminaries taken care of, we are ready to prove the main theorem.
If X is the root of T, then there are no rotations, so there is no potential change.
Thus, we may assume that there is at least one rotation.
Because every operation on a splay tree requires a splay, the amortized cost of any operation is within a constant factor of the amortized cost of a splay.
Thus, all splay tree access operations take O(logN) amortized time.
To show that insertions and deletions take O(logN), amortized time, potential changes that occur either prior to or after the splaying step should be accounted for.
A deletion consists of a nonsplaying step that attaches one tree to another.
This does increase the rank of one node, but that is limited by log N (and is compensated by the removal of a node, which at the time was a root)
Thus the splaying costs accurately bound the cost of a deletion.
By using a more general potential function, it is possible to show that splay trees have several remarkable properties.
Low potential in a data structure means that the cost of each operation has been roughly equal to the amount allocated for it.
Negative potential means debt; more time has been spent than has been allocated, so the allocated (or amortized) time is not a meaningful bound.
As expressed by Equation (11.2), the amortized time for an operation is equal to the sum of the actual time and potential change.
Taken over an entire sequence of operations, the amortized time for the sequence is equal to the total sequence time plus the net change in potential.
As long as this net change is positive, then the amortized bound provides an upper bound for the actual time spent and is meaningful.
The keys to choosing a potential function are to guarantee that the minimum potential occurs at the beginning of the algorithm, and to have the potential increase for cheap operations and decrease for expensive operations.
It is important that the excess or saved time be measured by an opposite change in potential.
Show how to reduce the number of links, at the cost of at most a constant factor in the running time.
Let the weight function W(i) be some function assigned to each node in the tree, and let S(i) be the sum of the weights of all the nodes in the subtree rooted at i, including i itself.
Let N be the number of nodes in the tree, and let M be the number of accesses.
Give a formal amortized analysis, with potential function, to show that the amortized cost of an insertion is still O(1)
Describe how to support these operations in constant amortized time per operation.
This suggests using the sum over all nodes of the logarithm of each node’s depth as a potential function.
What is the maximum value of the potential function? b.
What is the minimum value of the potential function? c.
The difference in the answers to parts (a) and (b) gives some indication that.
An excellent survey of amortized analysis is provided in [10]
Most of the references below duplicate citations in earlier chapters.
Amortized analysis is used in [8] to design an online algorithm that processes a series of queries in time only a constant factor larger than any off-line algorithm in its class.
We then examine a data structure that can be used for multidimensional data.
Finally, we examine the pairing heap, which seems to be the most practical alternative to the Fibonacci heap.
Nonrecursive, top-down (instead of bottom-up) search tree implementations when appropriate.
Detailed, optimized implementations that make use of, among other things, sentinel nodes.
In Chapter 4, we discussed the basic splay tree operation.
When an item X is inserted as a leaf, a series of tree rotations, known as a splay, makes X the new root of the tree.
A splay is also performed during searches, and if an item is not found, a splay is performed on the last node on the access path.
In Chapter 11, we showed that the amortized cost of a splay tree operation is O(logN)
A direct implementation of this strategy requires a traversal from the root down the tree, and then a bottom-up traversal to implement the splaying step.
This can be done either by maintaining parent links, or by storing the access path on a stack.
Unfortunately, both methods require a substantial amount of overhead, and both must handle many special cases.
In this section, we show how to perform rotations on the initial access path.
The result is a procedure that is faster in practice, uses only O(1) extra space, but retains the O(logN) amortized time bound.
Figure 12.1 shows the rotations for the zig, zig-zig, and zig-zag cases.
As is customary, three symmetric rotations are omitted.) At any point in the access, we have a current node 541
Tree L stores nodes in the tree T that are less than X, but not in X’s subtree; similarly tree R stores nodes in the tree T that are larger than X, but not in X’s subtree.
Initially, X is the root of T, and L and R are empty.
If the rotation should be a zig, then the tree rooted at Y becomes the new root of the middle tree.
Note carefully that Y does not have to be a leaf for the zig case to apply.
If we are searching for an item that is smaller than Y, and Y has no left child (but does have a right child), then the zig case will apply.
The crucial point is that a rotation between X and Y is performed.
The zig-zag case brings the bottom node Z to the top in the middle tree and attaches subtrees X and Y to R and L, respectively.
Note that Y is attached to, and then becomes, the largest item in L.
For simplicity we don’t distinguish between a “node” and the item in the node.
In the code, the smallest node in R does not have a null left link because there is no need for it.
This means that printTree(r) will include some items that logically are not in R.
This would seem advantageous because testing for a host of cases is time-consuming.
The disadvantage is that by descending only one level, we have more iterations in the splaying procedure.
The search for 19 then results in a terminal zig.
The reassembly, in accordance with Figure 12.3, terminates the splay step.
The SplayTree class, whose skeleton is shown in Figure 12.5, includes a constructor that is used to allocate the nullNode sentinel.
We use the sentinel nullNode to represent logically a null reference.
We will repeatedly use this technique to simplify the code (and consequently make the code somewhat faster)
The header node allows us to be certain that we can attach X to the largest node in R without having to worry that R might be empty (and similarly for the symmetric case dealing with L)
As we mentioned above, before the reassembly at the end of the splay, header.left and header.right reference the roots of R and L, respectively (this is not a typo—follow the links)
Figure 12.7 shows the method to insert an item into a tree.
A new node is allocated (if necessary), and if the tree is empty, a one-node tree is created.
If the data in the new root is equal to x, we have a duplicate; instead of reinserting x, we preserve newNode for a future insertion and return immediately.
If the new root contains a value larger than x, then the new root and its right subtree become a right subtree of newNode, and root’s left subtree becomes the left subtree of newNode.
Similar logic applies if root’s new root contains a value smaller than x.
In Chapter 4, we showed that deletion in splay trees is easy, because a splay will place the target of the deletion at the root.
We close by showing the deletion routine in Figure 12.8
It is indeed rare that a deletion procedure is shorter than the corresponding insertion procedure.
A historically popular alternative to the AVL tree is the red-black tree.
Operations on redblack trees take O(logN) time in the worst case, and, as we will see, a careful nonrecursive implementation (for insertion) can be done relatively effortlessly (compared with AVL trees)
A red-black tree is a binary search tree with the following coloring properties:
If a node is red, its children must be black.
Every path from a node to a null reference must contain the same number of black nodes.
As we have already mentioned, if the parent of the newly inserted item is black, we are done.
There are several cases (each with a mirror image symmetry) to consider if the parent is red.
First, suppose that the sibling of the parent is black (we adopt the convention that null nodes are black)
Let X be the newly added leaf, P be its parent, S be the sibling of the parent (if it exists), and G be the grandparent.
Only X and P are red in this case; G is black, because otherwise there would be two consecutive red nodes prior to the insertion, in violation of red-black rules.
Adopting the splay tree terminology, X, P, and G can form either a zig-zig chain or a zig-zag chain (in either of two directions)
Figure 12.10 shows how we can rotate the tree for the case where P is a left child (note there is a symmetric case)
Even though X is a leaf, we have drawn a more general case that allows X to be in the middle of the tree.
In both cases, the subtree’s new root is colored black, and so even if the original great-grandparent was red, we removed the possibility of two consecutive red nodes.
Equally important, the number of black nodes on the paths into A, B, and C has remained unchanged as a result of the rotations.
After the rotation, there must still be only one black node.
But in both cases, there are three nodes (the new root, G, and S) on the path to C.
Since only one may be black, and since we cannot have consecutive red nodes, it follows that we’d have to color both S and the subtree’s new root red, and G (and our fourth node) black.
That’s great, but what happens if the great-grandparent is also red? In that case, we.
Figure 12.10 Zig rotation and zig-zag rotation work if S is black.
As Figure 12.12 shows, the red-black tree that results is frequently very well balanced.
Experiments suggest that the average red-black tree is about as deep as an average AVL tree and that, consequently, the searching times are typically near optimal.
The advantage of red-black trees is the relatively low overhead required to perform insertion, and the fact that in practice rotations occur relatively infrequently.
An actual implementation is complicated not only by the host of possible rotations, but also by the possibility that some subtrees (such as 10’s right subtree) might be empty, and the special case of dealing with the root (which among other things, has no parent)
Thus, we use two sentinel nodes: one for the root, and nullNode, which indicates a null reference,
Figure 12.14 shows the RedBlackTree skeleton (omitting the methods), along with the constructors.
Next, Figure 12.15 shows the routine to perform a single rotation.
Rather than keeping track of the type of rotation as we descend the tree, we pass item as a parameter.
Since we expect very few rotations during the insertion procedure, it turns out that it is not only simpler, but actually faster, to do it this way.
The routine handleReorient is called when we encounter a node with two red children, and also when we insert a leaf.
Because the result is attached to the parent, there are four cases.
The most tricky part is the observation that a double rotation is really two single rotations, and is done only when branching to X (represented in the insert method by current) takes opposite directions.
As we mentioned in the earlier discussion, insert must keep track of the parent, grandparent, and great-grandparent as the tree is descended.
Since these are shared with handleReorient, we make these class members.
Note that after a rotation, the values stored in the grandparent and great-grandparent are no longer correct.
However, we are assured that they will be restored by the time they are next needed.
If a leaf is black, however, the deletion is more complicated because removal of a black node will violate condition 4
The solution is to ensure during the top-down pass that the leaf is red.
Throughout this discussion, let X be the current node, T be its sibling, and P be their parent.
As we traverse down the tree, we attempt to ensure that X is red.
When we arrive at a new node, we are certain that P is red (inductively, by the invariant we are trying to maintain), and that X and T are black (because we can’t have two consecutive red nodes)
Figure 12.17 Three cases when X is a left child and has two black children.
Note carefully that this case will apply for the leaf, because nullNode is considered to be black.
If both children are red, we can apply either rotation.
As usual, there are symmetric rotations for the case when X is a right child that are not shown.
Our last type of binary search tree, known as the treap, is probably the simplest of all.
Like the skip list, it uses random numbers and gives O(logN) expected time behavior for any input.
Searching time is identical to an unbalanced binary search tree (and thus slower than balanced search trees), while insertion time is only slightly slower than a recursive unbalanced binary search tree implementation.
Although deletion is much slower, it is still O(logN) expected time.
The treap is so simple that we can describe it without a picture.
Each node in the tree stores an item, a left and right link, and a priority that is randomly assigned when the node is created.
A treap is a binary search tree with the property that the node priorities satisfy heap order: Any node’s priority must be at least as large as its parent’s.
After the rotation, t is nullNode, and the left child stores the item that is to be deleted.
Note also that our implementation assumes that there are no duplicates; if this is not true, then the remove could fail (why?)
So if the original used quadratic space, so does the compressed trie.
Fortunately, we can get by with linear space as follows:
In the internal nodes, we store the number of common characters matched from the root until the internal node; this number represents the letter depth.
Figure 12.27 Mapping of character in string to an array of integers.
Example: In our example, if we look at the original character set and use $ to represent the padded character, we get.
At this point, there are no ties for a while, so we quickly advance to the last characters of each group:
Figure 12.34 Routine to compute and assign the tri-character names.
We can use the basic counting radix sort from Chapter 7 to obtain a linear-time sort.
The merge routine has the same basic look and feel as the standard merging algorithm seen in Figure 7.10
Suppose that an advertising company maintains a database and needs to generate mailing labels for certain constituencies.
In one dimension, the problem can be solved by a simple recursive algorithm in O(M + logN) average time, by traversing a preconstructed binary search tree.
Here M is the number of matches reported by the query.
We would like to obtain a similar bound for two or more dimensions.
A moment’s thought will convince you that a randomly constructed 2-d tree has the same structural properties as a random binary search tree: The height is O(logN) on average, but O(N) in the worst case.
Unlike binary search trees, for which clever O(logN) worst-case variants exist, there are no schemes that are known to guarantee a balanced 2-d tree.
The problem is that such a scheme would likely be based on tree rotations, and tree rotations don’t work in 2-d trees.
The best one can do is to periodically rebalance the tree by reconstructing a subtree, as described in the exercises.
Similarly, there are no deletion algorithms beyond the obvious lazy deletion strategy.
Several kinds of queries are possible on a 2-d tree.
We can ask for an exact match, or a match based on one of the two keys; the latter type of request is a partial match query.
Both of these are special cases of an (orthogonal) range query.
An insertion or exact match search in a 2-d tree takes time that is proportional to the depth of the tree, namely, O(logN) on average and O(N) in the worst case.
The running time of a range search depends on how balanced the tree is, whether or not a partial match is requested, and how many items are actually found.
Although there are several exotic structures that support range searching, the k-d tree is probably the simplest such structure that achieves respectable running times.
Figure 12.46 shows the PairNode class and Position interface, which are both nested in the PairingHeap class.
The devil, of course, is in the details: How is combineSiblings implemented? Several variants have been proposed, but none has been shown to provide the same amortized.
Internal method that is the basic operation to maintain order.
The only simple merging strategy that is easily seen to be poor is a left-to-right single-pass merge (Exercise 12.29)
Recurring themes include tree rotations and the use of sentinel nodes to eliminate many of the annoying tests for null references that would otherwise be necessary.
Show that with a recursive call to S3S5S6, we have enough information to sort.
Alternatively, generate a random number each time an item X is accessed.
If this number is smaller than X’s current priority, use it as X’s new priority (performing the appropriate rotation)
How much coding effort is saved by using the sentinel?
Adopt the following strategy: If the left and right subtrees have weights that are not within a factor of 2 of each other, then completely rebuild the subtree rooted at the node.
We can rebuild a node in O(S), where S is the weight of the node.
We can rebuild a node in a k-d tree in O(S log S) time, where S is the weight of.
We can apply the algorithm to k-d trees, at a cost of O(log2 N) per insertion.
Explain in detail all the reasons that the result is no longer a usable 2-d tree.
You should be able to obtain the following bounds: insert in O(logN), deleteMin in O(2k logN), and buildHeap in O(kN)
Clearly, the basic algorithms still work; what are the new time bounds?
What would you expect the average running time to be for a random tree?
Give a formula for the number of regions that result from the partition after N.
Top-down splay trees were described in the original splay tree paper [36]
A similar strategy, but without the crucial rotation, was described in [38]
An implementation of top-down red-black trees without sentinel nodes is given in [15]; this provides.
A related data structure is the priority search tree [27]
The solutions to most of the exercises can be found in the primary references.
