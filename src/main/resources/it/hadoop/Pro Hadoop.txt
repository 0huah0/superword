Pro Hadoop is a guide to using Hadoop Core, a wonderful tool that allows you to use ordinary hardware to solve extraordinary problems.
In the course of my work, I have needed to build applications that would not fit on a single affordable machine, creating custom scaling and distribution tools in the process.
With the advent of Hadoop and MapReduce, I have been able to focus on my applications instead of worrying about how to scale them.
It took some time before I had learned enough about Hadoop Core to actually be effective.
This book is a distillation of that knowledge, and a book I wish was available to me when I first started using Hadoop Core.
I begin by showing you how to get started with Hadoop and the Hadoop Core shared file system, HDFS.
Then you will see how to write and run functional and effective MapReduce jobs on your clusters, as well as how to tune your jobs and clusters for optimum performance.
I provide recipes for unit testing and details on how to debug MapReduce jobs.
I also include examples of using advanced features such as map-side joins and chain mapping.
To bring everything together, I take you through the step-by-step development of a nontrivial MapReduce application.
This will give you insight into a real-world Hadoop project.
It is my sincere hope that this book provides you an enjoyable learning experience and with the knowledge you need to be the local Hadoop Core wizard.
No part of this work may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage or retrieval system, without the prior written permission of the copyright owner and the publisher.
Rather than use a trademark symbol with every occurrence of a trademarked name, we use the names only in an editorial fashion and to the benefit of the trademark owner, with no intention of infringement of the trademark.
Java™ and all Java-based marks are trademarks or registered trademarks of Sun Microsystems, Inc., in the US and other countries.
Apress, Inc., is not affiliated with Sun Microsystems, Inc., and this book was written without endorsement from Sun Microsystems, Inc.
Apress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use.
For more information, reference our Special Bulk Sales–eBook Licensing web page at http://www.apress.com/info/bulksales.
Although every precaution has been taken in the preparation of this work, neither the author(s) nor Apress shall have any liability to any person or entity with respect to any loss or damage caused or alleged to be caused directly or indirectly by the information contained in this work.
The source code for this book is available to readers at http://www.apress.com.
You may need to answer questions pertaining to this book in order to successfully download the code.
He had the idea, walked me through much of the process,
JaSOn Venner is a software developer with more than 20 years of experience developing highly scaled, high-performance systems.
Earlier, he worked primarily in the financial services industry, building high-performance check-processing systems.
His more recent experience has been building the infrastructure to support highly utilized web sites.
He has an avid interest in the biological sciences and is an FAA certificated flight instructor.
Sia CYrUS’s experience in computing spans many decades and areas of software development.
During the 1980s, he specialized in database development in Europe.
In the 1990s, he moved to the United States, where he focused on client/server applications.
Since 2000, he has architected a number of middle-tier business processes.
And most recently, he has been specializing in Web 2.0, Ajax, portals, and cloud computing.
Sia is an independent software consultant who is an expert in Java and development of Java enterprise-class applications.
He has been responsible for innovative and generic software, holding a U.S.
Sia created a very successful configuration-based framework for the telecommunications industry, which he later converted to the Spring Framework.
When not experimenting with new technologies, Sia enjoys playing ice hockey, especially with his two boys, Jason and Brandon.
I remember the days when I couldn’t afford to buy a compiler, and had to sneak time on the university computers, when only people who signed horrible NDAs and who worked at large organizations could read the Unix source code.
His dedication and yes, fanaticism, has changed our world substantially for the better.
Hadoop rides on the back, sweat, and love of Doug Cutting, and many people of Yahoo! Inc.
All of the Hadoop users and contributors who help each other on the mailing lists are wonderful people.
I would also like to thank the Apress staff members who have applied their expertise to make this book into something readable.
While Hadoop supplied the tools to scale applications, it lacked documentation on how to use the framework effectively.
It enables you to rapidly and painlessly get up to speed with Hadoop.
This is the book I wish was available to me when I started using Hadoop.
Who this Book is For This book has three primary audiences: developers who are relatively new to Hadoop or MapReduce and must scale their applications using Hadoop; system administrators who must deploy and manage the Hadoop clusters; and application designers looking for a detailed understanding of what Hadoop will do for them.
Hadoop experts will learn some new details and gain insights to add to their expertise.
How this Book is Structured This book provides step-by-step instructions and examples that will take you from just beginning to use Hadoop to running complex applications on large clusters of machines.
It then walks you through getting the software, installing it on your computer, and running the basic examples.
Chapter 2, The Basics of a MapReduce Job: This chapter explores what is involved in writing the actual code that performs the map and the reduce portions of a MapReduce job, and how to configure a job to use your map and reduce code.
Chapter 3, The Basics of Multimachine Clusters: This chapter walks you through the basics of creating a multimachine Hadoop cluster.
It explains what the servers are, how the servers interact, basic configuration, and how to verify that your cluster is up and running successfully.
You’ll also find out what to do if a cluster doesn’t start.
Chapter 4, HDFS Details for Multimachine Clusters: This chapter covers the details of the Hadoop Distributed File System (HDFS) and provides detailed guidance on the installation, running, troubleshooting, and recovery of your HDFS installations.
Chapter 5, MapReduce Details for Multimachine Clusters: This chapter gives you a detailed understanding of what a MapReduce job is and what the Hadoop Core framework actually does to execute your MapReduce job.
You will learn how to set your job classpath and use shared libraries.
It also covers the input and output formats used by MapReduce jobs.
Chapter 6, Tuning Your MapReduce Jobs: In this chapter, you will learn what you can tune, how to tell what needs tuning, and how to tune it.
With this knowledge, you will be able to achieve optimal performance for your clusters.
Chapter 7, Unit Testing and Debugging: When your job is run across many machines, debugging becomes quite a challenge.
Chapter 7 walks you through how to debug your jobs.
The examples and unit testing framework provided in this chapter also help you know when your job is working as designed.
Chapter 8, Advanced and Alternate MapReduce Techniques: This chapter demonstrates how to use several advanced features of Hadoop Core: map-side joins, chain mapping, streaming, pipes, and aggregators.
You will also learn how to configure your jobs to continue running when some input is bad.
Streaming is a particularly powerful tool, as it allows scripts and other external programs to be used to provide the MapReduce functionality.
Chapter 9, Solving Problems with Hadoop: This chapter describes step-by-step development of a nontrivial MapReduce job, including the whys of the design decisions.
The sample MapReduce job performs range joins, and uses custom comparator and partitioner classes.
Chapter 10, Projects Based on Hadoop and Future Directions: This chapter provides a summary of several projects that are being built on top of Hadoop Core: distributed column-oriented databases, distributed search, matrix manipulation, and machine learning.
There are also references for training and support and future directions for Hadoop Core.
Additionally, this chapter provides a short summary of my favorite tools in the examples: a zero-configuration, two-node virtual cluster.
Prerequisites For those of you who are new to Hadoop, I strongly urge you to try Cloudera’s open source Distribution for Hadoop (http://www.cloudera.com/hadoop)
It provides the stable base of Hadoop 0.18.3 with bug fixes and some new features back-ported in and added-in hooks to the support scribe log file aggregation service (http://scribeserver.wiki.sourceforge.net/)
The Cloudera folks have Amazon machine images (AMIs), Debian and RPM installer files, and an online configuration tool to generate configuration files.
The following are the stock Hadoop Core distributions at the time of this writing:
Hadoop	0.18.3	is	a	good	distribution,	but	has	a	couple	of	issues	related	to	file	descriptor leakage and reduce task stalls.
Hadoop	0.19.0	should	be	avoided,	as	it	has	data	corruption	issues	related	to	the	append and sync changes.
Hadoop	0.19.1	looks	to	be	a	reasonably	stable	release	with	many	useful	features.
Hadoop	0.20.0	has	some	major	API	changes	and	is	still	unstable.
Separate Eclipse projects are provided for each of these releases.
You can access the source code from this book’s details page or find the source code at the following URL (search for Hadoop): http://www.apress.com/book/ sourcecode.
The sample code is designed to be imported into Eclipse as a complete project.
There are several versions of the code, each a designated version of Hadoop Core that includes that Hadoop Core version.
The src directory has the source code for the examples.
The test examples are under test/src in the corresponding package directory.
The directory src/config contains the configuration files that are loaded as Java resources.
Three directories contain JAR or zip files that have specific licenses.
The directory bsd_license contains the items that are provided under the BSD license.
The directory other_ licenses contains items that have other licenses.
A README.txt file has more details about the downloadable code.
Contacting the author Jason Venner can be contacted via e-mail at jvenner@prohadoopbook.com.
Applications frequently require more resources than are available on an inexpensive machine.
Many organizations find themselves with business processes that no longer fit on a single cost-effective computer.
A simple but expensive solution has been to buy specialty machines that have a lot of memory and many CPUs.
This solution scales as far as what is supported by the fastest machines available, and usually the only limiting factor is your budget.
Such a cluster typically attempts to look like a single machine, and typically requires very specialized installation and administration services.
A more economical solution for acquiring the necessary computational resources is cloud computing.
A common pattern is to have bulk data that needs to be transformed, where the processing of each data item is essentially independent of other data items; that is, using a single-instruction multiple-data (SIMD) algorithm.
Hadoop Core provides an open source framework for cloud computing, as well as a distributed file system.
This book is designed to be a practical guide to developing and running software using Hadoop Core, a project hosted by the Apache Software Foundation.
This chapter introduces Hadoop Core and details how to get a basic Hadoop Core installation up and running.
Introducing the MapReduce Model Hadoop supports the MapReduce model, which was introduced by Google as a method of solving a class of petascale problems with large clusters of inexpensive machines.
The model is based on two distinct steps for an application:
Map: An initial ingestion and transformation step, in which individual input records can be processed in parallel.
Reduce: An aggregation or summarization step, in which all associated records must be processed together by a single entity.
The core concept of MapReduce in Hadoop is that input may be split into logical chunks, and each chunk may be initially processed independently, by a map task.
The results of these individual processing chunks can be physically partitioned into distinct sets, which are then.
A map task may run on any compute node in the cluster, and multiple map tasks may be running in parallel across the cluster.
The map task is responsible for transforming the input records into key/value pairs.
The output of all of the maps will be partitioned, and each partition will be sorted.
Each partition’s sorted keys and the values associated with the keys are then processed by the reduce task.
There may be multiple reduce tasks running in parallel on the cluster.
The application developer needs to provide only four items to the Hadoop framework: the class that will read the input records and transform them into one key/value pair per record, a map method, a reduce method, and a class that will transform the key/value pairs that the reduce method outputs into output records.
This crawler received as input large sets of media URLs that were to have their content fetched and processed.
The media items were large, and fetching them had a significant cost in time and resources.
Filter the URLs against a set of exclusion and inclusion filters.
I had 20 machines to work with on this project.
The previous incarnation of the application was very complex and used an open source queuing framework for distribution.
Hundreds of work hours were invested in writing and tuning the application, and the project was on the brink of failure.
Hadoop was suggested by a member of a different team.
After spending a day getting a cluster running on the 20 machines, and running the examples, the team spent a few hours working up a plan for nine map methods and three reduce methods.
The goal was to have each map or reduce method take less than 100 lines of code.
By the end of the first week, our Hadoop-based application was running substantially faster and more reliably than the prior implementation.
The fingerprint step used a third-party library that had a habit of crashing and occasionally taking down the entire machine.
The ease with which Hadoop distributed the application across the cluster, along with the ability to continue to run in the event of individual machine failures, made Hadoop one of my favorite tools.
Both Google and Yahoo handle applications on the petabyte scale with MapReduce clusters.
The Hadoop project provides and supports the development of open source software that supplies a framework for the development of highly scalable distributed computing applications.
The Hadoop framework handles the processing details, leaving developers free to focus on application logic.
And Hadoop happened to be the name of a stuffed yellow elephant owned by the child of the principle architect.
The introduction on the Hadoop project web page (http://hadoop.apache.org/) states:
The Apache Hadoop project develops open-source software for reliable, scalable, distributed computing, including:
Hadoop Core, our flagship sub-project, provides a distributed filesystem (HDFS) and.
HBase builds on Hadoop Core to provide a scalable, distributed database.
Pig is a high-level data-flow language and execution framework for parallel computation.
Distributed applications use ZooKeeper to store and mediate updates for critical shared state.
Hive is a data warehouse infrastructure built on Hadoop Core that provides data summarization, adhoc querying and analysis of datasets.
The Hadoop Core project provides the basic services for building a cloud computing environment with commodity hardware, and the APIs for developing software that will run on that cloud.
The two fundamental pieces of Hadoop Core are the MapReduce framework, the cloud computing environment, and he Hadoop Distributed File System (HDFS)
Note Within the Hadoop Core framework, MapReduce is often referred to as mapred, and HDFS is often referred to as dfs.
The Hadoop Core MapReduce framework requires a shared file system.
This shared file system does not need to be a system-level file system, as long as there is a distributed file system plug-in available to the framework.
In Hadoop JIRA (the issue-tracking system), item 4686 is a tracking ticket to separate HDFS into its own Hadoop project.
Users are also free to use any distributed file system that is visible as a system-mounted file system, such as Network File System (NFS), Global File System (GFS), or Lustre.
When HDFS is used as the shared file system, Hadoop is able to take advantage of knowledge about which node hosts a physical copy of input data, and will attempt to schedule the task that is to read that data, to run on that machine.
This book mainly focuses on using HDFS as the file system.
Hadoop Core MapReduce The Hadoop Distributed File System (HDFS)MapReduce environment provides the user with a sophisticated framework to manage the execution of map and reduce tasks across a cluster of machines.
The user is required to tell the framework the following:
The	location(s)	in	the	distributed	file	system	of	the	job	input.
The	location(s)	in	the	distributed	file	system	for	the	job	output.
The	JAR	file(s)	containing	the	map	and	reduce	functions	and	any	support	classes.
If a job does not need a reduce function, the user does not need to specify a reducer class, and a reduce phase of the job will not be run.
The framework will partition the input, and schedule and execute map tasks across the cluster.
If requested, it will sort the results of the map task and execute the reduce task(s) with the map output.
The final output will be moved to the output directory, and the job status will be reported to the user.
The framework will convert each record of input into a key/value pair, and each pair will be input to the map function once.
The map output is a set of key/value pairs—nominally one pair that is the transformed input pair, but it is perfectly acceptable to output multiple pairs.
The map output pairs are grouped and sorted by key.
The reduce function is called one time for each key, in sort sequence, with the key and the set of values that share that key.
The reduce method may output an arbitrary number of key/value pairs, which are written to the output files in the job output directory.
If the reduce output keys are unchanged from the reduce input keys, the final output will be sorted.
The framework provides two processes that handle the management of MapReduce jobs:
TaskTracker	manages the execution of individual map and reduce tasks on a compute node in the cluster.
JobTracker	accepts job submissions, provides job monitoring and control, and manages the distribution of tasks to the TaskTracker nodes.
Generally, there is one JobTracker process per cluster and one or more TaskTracker processes per node in the cluster.
The JobTracker is a single point of failure, and the JobTracker will work around the failure of individual TaskTracker processes.
Note One very nice feature of the Hadoop Core MapReduce environment is that you can add TaskTracker nodes to a cluster while a job is running and have the job spread out onto the new nodes.
The Hadoop Distributed File System HDFS is a file system that is designed for use for MapReduce jobs that read input in large chunks of input, process it, and write potentially large chunks of output.
For reliability, file data is simply mirrored to multiple storage nodes.
This is referred to as replication in the Hadoop community.
As long as at least one replica of a data chunk is available, the consumer of that data will not know of storage server failures.
NameNode	handles management of the file system metadata, and provides management and control services.
There will be one NameNode process in an HDFS file system, and this is a single point of failure.
Hadoop Core provides recovery and automatic backup of the NameNode, but no hot failover services.
There will be multiple DataNode processes within the cluster, with typically one DataNode process per storage node in a cluster.
Note It is common for a node in a cluster to provide both TaskTracker services and DataNode services.
It is also common for one node to provide the JobTracker and NameNode services.
Installing Hadoop As with all software, you need some prerequisite pieces before you can actually use Hadoop.
It is possible to run and develop Hadoop applications under Windows, provided that Cygwin is installed.
It is strongly suggested that nodes in a production Hadoop cluster run a modern Linux distribution.
Note To use Hadoop, you’ll need a basic working knowledge of Linux and Java.
All the examples in this book are set up for the bash shell.
The Prerequisites The examples in this book were developed with the following:
Hadoop versions prior to 0.18.2 make much less use of generics, and the book examples are unlikely to compile with those versions.
Java versions prior to 1.6 will not support all of the language features that Hadoop Core requires.
In addition, Hadoop Core appears to run most users of other vendors’ JDKs.
I prefer the Red Hat Package Manager (RPM) tool	that	is	used	by	Red	Hat,	Fedora,	and	CentOS,	and	the	examples	reference	RPM-based installation procedures.
The wonderful folks of the Fedora project provide torrents (downloaded with BitTorrent) for most of the Fedora versions at http://torrent.fedoraproject.org/
For those who want to bypass the update process, the people of Fedora Unity provide distributions of Fedora releases that have the updates applied, at http://spins.fedoraunity.org/spins.
The re-spins require the use of the custom download tool Jigdo.
For the novice Linux user who just wants to play around a bit, the Live CD and a USB stick for permanent storage can provide a simple and quick way to boot up a test environment.
For a more sophisticated user, VMware Linux installation images are readily available at http://
The rpm command has options that will tell you which files were in an RPM package: -q to query, -l to list, and -p to specify that the path to package you are querying is the next argument.
Look for the string '/bin/javac$', using the egrep program, which searches for simple regular expressions in its input stream:
If you don’t use quotes, or use double quotes, the shell may try to resolve the $ character as a variable.
This assumes a working directory of ~/Downloads when running the JDK installation program, as the installer unpacks the bundled RPM files in the current working directory.
Add the following two lines to your .bashrc or .bash_profile:
This script assumes you downloaded the RPM installer for the JDK.
This script attempts to work out the installation directory of the jdk, # given the installer file.
Work out the actual installed package name using the rpm command #
Where did the rpm install process place the java compiler program 'javac'
If we found javac, then we can compute the setting for JAVA_HOME.
Run the script in Listing 1-1 to work out the JDK installation directory and update your environment so that the JDK will be used by the examples:
Run a Cygwin bash shell by clicking the icon shown in Figure 1-3
The appropriate setting the Java installation is on your computer.
When a symbolic link is made and the JAVA_HOME set to the symbolic link location, bin/hadoop works well enough to use.
Cygwin maps Windows drive letters to the path /cygdrive/X, where X is the drive letter, and the Cygwin path element separator character is /, compared to Windows use of \
You must keep two views of your files in mind, particularly when running Java programs via the bin/hadoop script.
The bin/hadoop script and all of the Cygwin utilities see a file system that is a subtree of the Windows file system, with the Windows drives mapped to the /cygdrive directory.
In a standard Cygwin installation, the /tmp directory is also the C:\cygwin\tmp directory.
Java will parse /tmp as C:\tmp, a completely different directory.
When you receive File Not Found errors from Windows applications launched from Cygwin, the common problem is that the Windows application (Java being a Windows application) is looking in a different directory than you expect.
Note You will need to customize the Cygwin setup for your system.
The exact details change with different Sun JDK releases and with different Windows installations.
Getting Hadoop Running After you have your Linux or Cygwin under Windows environment set up, you’re ready to download and install Hadoop.
From there, find the tar.gz file of the distribution of your choice, bearing in mind what I said in the introduction, and download that file.
If you are a cautious person, go to the backup site and get the PGP checksum or the MD5 checksum of the download file.
Unpack the tar file in the directory where you would like your test installation installed.
I typically unpack this in a src directory, off my personal home directory:
Add the following two lines to your .bashrc or .bash_profile file and execute them in your.
If you chose a different directory than ~/src, adjust these export statements to reflect your chosen location.
These settings are required to run the examples in this book.
The 'if [' construct you see is a shortcut for 'if test' ....
We are now going to see if a java program and hadoop programs # are in the path, and if they are the ones we are expecting.
If you have installed a different version your results may vary.
Double check that the java in the PATH is the expected version.
Your PATH appears to have the JAVA_HOME java program as the default java.
Your PATH appears to have the HADOOP_HOME hadoop program as the default hadoop.
Running Hadoop Examples and Tests The Hadoop installation provides JAR files with sample programs and tests that you can run.
Before you run these, you should have verified that your installation is complete and that your runtime environment is set up correctly.
Included in the JAR file are the programs listed in Table 2-1
To demonstrate using the Hadoop examples, let’s walk through running the pi program.
The number of samples is the number of points randomly set in the square.
The larger this value, the more accurate the calculation of pi.
For the sake of simplicity, we are going to make a very poor estimate of pi by using very few operations.
The pi program takes two integer arguments: the number of maps and the number of samples per map.
The total number of samples used in the calculation is the number of maps times the number of samples per map.
The reduce task sums the number of inside points and the number of outside points.
The command-line arguments are processed in three steps, with each step consuming some of the command-line arguments.
By default, all output by the framework will have a leading date stamp, a log level, and the name of the class that emitted the message.
In addition, the default is only to emit log messages of level INFO or higher.
For brevity, I’ve removed the data stamp and log level from the output reproduced in this book.
Of	particular interest here is that the last line of output states something of the form “Estimated value of PI is.
In that case, you know that your local installation of Hadoop is ready for you to play with.
Now we will go through the output in Listing 2-3 chunk by chunk, so that you have an understanding of what is going on and can recognize when something is wrong.
The first section is output by the pi estimator as it is setting up the job.
The framework has taken over at this point and sets up input splits (each fragment of input is called an input split) for the map tasks.
The following line provides the job ID, which you could use to refer to this job with the job control tools:
The following lines let you know that there are two input files and two input splits:
The map output key/value pairs are partitioned, and then the partitions are sorted, which is referred to as the shuffle.
The file created for each sorted partition is called a spill.
There will be one spill file for each configured reduce task.
For each reduce task, the framework will pull its spill from the output of each map task, and merge-sort these spills.
In Listing 2-3, the next block provides detailed information on the map task and shuffle process that was run.
The framework is expecting to produce output for one reduce task (numReduceTasks: 1), which receives all of the map task output records.
Also, it expects that the map outputs have been partitioned and sorted and stored in the file system (Finished spill 0)
If there were multiple reduce tasks specified, you would see a Finished spill N for each reduce task.
The rest of the lines primarily have to do with output buffering and may be ignored.
Generated 1 samples is the output of the ending status of the map job.
Listing 2-3 has exactly two map tasks, per your command-line instructions to the task, and one reduce task, per the job design..
With a single reduce task, each map task’s output is placed into a single partition and sorted.
This results in two files, or spills, as input to the framework sort phase.
Each reduce task in a job will have its output go to the output directory and be named part-0N, where N is the ordinal number starting from zero of the reduce task.
The numeric portion of the name is traditionally five digits, with leading zeros as needed.
The next block describes the single reduce task that will be run:
The next block of output provides summary information about the completed job:
The final two lines are printed by the PiEstimator code, not the framework.
Hadoop Tests Hadoop provides a JAR that contains tests hadoop-0.19.0-test.jar, which are primarily for testing the distributed file system or MapReduce jobs on top of the distributed file system.
DFSCIOTest	 	Distributed	I/O	benchmark	of	libhdfs, a shared library that provides HDFS file services for C/C++ applications.
Troubleshooting The issues that can cause problems in running the examples in this book will most likely be due to environment differences.
You may also experience problems if you have space shortages on your computer.
For Windows users, C:\cygwin\bin;C:\cygwin\usr\bin must be added to the system environment Path variable, or the Hadoop Core servers will not start.
You can set this system variable through the System Control Panel.
In the System Properties dialog box, click the Advanced tab, and then click the Environment Variables button.
In the System Variables section of the Environment Variables dialog box, select Path, click Edit, and add the following string:
While not critical, the current working directory for the shell session used for running the.
If you see the message java.lang.OutOfMemoryError: Java heap space in your output,
Summary Hadoop Core provides a robust framework for distributing tasks across large numbers of general-purpose computers.
Application developers just need to write the map and reduce methods for their data, and use one of the existing input and output formats.
The framework provides a rich set of input and output handlers, and you can create custom handlers, if necessary.
Getting over the installation hurdle can be difficult, but it is getting simpler as more people and organizations understand the issues and refine the processes and procedures.
Cloudera (http://www.cloudera.com) now provides a self-installing Hadoop distribution in RPM format.
Read the information on the http:// hadoop.apache.org/core web site, join the mailing lists referenced there (to join the Core user mailing list, send an e-mail to core-user-subscribe@hadoop.apache.org), and have fun writing your applications.
The chapters to come will guide you through the trouble spots as you develop your own applications with Hadoop.
This chapter walks you through what is involved in a MapReduce job.
You will be able to write and run simple stand-alone MapReduce programs by the end of the chapter.
The examples in this chapter assume the setup as described in Chapter 1
They should be explicitly run in a special local mode configuration for executing on a single machine, with no requirements for a running the Hadoop Core framework.
This single machine (local) configuration is also ideal for debugging and for unit tests.
The code for the examples is available from this book’s details page at the Apress web site (http://www.apress.com)
The downloadable code also includes a JAR file you can use to run the examples.
Let’s start by examining the parts that make up a MapReduce job.
The Parts of a Hadoop MapReduce Job The user configures and submits a MapReduce job (or just job for short) to the framework, which will decompose the job into a set of map tasks, shuffles, a sort, and a set of reduce tasks.
The framework will then manage the distribution and execution of the tasks, collect the output, and report the status to the user.
Start of the individual map tasks with their input split Hadoop framework.
Map function, called once for each input key/value pair User.
Shuffle, which partitions and sorts the per-map output Hadoop framework.
Sort, which merge sorts the shuffle output for each partition of all map  Hadoop framework outputs.
Start of the individual reduce tasks, with their input partition Hadoop framework.
Reduce function, which is called once for each unique input key, with all of  User the input values that share that key.
Collection of the output and storage in the configured job output directory,  Hadoop framework in N parts, where N is the number of reduce tasks.
The user is responsible for handling the job setup, specifying the input location(s), specifying the input, and ensuring the input is in the expected format and location.
The framework is responsible for distributing the job among the TaskTracker nodes of the cluster; running the map, shuffle, sort, and reduce phases; placing the output in the output directory; and informing the user of the job-completion status.
All the examples in this chapter are based on the file MapReduceIntro.java, shown in Listing 2-1
The job created by the code in MapReduceIntro.java will read all of its textual input line by line, and sort the lines based on that portion of the line before the first tab character.
If there are no tab characters in the line, the sort will be based on the entire line.
The MapReduceIntro.java file is structured to provide a simple example of configuring and running a MapReduce job.
The records are parsed into Key and Value using the first TAB * character as a separator.
If there is no TAB character the entire * line is the Key.
Construct the job conf object that will be used to submit this job * to the Hadoop framework.
Configure the output of the job to go to the output * directory.
Send the job configuration to the framework and request that the * job be run.
Input Splitting For the framework to be able to distribute pieces of the job to multiple machines, it needs to fragment the input into individual pieces, which can in turn be provided as input to the individual distributed tasks.
The default rules for how input splits are constructed from the actual input files are a combination of configuration parameters and the capabilities of the class that actually reads the input records.
An input split will normally be a contiguous group of records from a single input file, and in this case, there will be at least N input splits, where N is the number of input files.
If the number of requested map tasks is larger than this number, or the individual files are larger than the suggested fragment size, there may be multiple input splits constructed of each input file.
The user has considerable control over the number of input splits.
The number and size of the input splits strongly influence overall job performance.
It is used in jobs that only need to reduce the input, and not transform the raw input.
Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.
See the License for the specific language governing permissions and * limitations under the License.
The magic piece of code is the line output.collect(key, val), which passes a key/value pair back to the framework for further processing.
All map functions must implement the Mapper interface, which guarantees that the map function will always be called with a key.
The key is an instance of a WritableComparable object, a value that is an instance of a Writable object, an output object, and a reporter.
Note The code for the Mapper.java and Reducer.java interfaces is available from this book’s details page at the Apress web site (http://www.apress.com), along with the rest of the downloadable code for this book.
The framework will make one call to your map function for each record in your input.
There will be multiple instances of your map function running, potentially in multiple Java Virtual Machines (JVMs), and potentially on multiple machines.
One common mapper drops the values and passes only the keys forward:
A Simple Reduce Function: IdentityReducer The Hadoop framework calls the reduce function one time for each unique key.
The framework provides the key and the set of values that share that key.
The framework-supplied class IdentityReducer is a simple example that produces one output record for every value.
Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.
See the License for the specific language governing permissions and * limitations under the License.
Performs no reduction, writing all input values directly to the output.
If you require the output of your job to be sorted, the reducer function must pass the key output any number of records, including zero records, with the same key and different values.
This particular constraint is also why the map tasks may be multithreaded, while the reduce tasks are explicitly only single-threaded.
A common reducer drops the values and passes only the keys forward:
Configuring a Job All Hadoop jobs have a driver program that configures the actual MapReduce job and submits it to the Hadoop framework.
The sample class MapReduceIntro provides a walk-through for using the JobConf object to configure and submit a job to the Hadoop framework for execution.
The code relies on a class called MapReduceIntroConfig, shown in Listing 2-4, which ensures that the input and output directories are set up and ready.
A simple class to handle the housekeeping for the MapReduceIntro * example job.
If the directory * /tmp/MapReduceIntroOutput is present, it is removed.
Some simple defaults for the job input and job output.
This is the directory that the framework will look for input files in.
The search is recursive if the entry is a directory.
This is the directory that the job output will be written to.
Ensure that no global file system is required to run this job.
Reduce the in ram sort space, so that the user does not need to * increase the jvm memory size.
This is the standard way to create a file using the Hadoop * Framework.
An error will be thrown if the file already * exists.
It is very important to ensure that file descriptors are * closed.
This method also demonstrates some of the basic APIs for interacting * with file systems and files.
Note: the code has no particular knowledge * of the type of file system.
Determine if a directory has any non zero files in it or its descendant * directories.
This is the standard way to read a directory's contents.
This can be * quite expensive for a large directory.
This method returns null under some circumstances, in particular if * the directory does not exist.
This is needed to know what file *            systems and file system plugins are being used.
It is good practice to pass in a class that is contained in the JAR file that has your map and reduce functions.
This ensures that the framework will make the JAR available to the map and reduce tasks run for your job.
Now that you have a JobConfig object, conf, you need to set the required parameters for the job.
These include the input and output directory locations, the format of the input and output, and the mapper and reducer classes.
All jobs will have a map phase, and the map phase is responsible for handling the job input.
The configuration of the map phase requires you to specify the input locations and the class that will produce the key/value pairs from the input, the mapper class, and potentially, the suggested number of map tasks, map output types, and per-map task threading, as listed in Table 2-2
Class to read and convert the input path elements to key/ Yes value pairs.
Number of threads to run in each map task No 1
Most Hadoop Core jobs have their input as some set of files, and these files are either a textual key/value pair per line or a Hadoop-specific binary file format that provides serialized key/value pairs.
The class that handles the key/value text input is KeyValueTextInputFormat.
The class that handles the Hadoop-specific binary file is SequenceFileInputFormat.
Specifying Input Formats The Hadoop framework provides a large variety of input formats.
The major distinctions are between textual input formats and binary input formats.
TextInputFormant: The key is the line number, and the value is the line.
NLineInputFormat: Similar to KeyValueTextInputFormat, but the splits are based on N lines of input rather than Y bytes of input.
MultiFileInputFormat: An abstract class that lets the user implement an input format that aggregates multiple files into one split.
SequenceFIleInputFormat: The input file is a Hadoop sequence file, containing serialized key/value pairs.
KeyValueTextInputFormat and SequenceFileInputFormat are the most commonly used input formats.
The examples in this chapter use KeyValueTextInputFormat, as the input files are human-readable.
The following block of code informs the framework of the type and location of the job input:
The line conf.setInputFormat(KeyValueTextInputFormat.class) informs the framework that all of the files used for input will be textual key/value pairs, one per line.
The KeyValueTextInputFormat format reads a text file and splits it into records, one record per line.
The records are further divided into key/value pairs by splitting the line at the first tab character.
If there is no tab character in the line, the entire line is the key, and the value object will contain a zero-length string.
There is no way to distinguish an input line that contains a single tab as the last character and the same line without a trailing tab character.
Suppose that an input file has the following three lines, where TAB is replaced by an US-ASCII horizontal tab character (0x09):
Your mapper would be called with the following key/value pairs:
The actual order in which the keys are passed to your map function is indeterminate.
In a real-world example, the actual machine that ran the map that got a given key would be indeterminate.
It is very likely, however, that sets of contiguous records in the input will be processed by the same map task, as each task is given one input split from which to work.
The input bytes are considered to be in the UTF-8 character set.
As of Hadoop 0.18.2, there is no configurable way to change the character set interpretation of the input files handled by the KeyValueTextInputFormat class.
Now that the framework knows where to look for the input files and the class to use to generate key/value pairs from the input files, you need to inform the framework which map function to use.
Note  The simple example in this chapter does not use the optional configuration parameters.
If the map function needs to output a different key or value class than the job output, those classes may be set here.
This is ideal if the map function is not able to fully utilize the resources allocated for the map task.
A simple case of where this might be beneficial is a map task that performs DNS lookups on the IP addresses in a server log.
Setting the Output Parameters The framework requires that the output parameters be configured, even if the job will not produce any output.
The framework will collect the output from the specified tasks (either the output of the map tasks for a MapReduce job that did not include reduce tasks or the output of the job’s reduce tasks) and place them into the configured output directory.
To avoid issues with file name collisions when placing the task output into the output directory, the framework requires that the output directory not exist when you start the job.
In our simple example, the MapReduceIntroConfig class handles ensuring that the output directory does not exist and provides the path to the output directory.
The output parameters are actually a little more comprehensive than just the setting of the output path.
The code will also set the output format and the output key and value classes.
The Text class is the functional equivalent of a String.
It implements the WritableComparable interface, which is necessary for keys, and the Writable interface (which is actually a subset of WritableComparable), which is necessary for values.
Unlike String, Text is mutable, and the Text class has some explicit methods for UTF-8 byte handling.
The key feature of a Writable is that the framework knows how to serialize and deserialize a Writable object.
The WritableComparable adds the compareTo interface so the framework knows how to sort the WritableComparable objects.
The following code block provides an example of the minimum required configuration for the output of a MapReduce job:
Configure the output of the job to go to the output directory.
The TextOutput format class produces a record of * output for each Key,Value pair, with the following format.
The setting is familiar from the input example discussed earlier in the chapter.
These settings inform the framework of the types of the key/value pairs to expect for the reduce phase.
By default, these classes will also be used to set the values the framework will expect from the map output.
Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.
See the License for the specific language governing permissions and * limitations under the License.
Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.
See the License for the specific language governing permissions and * limitations under the License.
Configuring the Reduce Phase To configure the reduce phase, the user must supply the framework with five pieces of information:
The	number	of	reduce	tasks;	if	zero,	no	reduce	phase	is	run.
The	input	key	and	value	types	for	the	reduce	task;	by	default,	the	same	as	the	reduce output.
The	output	key	and	value	types	for	the	reduce	task.
Here, we will look at setting the number of reduce tasks and the reducer class.
The configured number of reduce tasks determines the number of output files for a job that will run the reduce phase.
Tuning this value will have a significant impact on the overall performance of your job.
The time spent sorting the keys for each output file is a function of the number of keys.
In addition, the number of reduce tasks determines the maximum number of reduce tasks that can be run in parallel.
The framework generally has a default number of reduce tasks configured.
This value is set by the mapred.reduce.tasks parameter, which defaults to 1
This will result in a single output file containing all of the output keys, in sorted order.
There will be one reduce task, run on a single machine that processes every key.
The number of reduce tasks is commonly set in the configuration phase of a job.
In general, unless there is a significant need for a single output file, the number of reduce tasks is set to roughly the number of simultaneous execution slots in the cluster.
In Chapter 9, the class DataJoinReduceOutput is provided as a sample for efficiently merging multiple reduce task outputs into a single sorted file.
A typical cluster is composed of M TaskTracker machines, with C CPUs, each of which supports T threads.
In my environment, the machines typically have eight CPUs that support one thread per CPU, and a small cluster might have ten TaskTracker machines.
If your tasks tend not to be CPU-bound, you may adjust the number of execution slots configured to optimize the CPU utilization on your TaskTracker machines.
The configuration parameter mapred.tasktracker.map.tasks.maximum controls the maximum number of map tasks that will be run simultaneously on a TaskTracker node.
The configuration parameter mapred.tasktracker.reduce.tasks.maximum controls the maximum number of reduce tasks that will be run simultaneously on a TaskTracker node.
This requires tuning on a per-job basis and is a weakness in Hadoop at present, as the maximum values are not per-job configurable and instead require a cluster restart.
The reducer class needs to be set only if the number of reduce tasks is not zero.
It is very common to not need a reducer, since frequently you do not require sorted output or value grouping by key.
The framework relies on the output parameters being set correctly.
One of the more common errors is to have each reduce task fail with an exception of the form:
This error indicates that output key class has been defaulted by the framework, or was set incorrectly during the job configuration.
Or if your map output is not the same as your job output, use this form:
This error may occur for the value class as well:
Running a Job The ultimate aim of all your MapReduce job configuration is to actually run that job.
The MapReduceIntro.java example (Listing 2-1) demonstrates a common and simple way to run a job:
Send the job configuration to the framework * and request that the job be run.
The RunningJob class provides a number of methods for examining the response.
Run MapReduceIntro.java as follows (using the CH2.jar file provided with this book’s downloadable code):
The single output file of the reduce task in the file /tmp/MapReduceIntroOutput/part-00000
The first thing you will notice is that the numbers don’t seem to be in order.
The code that generates the input produces a random number for the key of each line, but the example tells the framework that the keys are Text.
Therefore, the numbers have been sorted as text rather than as numbers.
Creating a Custom Mapper and Reducer As you’ve seen, your first Hadoop job, in MapReduceIntro, produced sorted output, but the sorting was not suitable, as it sorted lexically rather than numerically, and the keys for the job were numbers.
Now, let’s work out what is required to sort numerically, using a custom mapper.
Then we’ll look at a custom reducer that outputs the values in a format that is easy to parse.
Let’s try making the output key class a LongWritable, another class supplied by the framework:
As you can see, just changing the output key class was insufficient.
If you are going to change the output key class to a LongWritable, you also need to modify the map function so that it outputs LongWritable keys.
For the job to actually produce output that is sorted numerically, you must change the job configuration and provide a custom mapper class.
This is done by two calls on the JobConf object:
This class is identical to MapReduceIntro, except for these two replacement method calls.
Note The job configuration could also provide a custom sort option.
One way to do this is to provide a JobConf object.
An example of implementing a custom comparator is provided in Chapter 9
You also need to provide a mapper class that performs the transformation.
The TransformKeysToLongMapper.java class file has a number of changes from the IdentityMapper class (shown earlier in Listing 2-2)
First, the class declaration is no longer generic; the types have been made concrete:
Transform the input Text, Text key value * pairs into LongWritable, Text key/value pairs.
Notice that the code actually provides the types for the key/value pairs for input and for output.
The reporter object provides a mechanism for informing the framework of the current status of your job.
Each call on the reporter object or the output collector provides a heartbeat to the framework, informing it that the task is not deadlocked or otherwise unresponsive.
If your map or reduce method takes substantial time, the method must make periodic calls on the reporter.
The framework will kill tasks that have not reported in 600 seconds by default.
Listing 2-6 shows the body of the TransformKeysToLongMapper mapper that uses the reporter object.
This is a somewhat expected case and we handle it specially.
In particular it is very handy to report the number of exceptions.
If this is done, the driver can make better assumptions * on the success or failure of the job.
This block of code introduces a new object, reporter, and some best practice patterns.
The key piece of this is the transformation of the Text key to a LongWritable key.
The code in Listing 2-6 is sufficient to perform the transformation, and also includes some additional code for tracking and reporting.
If the job is configured to multithread the map method, via conf.setMapRunner(Multithreaded MapRunner.class), the map method will be called by multiple threads.
Extreme care must be taken in using the mapper class member variables.
A ThreadLocal LongWritable object could be used to ensure thread safety.
Object churn is a significant performance issue in a map method, and to a lesser extent, in the reduce method.
It is a good practice to wrap your map and reduce methods in a try block that catches Throwables and reports on the catches.
The JobTracker, the Hadoop Core server process that manages job execution on the cluster, accumulates the counter values and provides a final count in the job output, as well.
This interface will be discussed in more detail in Chapter 6, which covers the setup of a multimachine cluster.
The first catch block handles exceptions related to key transformation:
This is a somewhat expected case and we handle it specially.
You expect that some of the keys may not convert correctly into Long values, so you counter in the Input group, of the name number format, by 1
If the counter does not already exist, it will be created.
In the sample input, there are no records that will cause a number format exception.
The only counters that are accumulated are Input.total records and Input.parsed records.
These two counters will show up in the job output as part of the Input group:
If one or more keys caused an exception during the conversion to Long, the output might look more like this:
Note The sum of the parsed records and the number formats should equal the total records.
The counters are also available via the RunningJob object, allowing for a more comprehensive check of the success status.
The totals for your job will vary from this example.
After the Job Finishes Once the job finishes, the framework will provide you with a filled-out RunningJob object.
This object has information about the framework’s opinion on the success status of your job via the unable to complete any single map task or if the job was killed.
This generally doesn’t provide enough information to make a determination on the actual success.
It may be that there was an exception in the map or method for every key or for most keys.
If the map or reduce function provides job counters for these cases, your job driver will be able to make a better determination regarding the actual success or failure of your job.
In the sample mapper, several counters were collected under different circumstances:
The map task potentially outputs 4 counters in the input group.
Now that the job driver has the counters issued by the map method, a much more accurate determination of success can be made.
In one of my production clusters, a TaskTracker node was incorrectly configured.
The result of this misconfiguration was that none of the computationally intense work could be run in the map task, and the map method would return immediately with an exception.
As far as the framework was concerned, this machine was super fast, and it scheduled almost all of the map tasks on this machine.
The job was successful as far as the framework was concerned, but totally unsuccessful per the business rules.
Save yourself much embarrassment—collect information about the successes and failures in the mapper and reducer objects and check those results in your job driver.
Was this Job really Successful? The check for success primarily involves ensuring that the number of records output is roughly the same as the number of records input.
Hadoop jobs are generally dealing with bulk realworld data, which is never 100% clean, so a small error rate is generally acceptable.
Log an error for each type of exception with the count.
In this particular case, you would expect a small number of NumberFormatExceptions but no other exceptions.
If the total number of input records is roughly the number of parsed input records, and you have no unexpected exceptions, this job is a success.
Creating a Custom Reducer The reduce method is called once for each key, and passes the key and an iterator to all of the map output values that share that key.
The reduce task is an ideal place for summarizing data and for doing basic duplicate suppression.
Note For managing duplicate suppression against a prior seen set, it is usually best to keep the prior seen set in either HBase (the Hadoop database) or in a sorted format, such as a Hadoop map file.
If this is not done, then the dataset of seen records and the dataset of input records must be merged and sorted, which can take considerable time if either dataset is large.
In the HBase case, if the input data is already sorted, the duplicate status of an input record can be rapidly determined.
With a simple sorted seen set, map-side joins may be performed.
For the sample custom reducer, let’s merge the values into a comma-separated values (CSV) form, so you have one output line per key, with all of the values in a simple-to-parse format.
After your work with the custom mapper in the preceding sections, creating a custom reducer will seem familiar.
This version is in MapReduceIntroLongWritableReduce.java, which is based on MapReduceIntroLongWritableCorrect.java.
First, the framework needs to be informed of the reducer class.
The key piece is, as usual, to inform the framework of the reducer class, so add the following single line:
There have been no changes to the output classes, so no other changes are required to MapReduceIntroLongWritableCorrect.java.
As with the mapper example, TransformKeysToLongMapper, you start with your class declaration, which has partially specified the generic types:
The reduce method doesn’t need to know the incoming value class; it requires only the and for simplicity’s sake, given this transformation, the output value is declared to be Text.
The actual method declaration also has the same type specification:
Merge the values for each key into a CSV text string.
The framework will throw an error if the job is expecting a different output value type than Text.
As with the mapper example, you have a method body that employs the reporter.
As a performance optimization, to reduce object churn, two class fields are declared.
The buffer object is used to build the CSV-style line for the output, and mergedValue is the fields, rather than as local variables, because the individual reduce tasks are run only as single threads by the framework.
Note There may be multiple reduce tasks running simultaneously, but each task is running in a separate JVM, and the JVMs are potentially running on separate physical machines.
Recall that, ideally, a reduce task will make no changes to the key, and will use that key as boilerplate for the object churn optimizations to reset the StringBuilder object, and a loop to process each of the values for this key:
The rest of the preceding code block simply builds a comma-separated list of values, with Excel-style CSV quoting.
The actual output block must build a new value for the output.
In this case, a class field mergedValue will be used.
In a larger job, there may be a billion keys passed through the In this example, there are also counters for the output records:
This example uses Text as the output value class; it is acceptable to use any Writable as the output method on your object.
This class provides basic implementations of two additional methods that are required of a input split:
It is very common for the developer to have a JobConf member variable, which would be initialized in this method with the passed-in JobConf object.
Keep track of the maximum number of keys a value had.
Report it in the counters so that per task counters can be examined as needed * and set the task status to include this maximum count.
The reporter field was made a class instance field, via protected Reporter reporter, and of values is kept in valueCount, and if it’s larger than the instance member field, maxValueCount, maxValueCount is set to it.
This enables you to output the maximum number of values that shared a specific key.
In this case, the overall summary value is not particularly useful, as that value is the sum of all of the maximum values, but the per-task value is interesting and available via the web interface.
A more useful solution would be to maintain an additional output file and output the key/value counts into that file.
When you select a completed or running task through the web interface (which is on port 50030 on the machine running the JobTracker, by default), you are presented with the counter summary for the job and links to detailed information about the map and reduce tasks.
Each map and reduce task will have a link to the counters.
Using a Custom Partitioner By default, the framework partitions your output based on the hash value of the key, using the HashPartitioner class.
There are times when you need your output data partitioned differently.
The standard example is a single output file where multiple output files would usually.
If you need different partitioning, you have the option of setting a partitioner.
Some simple partitioner concepts could be to sort into odd/even or, if the minimum and maximum key values are known, to sort into key rangebased buckets.
When the framework is performing the shuffle, each key output by the mapper is examined, and the following operation is performed:
The value partitions is the number of reduce tasks to perform.
The key, if actually output by the reducer, will end up in the output file part partition, with an appropriate number of leading zeros so that the file names are all the same length.
The partitioner interface is very simple, as shown in Listing 2-7
The key (or a subset of the key) is used to derive * the partition, typically by a hash function.
The total number of partitions * is the same as the number of reduce tasks for the job.
Get the paritition number for a given key (hence record) given the total * number of partitions i.e.
You now have a basic understanding of the JobConf object and how to use it to inform the framework of the requirements for your jobs.
You’ve seen how to write mapper and reducer classes, and how the reporter object is one of your best friends, because of the wonderful information it can provide about what is happening during the execution of your jobs.
Output partitions finally make sense, and you have a sense of when and why you configure your job to reduce, and how many reducers you will use.
As a brilliant Hadoop expert, you are totally prepared to inform people of why the files they open in mapper or reducer classes are empty or short, because you know you need to close files before the framework will flush the last file system block size worth of data to disk.
In the next chapter, you’ll learn how to set up of a multimachine cluster.
This chapter explains how to set up a multimachine cluster.
You’ll learn about the makeup of a cluster, the tools for managing clusters, and how to configure a cluster.
Here, we’ll walkthrough a simple cluster configuration, using the minimum HDFS setup necessary to bring up the cluster.
Chapter 4 will go into the details for a high-usage HDFS.
The Makeup of a Cluster A typical Hadoop Core cluster is made up of machines running a set of cooperating server processes.
The machines in the cluster are not required to be homogeneous, and commonly they are not.
The cluster machines may even have different CPU architectures and operating systems.
But if the machines have similar processing power, memory, and disk bandwidth, cluster administration is a lot easier, because in that case, only one set of configuration files and runtime environments needs to be maintained and distributed.
A cluster will have one JobTracker server, one NameNode server, and one secondary NameNode server, and DataNodes and TaskTrackers.
The JobTracker coordinates the activities of the TaskTrackers, and the NameNode manages the DataNodes.
In the context of Hadoop, a node/machine running the TaskTracker or DataNode server is considered a slave node.
It is common to have nodes that run both the TaskTracker and DataNode servers.
The Hadoop server processes on the slave nodes are controlled by their respective masters, the JobTracker and NameNode servers.
Manages metadata: (file names, file blocks, block locations, open files) and DadaNodes.
Provides a backup for the NameNode data and manages file system.
Let’s look at each of the server processes run by the machines in a cluster:
JobTracker: The JobTracker provides command and control for job management.
It supplies the primary user interface to a MapReduce cluster.
There is one instance of this server running on a cluster.
The machine running the JobTracker server is the MapReduce master.
TaskTracker: The TaskTracker provides execution services for the submitted jobs.
Each TaskTracker manages the execution of tasks on an individual compute node in the MapReduce cluster.
There is one instance of this server per compute node.
Note If your MapReduce jobs utilize external packages or services, it is very important that these external version or are absent, unexpected and difficult-to-diagnose errors occur.
NameNode: The NameNode provides metadata storage for the shared file system.
The NameNode supplies the primary user interface to the HDFS.
It also manages all of the metadata for the HDFS.
There is one instance of this server running on a cluster.
The metadata includes such critical information as the file directory structure and which DataNodes have copies of the data blocks that contain each file’s data.
The machine running the NameNode server process is the HDFS master.
Secondary NameNode: The secondary NameNode provides both file system metadata backup and metadata compaction.
It supplies near real-time backup of the metadata for the NameNode.
There is at least one instance of this server running on a cluster, ideally on a separate physical machine than the one running the NameNode.
The secondary NameNode also merges the metadata change history, the edit log, into the NameNode’s file system image.
Real-time backup of the NameNode data: Many installations configure the NameNode to store the file system metadata to multiple locations, where at least one of these locations resides on a separate physical machine.
Other installations use a tool such as DRBD (http://www.drbd.org/) to replicate the host file system in near real time to a separate physical machine.
DataNode: The DataNode provides data storage services for the shared file system.
The NameNode coordinates the storage and retrieval of the individual data blocks managed by a DataNode.
There is one instance of this server process per HDFS storage node.
Balancer: During normal usage, the disk utilization on the DataNode machines may become uneven.
This is particularly common if some DataNodes have less disk space available for use by HDFS.
The Balancer moves data blocks between DataNodes to even out the per-DataNode available disk space.
The Balancer will also rebalance the cluster as new DataNodes are added to an existing cluster.
It must be run by the user via the command bin/hadoop balancer [-threshold utilization between DataNodes for the cluster to be considered balanced.
As of Hadoop 0.19.0, this is not a configuration parameter.
These server processes are typically started once per cluster instance.
DataNodes and TaskTrackers may be dynamically added and removed from a running cluster, as described in the next section.
All of these servers are implemented in Java and require at least Java version 1.6
Cluster Administration Tools The Hadoop Core installation provides a number of scripts in the bin subdirectory of the installation that are used to stop and start the entire cluster or various pieces of the cluster.
There are also administrative scripts for the Hadoop Core servers.
The administrator has the option of starting or stopping the full set of Hadoop Core servers with the start-all.sh and stop-all.sh scripts.
These scripts start all of the server processes on the cluster machines.
The NameNode and JobTracker will be started or stopped on the machine on which the script is run, and DataNodes and TaskTracker nodes will be started on the configured slave machines.
Any requested secondary NameNodes will also be started on configured machines.
These scripts start stop-mapred.sh  or stop only the JobTracker and TaskTracker nodes.
The Balancer is expected to run on the stop-balancer.sh machine on which these scripts are executed.
The preceding start and stop scripts actually use the hadoop-daemon.sh script to start or stop the servers.
This script is used by the start and stop scripts to start the DataNodes, TaskTrackers, and secondary NameNodes.
This script will use the hadoop-daemon.sh script to start the servers on each specific machine in the set of machines on which it operates.
This feature of Hadoop Core is expected to be discontinued.
As of Hadoop 0.17, the Serialization service is the preferred method for handling external data structures.
Cluster Configuration The Hadoop Core servers load their configuration from files in the conf directory of your Hadoop Core installation.
As a general rule, identical copies of the configuration files are maintained in the conf directory of every machine in the cluster.
It is also common to have the NameNode and the JobTracker servers on the same node, especially in smaller installations.
Note Many difficult-to-diagnose problems occur when the configuration files or the supporting runtime environments differ between TaskTracker nodes.
Hadoop Configuration Files The configuration files fall into the following groups:
Hadoop Core configuration: The Hadoop Core is configured by two XML files: hadoop-default.xml and hadoop-site.xml.
In this chapter, we will walk through constructing a hadoop-site.xml file for a small cluster.
Slaves and masters: Two files are used by the startup and shutdown commands discussed in the previous section to start and stop the DataNode, TaskTracker, and secondary NameNode servers.
The slaves file contains a list of hosts, one per line, that are to host DataNode and TaskTracker servers.
The masters file contains a list of hosts, one per line, that are to host secondary NameNode servers.
If the start-all.sh script is used, a DataNode and TaskTracker will be started on each host in the slaves file, and a secondary NameNode will be started on each host in the masters file.
If the start-dfs.sh script is used, DataNodes will be started on the hosts listed in slaves, and secondary NameNodes will be started on the hosts listed in masters.
Per-process runtime environment: The file hadoop-env.sh is responsible for tailoring the per-process environment.
In particular, it includes the JAVA_HOME environment variable, which provides the JVM installation location.
This file also offers a way to provide custom parameters for each of the servers.
Reporting: Hadoop Core may be configured to report detailed information about the activities on the cluster.
Hadoop Core may report to a file, via Ganglia (http://ganglia.info/), which provides a framework for displaying graphical reports summarizing the activities of large clusters of machines.
Hadoop Core Server Configuration The hadoop-default.xml file defines more than 150 parameters, divided into six groups:
Note Some of the parameters listed in hadoop-default.xml may be modified on a per-job basis by setting alternate values using the JobConf.set* methods.
The administrator may specify that a parameter is final by adding <final>true</final> to the parameter’s declaration in the hadoop-site.xml file.
In general, the modification of the server configuration parameters by a job have no effect on the servers.
It is customary to consider the hadoop-default.xml file to be read-only, and to make changes only to the hadoop-site.xml file.
The framework will load configuration files in order, with the values defined in later files superseding those earlier definitions.
The loading order is hadoop-default.xml, hadoop-site.xml, and then any user specified resources.
Note Values that have a ${text} are replaced with the system property value or a previously defined value.
The search order is system properties, then previously defined values.
Three critical parameters must be configured for any Hadoop cluster: hadoop.tmp.dir, fs.default.name, and mapred.job.tracker.
Several other parameters are important to tune but not critical: mapred.tasktracker.map.tasks.maximum, mapred.tasktracker.reduce.tasks.
The default values for these parameters are suitable for single-machine, single-CPU temporary use only.
The following sections discuss the minimum set for cluster configuration.
Note The hadoop-default.xml file includes some documentation on the parameters that control a cluster and a job, although some configuration parameters are not documented here.
It informs the framework of the directory to use for all Hadoop Core server data storage, as follows:
The	NameNode	will	store	file	system	metadata	in	a	subdirectory.
The	secondary	NameNode	will	store	the	backup	metadata	in	a	subdirectory.
A high-performance cluster will have values tailored to minimize I/O contention on individual devices.
To maximize performance, the I/O for these various functions needs to be distributed over multiple devices.
This may be a comma- or space-separated list of directories.
This is of critical importance and should be stored on a low-latency device with redundancy.
This directory will experience bulk I/O that has a short life.
This may be a comma- or space-separated list of directories.
By default, HDFS replicates data storage blocks to multiple DataNodes.
This may be a comma-separated list of directories, preferably on different devices.
I/O will be spread among the directories for increased performance.
This directory will also experience bulk I/O that has a short life.
This value must be unique per JobTracker if multiple MapReduce clusters share a single HDFS.
The cluster administrator must pick a location or locations for these directories that provide the required I/O performance and the required reliability.
The HDFS-level redundant block storage reduces the requirements for highly reliable block storage for individual DataNodes.
The directory specified by the dfs.data.dir parameter will experience bulk I/O transactions and should be optimized for maximum speed.
There will be a large number of files and directories created, each file being an HDFS data block.
If you don’t change this default value for ${hadoop.tmp.dir}, the HDFS data will be stored in /tmp and deleted by the system /tmp cleaning service.
This causes interesting chaos for users when their HDFS file’s data blocks start vanishing about a week after the file was created.
The framework will attempt to create the hadoop.tmp.dir directory and all of the subdirectories if they do not exist.
The user that the relevant server processes are running as must have the required permissions to be able to create these directories and to add and remove files from them.
This configuration assumes that HDFS is doing redundant block storage at the HDFS level.
If it is not configured, there is no shared file system.
The URI specified here informs the Hadoop Core framework of the default file system.
The default value is file:///, which instructs the framework to use the local file system.
The file system protocol is hdfs, the host to contact for services is NamenodeHost, and the port to connect to is 8020, which is the default port for HDFS.
If the default 8020 port is used, the URI may be simplified as hdfs://NamenodeHost/
You can choose an arbitrary port for the hdfs NameNode.
Jobtracker host and port The mapred.job.tracker parameter is critical to configure.
If it is not configured, only a single machine will be used for task execution The URI specified in this parameter informs the Hadoop Core framework of the JobTracker’s location.
The default value is local, which indicates that no JobTracker server is to be run, and all tasks will be run from a single JVM.
The JobtrackerHost is the host on which the JobTracker server process will be run.
Maximum Concurrent Map tasks per tasktracker The mapred.tasktracker.map.tasks.maximum parameter sets the maximum number of map tasks that may be run by a TaskTracker server process on a host at one time.
This value is read only when the TaskTracker is started.
Changes made by a job will not be honored or persist.
This parameter should be tuned to ensure that the CPU resources of the TaskTracker nodes are fully utilized.
If the machine hosts only the TaskTracker, it is common to set.
This may result in a large memory footprint, as each of the JVMs executing tasks will have a full memory allocation.
Many administrators set this value to 1 and require that individual jobs specify that the class MultiThreadedMapRunner is to be used via the JobConf.setMapRunner(MultiThreadedMap Runner.class) method, and specify the number of threads to use per map task.
This latter choice is preferred, as the number of threads may be set on a per-job basis, allowing the job to customize its CPU consumption.
The following sample snippet demonstrates a common pattern for per-job management of map task parallelism.
The choice of 100 was made for demonstration purposes and is not suitable for a CPU-intensive map task.
Maximum Concurrent reduce tasks per tasktracker The mapred.tasktracker.reduce.tasks.maximum parameter sets the maximum number of reduce tasks that may be run by an individual TaskTracker server at one time.
Unlike in a map task, the output key ordering is critical for a reduce tasks, which precludes running multithreaded reduce tasks.
This value also determines the number of parts in which your job output is placed.
The default value, 2, is specified in the conf/hadoop-default.xml file.
Changes made by a job will not be honored or persist.
A significant and unexpected influence on this is the heap requirements (io.sort.mb), which by default will cause 100MB of space to be used for sorting.
During the run phase of a job, there may be up to mapred.tasktracker.map.tasks.maximum map tasks and mapred.tasktracker.reduce.tasks.maximum reduce tasks running simultaneously on each TaskTracker node, as well as the TaskTracker JVM.
The node must have sufficient virtual memory to meet the memory requirements of all of the JVMs.
JVMs have non-heap memory requirements; for simplicity, 20MB is assumed.
This 5GB value does not include memory for other processes or servers that may be running on the node.
The default location of these additional options is the bottom-left corner of the page (so you usually need to scroll down the page to see them)
A Sample Cluster Configuration In this section, we will walk through a simple configuration of a six-node Hadoop cluster.
The standard machine configuration is usually an eight-CPU machine with 8GB of RAM, and a hardware RAID controller presenting a single partition to the operating system.
The single partition presentation is not ideal for Hadoop Core, as there is no opportunity to segregate the I/O for MapReduce and for HDFS.
Configuration Requirements This configuration will require the customization of several files in the conf directory and compliance with some simple network requirements.
The user that will own the Hadoop processes must also be determined.
Network requirements Hadoop Core uses Secure Shell (SSH) to launch the server processes on the slave nodes.
Hadoop Core requires that passwordless SSH work between the master machines and all of the slave and secondary machines.
For example, for OpenSSH, you can generate an unencrypted key for the user that will own the Hadoop Core server processes.
The user will need to have a directory, ~/.ssh, that only the user has access permissions for, on all machines in the cluster.
The following command generates a dsa key with an empty password in the file ~/.ssh/id_dsa:
The quotes in the command are a pair of single quote characters, side by side.
Execute the command chmod og-rwx ~/.ssh on each machine in the cluster.
You should now be able to run bin/slaves.sh uptime and receive the output of uptime.
These slave servers will need to contact their specific master (either the NameNode or the JobTracker), and this will require that several ports in the low 50000 range be unblocked and available.
Table 3-3 lists the default ports that must be unfiltered and available.
Note Hadoop Core uses a number of TCP ports in the low 50000 range.
If other applications, such as Squid, are also using ports in this range, difficult-to-diagnose problems may occur.
The IP address of this interface will be advertised by the DataNode as its contact address.
An example of a more complex network setup is for the machines in the Amazon cloud.
Each Amazon cloud machine has two network interfaces: one for Internet access and one for intracloud traffic.
It is helpful to set dfs.datanode.dns.interface to the name of the intracloud network interface.
Machine Configuration requirements In the simplest case, all of the machines in the cluster will be identically configured.
They will have the same number of CPUs, the same amount of physical RAM, and the same disk capacity and configuration, with the same file system mount points.
The same version of the JVM will be installed in the same location, and the Hadoop installation directory will be the same on all of the machines.
Hadoop Core maintains process ID files in the directory /tmp, by default.
This directory must exist and be writable by the user that will own the Hadoop server processes.
This directory is configurable by editing the conf/hadoop-env.sh file and uncommenting and optionally altering the setting for the environment variable HADOOP_PID_DIR.
Caution Remember that the parent of the directory to be used for hadoop.tmp.dir must exist and be writable by the Hadoop server user, or the hadoop.tmp.dir directory must exist and be writable by the Hadoop server user.
Configuration Files for the Sample Cluster The examples provided in this section were run using the VMware images provided by Cloudera as part of its boot camp (http://www.cloudera.com/hadoop-training-basic)
The source code for this file is provided with the rest of this book’s downloadable code.
The partition /hadoop is used as the value for hadoop.tmp.dir, and is assumed to be on the hardware RAID partition.
The port of 8020 is implicit in the protocol declaration.
The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class.
The uri's authority is used to determine the host, port, etc.
The value for mapred.job.tracker is not interpreted as a URI, but rather as a host:port pair.
If "local", then jobs are run in-process as a single map and reduce task.
For setting the maximum number of map and reduce tasks per TaskTracker, several assumptions are made.
The first assumption is that the map tasks will be threaded, and the individual jobs will choose a thread count that optimizes CPU utilization.
The DataNode also will run on the same machine; therefore, you must budget for CPU and memory resources.
In the best of all worlds, the DataNode would use a different set of disks for I/O than the TaskTracker.
It is possible that the reduce tasks jobs are CPU-bound, but generally, the shuffle phase is CPU-bound and the reduce phase is I/O bound.
In a high-performance cluster, these parameters will be carefully tuned for specific jobs.
Very few MapReduce jobs will run in the 200MB default heap size specified for the JVMs.
To alter this to a more reasonable default, mapred.child.java.opts is set to –Xmx512m -server.
The –server configures the JVM for the HotSpot JVM and provides other performance optimizations.
The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID.
Leave this unchanged from the default as io.sort.mb has been reduced for our test purposes.
Finally, webinterface.private.actions is set to true, the recommended value for any cluster that doesn’t require significant security.
Enable this option if the interfaces are only reachable by those who have the right authorization.
Enable this option if at all possible as it greatly simplifies debugging.
The default configuration contains a single line containing localhost, which provides no real protection from machine or disk failure.
It is a wise precaution to have a secondary NameNode on another machine.
For this simple configuration example, the masters file has a single line with slave01
Installation and use of Ganglia are covered in Chapter 8
Distributing the Configuration One of the reasons for requiring that all of the machines be essentially identical is that this greatly simplifies managing the cluster configuration.
All of the core configuration files can be identical, which allows the use of the Unix rsync command to distribute the configuration files.
The command I like to use assumes that HADOOP_HOME is set correctly:
Note This command assumes that the current machine is not listed in the slaves or masters file.
It’s a very good test of the passwordless SSH configuration.
This ensures that all servers are running the same configuration files and the same Hadoop JARs.
The -e ssh forces rsync to use SSH to establish the remote machine connections.
If your installation requires different configuration files on a per-machine basis, some other mechanism will be required to ensure consistency and correctness for the Hadoop installations and configuration files.
Verifying the Cluster Configuration You should take a few steps to verify that that the cluster is installed and configured properly.
At this point, you can assume that the Hadoop installation has been replicated to the same location across the cluster machines and that passwordless SSH is working.
So, you should check the location of the JVM and make sure that HADOOP_PID_DIR can be written.
To verify that the JVM is in place and that JAVA_HOME is set correctly, run the following commands from the master machine:
Every slave machine, as well as the local machine, should have an output line.
If Java is not available on a machine in the expected location, install the JVM on that machine and set the JAVA_HOME environment variable to reflect the JVM installation directory.
The next item to verify is that the various required paths exist with the proper permissions or that the proper paths can be created.
There are two directories to check: the directory specified as the hadoop.tmp.dir, in this case /hadoop, and the directory specified in the conf/hadoop.env.sh script for HADOOP_PID_DIR, in this case /var/hadoop/pids.
Caution All of the commands in this section must be run from the master machine and as the user that will own the cluster servers.
Failure to do this will invalidate the verification process and may cause the cluster startup to fail in complex ways.
The shell environment is also expected to be set up, as detailed in Chapter 2, such that both java and the bin directory of the Hadoop installation are the first two components of the PATH environment variable.
The user that owns the cluster processes will be referred to in some of the following text as CLUSTER_USER.
Execute the following command to verify that HADOOP_PID_DIR and hadmp.tmp.dir are writable.
Any error message about being unable to create the file dummy must be corrected before the next step is attempted.
At this point, you have checked that the cluster installation and configuration are correct, and that passwordless SSH is enabled.
It is time to format the HDFS file system and to start the HDFS and MapReduce services.
Formatting HDFS The command to format HDFS is very simple.
If there has been an existing HDFS file system with the same hadoop.tmp.dir, it is best to remove all traces of it with the Unix rm command before formatting a new file system.
It may be started with the MapReduce portion of the cluster via start-all.sh.
Here, I’ll show you how to start HDFS separately to demonstrate the commands, as it is common to separate the HDFS master and configuration from the MapReduce master and configuration.
Your output should look similar to the preceding sample output.
Some common reasons for failures are listed in the next section.
After roughly one minute, allowing the DataNodes time to start and connect to the NameNode, issue the command that reports on the status of the DataNodes:
If there are not five DataNodes, or the HDFS reports that it is in safe mode, something has gone wrong.
Detailed log messages will be available in the logs directory of the Hadoop installation on the slave that is not reporting.
The final test is to attempt to copy a file into HDFS, as follows:
A zero-length file named my_first_file will be created in the /user/USERNAME directory, where USERNAME is the username of the user running the touchz command.
Correcting Errors The most common errors should not occur if the verification steps detailed in the preceding sections completed with no errors.
The	directory	specified	for	hadoop.tmp.dir does not exist with full access permissions for CLUSTER_USER, or could not be created.
The	JVM	may	be	missing	or	installed	in	a	different	location.
HADOOP_HOME is not in an identical location on all of the cluster machines.
Passwordless	SSH	to	some	subset	of	the	machines	may	not	be	working,	or	the	master machine cannot connect to the slave machines via SSH due to firewall or network topology reasons.
This will be clear from the error response of the start command.
The	servers	on	the	slave	machines	may	not	be	able	to	connect	to	their	respective	master server due to firewall or network topology issues.
If this is a problem, there will be a somewhat descriptive error message in the server (TaskTracker or DataNode) log file on the slave machine.
Resolving network topology and firewall issues will require support from your local network administrator.
The most exotic error should not occur at this point, as your installation should be using the Hadoop Core default classpath.
If there is an error message in a log file that indicates that Jetty could not start its web server, there is a nonvalidating XML parser in the classpath ahead of the validating XML parser that Hadoop Core supplies.
This may be fixed by reordering the classpath or by explicitly setting the XML parser by setting a Java property.
You can modify the HADOOP_OPTS environment variable to include this string:
Alternatively, you can alter the setting of HADOOP_OPTS in conf/hadoop-env.sh.
Caution It is highly recommended that you verify the termination of all Hadoop Core server processes across the cluster before attempting a restart.
It is also strongly recommended that the hadoop.tmp.dir have its contents wiped on all cluster machines between attempts to start a new HDFS, and then the file system be reformatted as described in this chapter.
As shown in Figure 3-3, this interface shows information about the cluster.
It also provides links to browse the file system, view the NameNode logs, and to drill down to specific node information.
Starting MapReduce The MapReduce portion of the cluster will be started by the start-mapred.sh command.
If there are any errors when starting the TaskTrackers, the detailed error message will be in the logs directory on the specific slave node of the failed TaskTracker.
The common reasons for failure are very similar to those for HDFS node startup failure.
This is a clear indication that some process is holding open a required port.
Running a Test Job on the Cluster To test the cluster configuration, let’s run our old friend the Hadoop Core example pi (introduced in Chapter 1)
Recall that this program takes two arguments: the number of maps and the number of samples.
In this case, the cluster has five map slots, so you will set the number of maps to five.
Summary This chapter provided a simple walk-through of configuring a small Hadoop Core cluster.
It did not discuss the tuning parameters required for a larger or a high-performance cluster.
For a multimachine cluster to run, the configuration must include the following:
Persistent	location	on	the	cluster	machines	to	store	the	data	for	HDFS	(hadoop.tmp.dir)
You now have an understanding of what a master node is, what the NameNode and JobTracker servers do, and what DataNode and TaskTracker servers are.
You may have even set up a multimachine cluster and run jobs over it.
As you learned in the previous chapter, the defaults provided for multimachine clusters will work well for very small clusters, but they are not suitable for large clusters (the clusters will fail in unexpected and difficult-to-understand ways)
This chapter covers HDFS installation for multimachine clusters that are not very small, as well as HDFS tuning factors, recovery procedures, and troubleshooting tips.
But first, let’s look at some of the configuration trade-offs faced by IT departments.
Configuration Trade-Offs There appears to be an ongoing conflict between the optimal machine and network configurations for Hadoop Core and the configurations required by IT departments.
These strategies reduce the risk of machine failure and provide network diagnostics, flexibility, and simplified administration.
Hadoop Core does not need highly reliable storage on the DataNode or TaskTracker nodes.
The highest performance Hadoop Core installations will have separate and possibly multiple disks or arrays for each stream of I/O.
The DataNode storage will be spread over multiple disks or arrays to allow interleaved I/O, and the TaskTracker intermediate output will also go to a separate disk or array.
This configuration reduces the contention for I/O on any given array or device, thus increasing the maximum disk I/O performance of the machine substantially.
If the switch ports are inexpensive, using bonded network interface cards (NICs) to increase per machine network bandwidth will greatly increase the I/O performance of the cluster.
Hadoop Core provides high availability of DataNode and TaskTracker services without requiring special hardware, software, or configuration.
However, there is no simple solution for high availability for the NameNode or JobTracker.
High availability for the NameNode is an active area of development within the Hadoop community.
All of these techniques require special configuration and have some performance cost.
With Hadoop 0.19.0, there is built-in recovery for JobTracker failures, and generally, high availability for the JobTracker is not considered critical.
Hadoop Core is designed to take advantage of commodity hardware, rather than more expensive specialpurpose hardware.
Most of the disk drives that are being used for Hadoop are inexpensive SATA drives that generally have a sustained sequential transfer rate of about 70Mbps.
Mixed read/write operations provide about 100 Mbps, as all I/O operations require seeks on the same set of drives.
If the six drives were provided as individual drives or as three RAID 0 pairs, the mixed read/write I/O transfer rate would be higher, as seeks for each I/O operation could occur on different drives.
The common network infrastructure is GigE network cards on a GigE switch, providing roughly 100 Mbps I/O.
I’ve used bonded pairs of GigE cards to provide 200 Mbps I/O for high-demand DataNodes to good effect.
For Hadoop nodes, providing dumb, unmanaged crossbar switches for the DataNodes is ideal.
Hdfs installation for multimachine Clusters Setting up an HDFS installation for a multimachine cluster involves the following steps:
Distribute	your	installation	data	to	all	of	the	machines	that	will	host	HDFS	servers.
Building the HDFS Configuration As discussed in the previous chapter, building the HDFS configuration requires generating the conf/hadoop-site.xml, conf/slaves, and conf/masters files.
Generating the conf/hadoop-site.xml File In the conf/hadoop-site.xml file, you tell Hadoop where the data files reside on the file system.
At the simplest level, this requires setting a value for hadoop.tmp.dir, and providing a value for fs.default.name to indicate the master node of the HDFS cluster, as shown in Listing 4-1
The uri's scheme determines the config property (fs.SCHEME.impl) naming the FileSystem implementation class.
The uri's authority is used to determine the host, port, etc.
This will configure a cluster with a NameNode on the host master, and all HDFS storage under the directory /hdfs.
Generating the conf/slaves and conf/masters Files On the machine master, create the conf/slaves file, and populate it with the names of the hosts that will be DataNodes, one per line.
In the file conf/masters, add a single host to be the secondary NameNode.
For safety, make it a separate machine, rather than localhost.
Customizing the conf/hadoop-env.sh File The conf/hadoop-env.sh file provides system environment configuration information for all processes started by Hadoop, as well as all processes run by the user through the scripts in the bin directory of the installation.
At a very minimum, this script must ensure that the correct JAVA_HOME environment variable is set.
Table 4-1 provides a list of the required, commonly set, and optional environment variables.
It must be the root of the  System JDK JDK installation, such that ${JAVA_HOME}/ bin/java is the program to start a JVM.
HADOOP_CLIENT_OPTS  Additional command-line arguments for all nonserver processes started by bin/hadoop.
This is not applied to the server processes, such as the NameNode, JobTracker, TaskTracker, and DataNode.
HADOOP_PID_DIR  The directory that server process ID (PID)  /tmp files are written to.
Used by the service start and stop scripts to determine if a prior instance of a server is running.
The default, /tmp, is not a good location, as the server PID files will be periodically removed by the system temp file cleaning service.
HADOOP_NICENESS  CPU scheduling nice factor to apply to server processes.
The suggested settings prioritize the DataNode over the TaskTracker to ensure that DataNode requests are more rapidly serviced.
They also help ensure that the NameNode or JobTracker servers have priority if a DataNode or TaskTracker is colocated on the same machine.
These suggested settings facilitate smooth cluster operation and enable easier monitoring.
HADOOP_CLASSPATH  Extra entries for the classpath for all Hadoop Java processes.
If your jobs always use specific JARs, and these JARs are available on all systems in the same location, adding the JARs here ensures that they are available to all tasks and reduces the overhead in setting up a job on the cluster.
See this book’s appendix for details on the JobConf object.
HADOOP_OPTS  Additional command-line options for all  -server processes started by bin/hadoop.
Distributing Your Installation Data Distribute your Hadoop installation to all the machines in conf/masters and conf/slaves, as well as the machine master.
Ensure that the user that will own the servers can write to /hdfs and the directory set for HADOOP_PID_DIR in conf/hadoop-env.sh, on all of the machines in conf/masters, conf/slaves, and master.
Finally, ensure that passwordless SSH from master to all of the machines in conf/masters and conf/slaves works.
The user is responsible for ensuring that the JAVA_HOME environment variable is configured.
The framework will issue an ssh call to each slave machine:
The administrator is also responsible for ensuring that the JAVA_HOME environment variable is set correctly in the login scripts or in the hadoop-env.sh script.
Additionally, the administrator must ensure that the storage directories are writable by the.
In general, this simply means constructing the hadoop.tmp.dir directory set in the conf/hadoop-site.xml file, and chowning the directory to the user that the Hadoop servers will run as.
Formatting Your HDFS At this point, you are ready to actually format your HDFS installation.
Run the following to format a NameNode for the first time:
If you have already formatted a NameNode with this data directory, you will see that the command will try to reformat the NameNode:
If the user doing the formatting does not have write permissions, the output will be as follows:
If you are reformatting an HDFS installation, it is recommended that you wipe the hadoop.tmp.dir directories on all of the DataNode machines.
Starting Your HDFS Installation After you’ve configured and formatted HDFS, it is time to actually start your multimachine HDFS cluster.
You can use the bin/start-dfs.sh command for this, as follows:
If you have any lines like the following in your output, then the script was unable to ssh to the slave node:
If you have a block like the following, you have not distributed your Hadoop installation correctly to all of the slave nodes:
The following output indicates that the directory specified in hadoop-env.sh for the server PID files was not writable:
When Hadoop Core is starting services on the cluster, the required directories for the server operation are created if needed.
These include the PID directory and the working and temporary storage directories for the server.
If the framework is unable to create a directory, there will be error messages to that effect logged to the error stream of the script being used to start the services.
Any messages in the startup output about files or directories that are not writable, or directories that could not be created, must be addressed.
In some cases, the server will start, and the cluster may appear to run, but there will be stability and reliability issues.
At this point, the next step is to verify that the DataNode servers started and that they were able to establish service with the NameNode.
Verifying HDFS Is Running To verify that the server processes are in fact running, wait roughly 1 minute after the finish of start-dfs.sh, and then check that the NameNode and DataNode exist.
Checking the NameNodes On the master machine, run the jps command as follows:
The first value in the output is the PID of the java process, and it will be different in your output.
If there is no NameNode, the initialization failed, and you will need to examine the log file to determine the problem.
Listing 4-2 indicates that the NameNode process was unable to find a valid directory for HDFS metadata.
When this occurs, the command hadoop namenode –format must be run, to determine the actual failure.
If the format command completes successfully, the next start-dfs.sh run should complete successfully.
The example in Listing 4-3 demonstrates the failed format command output.
The actual directory listed will be different in actual usage, the directory path /tmp/test1/dir/dfs/name/current was constructed just for this test.
The web interface provided by the NameNode will show information about the status of the NameNode.
Checking the DataNodes You also need to verify that there are DataNodes on each of the slave nodes.
Use jps via the bin/slaves.sh command to look for DataNode processes:
If you do not have a DataNode on each of the slaves, something has failed.
Each machine may have a different reason for failure, so you’ll need to examine the log files on each machine.
The common reason for DataNode failure is that the dfs.data.dir was not writable, as shown in Listing 4-4
The DataNode may also be unable to contact the NameNode due to network connectivity or firewall issues.
In fact, I had half of a new cluster fail to start, and it took some time to.
Listing 4-5 shows an excerpt from a DataNode log for a DataNode that failed to start due to network connectivity problems.
If the IP address is correct, verify that the NameNode is accepting connections on that port.
Hadoop also provides the dfsadmin -report command-line tool, which will provide a somewhat verbose listing of the DataNodes in a service.
You can run this useful script from your system monitoring tools, so that alerts can be generated if DataNodes go offline.
Tuning factors Here, we will look at tuning the cluster system and the HDFS parameters for performance and reliability.
Commonly, the two most important factors are network bandwidth and disk throughput.
Memory use and CPU overhead for thread handling may also be issues.
Using large blocks helps reduce the cost of a disk seek compared with the read/write time, thereby increasing the aggregate I/O rate when multiple requests are active.
The large input-split size reduces the ratio of task setup time to task run time, as there is work to be done to set up a task before the TaskTracker can start the mapper or reducer on the input split.
The various tuning factors available control the maximum number of requests in progress.
In general, the more requests in progress, the more contention there is for storage operations and network bandwidth, with a corresponding increase in memory requirements and CPU overhead for handling all of the outstanding requests.
If the number of requests allowed is too low, the cluster may not fully utilize the disk or network bandwidth, or cause requests to timeout.
Most of the tuning parameters for HDFS do not yet have exact science behind the settings.
In general, the selections are a compromise between various factors.
Finding the sweet spot requires knowledge of the local system and experience.
File Descriptors Hadoop Core uses large numbers of file descriptors for MapReduce, and the DFSClient uses a large number of file descriptors for communicating with the HDFS NameNode and DataNode server processes.
The DFSClient code also presents a misleading error message when there has been a failure to allocate a file descriptor: No live nodes contain current block.
All file system operations and file operations performed by the application will be translated into method calls on the DFSClient object, which will in turn issue the appropriate Remote Procedure Call (RPC) calls to the NameNode and the DataNodes relevant for the operations.
Prior to Hadoop 0.18, blocking operations and fixed timeouts were used for the RPC calls.
Most sites immediately bump up the number of file descriptors to 64,000, and large sites, or sites that have MapReduce jobs that open many files, might go higher.
For a Linux machine, the simple change is to add a line to the /etc/security/limits.conf file of the following form:
This changes the per-user file descriptor limit to 64,000 file descriptors.
If you will run a much larger number of file descriptors, you may need to alter the per-system limits via changes to fs.file-max in /etc/sysctl.conf.
A line of the following form would set the system limit to 640,000 file descriptors:
At this point, you may alter the limits.conf file line to this:
Changes to limits.conf take effect on the next login, and changes to sysctl.conf take place on the next reboot.
You may run sysctl by hand (sysctl -p) to cause the sysctl.conf file to be reread and applied.
The web page at http://support.zeus.com/zws/faqs/2005/09/19/filedescriptors provides some instructions for several Unix operating systems.
Any user that runs processes that access HDFS should have a large limit on file descriptor access, and all applications that open files need careful checking to make sure that the files are explicitly closed.
Trusting the JVM garbage collection to close your open files is a critical mistake.
Block Service Threads Each DataNode maintains a pool of threads that handle block requests.
The parameter dfs.datanode.handler.count controls the number of threads the DataNode will use for handling IPC requests.
Currently, there does not appear to be significant research or tools on the tuning of this parameter.
The overall concept is to balance JVM overhead due to the number of threads with disk and network I/O.
The more requests active at a time, the more overlap for disk and network I/O there is.
At some point, the overlap results in contention rather than increased performance.
The default value for the dfs.datanode.handler.count parameter is 3, which seems to be fine for small clusters.
Medium-size clusters may use a value of 30, set as follows:
The NameNode maintains the metadata of the file system in memory.
Any operation that changes the metadata, such as open, write, or unlink, results in the NameNode writing a transaction to the disks, and an asynchronous operation to the secondary NameNodes.
This adds a substantial latency to any metadata altering transaction.
Through Hadoop 0.19.0, the NameNode edit logs are forcibly flushed to disk storage, but space for the updates is preallocated before the update to reduce the overall latency.
The parameter dfs.namenode.handler.count controls the number of threads that will service NameNode requests.
Increasing this value will substantially increase the memory utilization of the NameNode and may result in reduced performance due to I/O contention for the metadata updates.
However, if your map and reduce tasks create or remove large numbers of files, or execute many sync operations after writes, this number will need to be higher, or you will experience file system timeouts in your tasks.
In my setup, I have a cluster of 20 DataNodes that are very active.
For this cluster, the dfs.namenode.handler.count parameter is set to 512:
For large directories, this can be a considerable amount of data.
With many threads servicing requests, the amount of memory used by in transit serialized requests may be very large.
I have worked with a cluster where this transitory memory requirement exceeded 1GB.
Server Pending Connections Hadoop Core’s ipc.server.listen.queue.size parameter sets the listen depth for accepting connections.
This value is the number of outstanding, unserviced connection requests allowed by the operating system, before connections are refused.
Many operating systems have hard and small limits for this value.
If your thread counts are lower, this value will need to be higher, to prevent refused connections.
If you find connection-refused errors in your log files, you may want to increase the value of this parameter.
As a method for preventing out-of-disk conditions, Hadoop Core provides four parameters: two for HDFS and two for MapReduce.
It is common, especially in smaller clusters, for a DataNode and a TaskTracker to reside on each computer node in the cluster.
It is also common for the TaskTracker’s temporary storage to be stored in the same file system as the HDFS data blocks.
With HDFS, blocks written by an application that is running on the same machine as a DataNode will have one replica placed on that DataNode.
It is very easy for a task to fill up the disk space on a partition and cause HDFS failures, or for HDFS to fill up a partition and result in task failures.
Tuning the disk space reservation parameters will minimize these failures.
If you have more than one TaskTracker on the node, you may need to multiply this minimum by the TaskTracker count to ensure sufficient space.
This parameter has a default value of 0, disabling the check.
This parameter has a default value of 0, disabling the check.
This parameter has a default value of 0, disabling the check.
Caution The dfs.datanode.du.pct parameter is undocumented and may not have the meaning described here in your version of Hadoop.
Storage Allocations The Balancer service will slowly move data blocks between DataNodes to even out the storage allocations.
This is very helpful when DataNodes have different storage capacities.
The parameter dfs.balance.bandwidthPerSec controls the amount of bandwidth that may be used for balancing between DataNodes.
The Balancer task will run until the free space percentage for block storage of each DataNode is approximately equal, with variances in free space percentages of up to the defined threshold allowed.
The start-balancer.sh script prints the name of a file to the standard output that will contain the progress reports for the Balancer.
It is common for the Balancer task to be run automatically and periodically.
The start-balancer.sh script will not allow multiple instances to be run, and it is safe to run this script repeatedly.
In general, the Balancer should be used if some DataNodes are close to their free space limits while other DataNodes have plenty of available space.
This usually becomes an issue only when the DataNode storage capacity varies significantly or large datasets are written into HDFS from a machine that is also a DataNode.
The Balancer may not be able to complete successfully if the cluster is under heavy load, the threshold percentage is very small, or there is insufficient free space available.
If you need to stop the Balancer at any time, you can use the command stop-balancer.sh.
Disk I/O Hadoop Core is designed for jobs that have large input datasets and large output datasets.
This I/O will be spread across multiple machines and will have different patterns depending on the purpose of the I/O, as follows:
The	NameNode	handles	storing the metadata for the file system.
This includes the file paths, the blocks that make up the files, and the DataNodes that hold the blocks.
The NameNode writes a journal entry to the edit log when a change is made.
The NameNode keeps the entire dataset in memory to enable faster response time for requests that don’t involve modification.
Unless an archive (such as a .zip, .tar, tgz, .tar.gz, or .har file) is used, blocks will not be shared among multiple files.
The	TaskTracker	will hold a copy of all of the unpacked elements of the distributed cache for a job, as well as store the partitioned and sorted map output, potentially the merge sort output, and the reduce input.
There are also temporary data files and buffered HDFS data blocks.
The default for Hadoop is to place all of these storage areas under the directory defined by the parameter hadoop.tmp.dir.
In installations where multiple partitions, each on a separate physical drive or RAID assembly, are available, you may specify directories and directory sets that are stored on different physical devices to minimize seek or transfer contention.
All of the HDFS data stores are hosted on native operating system file systems.
For the NameNode data stores, RAID 1 with hot spares is preferred for the low-level storage.
For the DataNodes, no RAID is preferred, but RAID 5 is acceptable.
The file systems should be constructed with awareness of the stripe and stride of any RAID arrays and, where possible, should be mounted in such a way that access time information is not updated.
Linux supports disabling the access time updates for several file systems with the noatime and nodiratime file system mount time options.
I have found that this change alone has provided a 5% improvement in performance on DataNodes.
Journaled file systems are not needed for the NameNode, as the critical data is written synchronously.
Journaled file systems are not recommended for DataNodes due to the increase in write loading.
The downside of not having a journal is that crash recovery time becomes much larger.
Secondary NameNode Disk I/O tuning The secondary NameNode provides a replica of the file system metadata that is used to recover a failed NameNode.
It is critically important that its storage directories be on separate physical devices from the NameNode.
There is some debate about locating the secondary NameNode itself on a separate physical machine.
The merging of the edit logs into the file system image may be faster and have lower overhead when the retrieval of the edit logs and writing of the file system image are local.
However, having the secondary NameNode on a separate machine provides rapid recovery to the previous checkpoint time in the event of a catastrophic failure of the NameNode server machine.
The secondary NameNode uses the parameter fs.checkpoint.dir to determine which directory to use to maintain the file system image.
This parameter may be given a comma-separated list of directories.
The image is replicated across the set of comma-separated values.
The secondary NameNode uses the parameter fs.checkpoint.edits.dir to hold the edit log, essentially the journal for the file system.
It, like fs.checkpoint.dir, may be a commaseparated list of items, and the data will be replicated across the set of values.
The data written to the fs.checkpoint.edits.dir tends to be many synchronous small writes.
The update operations are allowed to run behind the NameNode’s updates.
The secondary NameNode server will take a snapshot from the NameNode at defined time intervals.
The interval is defined by the parameter fs.checkpoint.period, which is a time in seconds, with a default value of 3600
If the NameNode edit log grows by more than fs.checkpoint.size bytes (the default value is 67108864), a checkpoint is also triggered.
The secondary NameNode periodically (fs.checkpoint.period) requests a checkpoint from the NameNode.
At that point, the NameNode will close the current edit log and start a new edit log.
The file system image and the just closed edit log will be copied to the secondary NameNode.
The secondary NameNode will apply the edit log to the file system image, and then transfer the up-to-date file system image back to the NameNode, which replaces the prior file system image with the merged copy.
The secondary NameNode configuration is not commonly altered, except in very high utilization situations.
In these cases, multiple file systems on separate physical disks are used for the set of locations configured in the fs.checkpoint.edits.dir and fs.checkpoint.dir parameters.
NameNode Disk I/O tuning The NameNode is the most critical piece of equipment in your Hadoop Core cluster.
The NameNode, like the secondary NameNode, maintains a journal and a file system image.
The parameter dfs.name.dir provides the directories in which the NameNode will store the file system image and is a comma-separated list (again, the image will be replicated across the set of values)
The file system image is read and updated only at NameNode start time, as of Hadoop version 0.19
The parameter dfs.name.edits.dir contains the comma-separated list of directories to which the edit log or journal will be written.
This is updated for every file system metadataaltering operation, synchronously.
Your entire HDFS cluster will back up waiting for these updates to flush to the disk.
If your cluster is experiencing a high rate of file create, rename, delete, or high-volume writes, there will be a high volume of writes to this set of directories.
Any other I/O operations to file system partitions holding these directories will perform badly.
Some installations will place directories on remote mounted file systems in this list, to ensure that an exact copy of the data is available in the event of a NameNode disk failure.
Any data loss in the directories specified by dfs.name.dir and dfs.name.edits.dir will result in the loss of data in your HDFS.
Caution The NameNode is a critical single point of failure.
Any loss of data can wipe out your entire HDFS datastore.
DataNode Disk I/O tuning The DataNode provides two services: block storage, and retrieval of HDFS data and storage accounting information for the NameNode.
Through at least Hadoop 0.19.0, the storage accounting has significant problems if there are more than a few hundred thousand blocks per DataNode.
This is because a linear scan of the blocks is performed on a frequent basis to provide accounting information.
The following are some other DataNode parameters that may be adjusted:
It is a commaseparated list of directories that will be used for block storage by the DataNode.
Blocks will be distributed in a round-robin manner among the directories.
If multiple directories are to be used, it is best if each directory resides on an independent storage device.
This will allow concurrent I/O operations to be active on all of the devices.
The HDFS cluster can withstand the failure of one less than the value of dfs.replication before there will be service degradation, in that some files may not be served as all of the blocks are not available.
In small to medium-sized clusters, 3 is a good value.
For large clusters, it should be a number that is at least two larger than the expected number of machine failures per day.
The disk storage requirements and the write network bandwidth used are multiplied by this number.
Each node writes to the next node, to mitigate the network load on the individual machines.
The nodes are selected more or less on a random basis, with some simple rules.
If the origination machine is a DataNode for the HDFS cluster being written, one replica will go to that DataNode.
Each file data block in the file system will be in a single file in the dfs.data.dir directory.
A file in HDFS is composed of one or more data blocks.
The last data block of a file may have less than the dfs.block.size bytes of data present.
The block size may be specified on a per-file basis when the file is created.
The individual blocks of the file are replicated onto dfs.replication DataNodes.
The default input split size for a file being used as input in a MapReduce job is the dfs.block.size for the file.
In one of my applications, there was a three-order-of-magnitude speed increase in an application when a set of zip files were constructed once per reduce task, instead of large numbers of small files in the reduce task.
The performance tuning must take place at the application layer and the hardware layer.
Hadoop Core does provide the ability to select the network to bind to for data services and the ability to specify an IP address to use for hostname resolution.
This is ideal for clusters that have an internal network for data traffic and an external network for client communications.
If your installation has switch ports to spare and the switches support it, bonding your individual network connections can greatly increase the network bandwidth available to individual machines.
It is best to isolate this traffic from your other traffic.
At the application layer, if your applications are data-intensive and your data is readily compressible, using block- or record-level compression may drastically reduce the I/O that the job requires.
Compressed input files that are not SequenceFiles, a Hadoop Core binary file format, will not be split, and a single task will handle a single file.
Here, we will look at how HDFS protects from many individual failures and how to recover from other failures.
The	NameNode	stores	the	file	system	metadata	and	provides	direct	replication	of	the data and run-behind remote copies of the data.
The	DataNodes	provide	redundant	storage	by	replicating	data	blocks	to	multiple DataNodes.
NameNode Recovery The default Hadoop installation provides no protection from catastrophic NameNode server failures.
You can avoid data loss and unrecoverable machine failures by running the secondary NameNode on an alternate machine.
Storing the file system image and file system edit logs on multiple physical devices, or even multiple physical machines, also provides protection.
When a NameNode server fails, best practices require that all the JobTracker and TaskTrackers be restarted after the NameNode is restarted.
All incomplete HDFS blocks will be lost, but there should be no file or data loss for existing files or for completed blocks in actively written files.
The NameNode may be configured to write the metadata log to multiple locations on the host server’s file system.
In the event of data loss or corruption to one of these locations, the NameNode may be recovered by repairing or removing the failed location from the configuration, removing the data from that location, and restarting the NameNode.
For rapid recovery, you may simply remove the failed location from the configuration and restart the NameNode.
If the NameNode needs to be recovered from a secondary NameNode, the procedure is somewhat more complex.
All data written to HDFS after the last secondary NameNode checkpoint was taken will be removed and lost.
The default frequency of the checkpoints is specified by the fs.checkpoint.
At present, there are no public forensic tools that will recover data from blocks on the DataNodes.
DataNode Recovery and Addition The procedure for adding a new DataNode to a cluster and restarting a failed DataNode are identical and simple.
The server process just needs to be started, assuming the configuration is correct, and the NameNode will integrate the new server or reintegrate a restarted server into the cluster.
Tip As long as your cluster does not have underreplicated files and no file’s replication count is less than 3, it is generally safe to forcibly remove a DataNode from the cluster by killing the DataNode server process.
The next section covers how to decommission a DataNode gracefully.
The command hadoop-daemon.sh start datanode will start a DataNode server on a machine, if one is not already running.
The configuration in the conf directory associated with the script will be used to determine the NameNode address and other configuration parameters.
If more DataNodes than the dfs.replication value fail, some file blocks will be unavailable.
Your cluster will still be able to write files and access the blocks of the files that remain available.
It is advisable to stop your MapReduce jobs by invoking the stop-mapred.sh script, as most applications do not deal well with partial dataset availability.
When sufficient DataNodes have been returned to service, you may resume MapReduce job processing by invoking the start-mapred.sh script.
When you add new nodes, or return a node to service after substantial data has been written to HDFS, the added node may start up with substantially less utilization than the rest of the DataNodes in the cluster.
DataNode Decommissioning A running DataNode sometimes needs to be decommissioned.
While you may just shut down the DataNode, and the cluster will recover, there is a procedure for gracefully decommissioning a running DataNode.
This procedure becomes particularly important if your cluster has underreplicated blocks or you need to decommission more nodes than your dfs.replication value.
Caution You must not stop the NameNode during this process, or start this process while the NameNode is not running.
One is to exclude the hosts from connecting to the NameNode, which takes effect if the parameter is set when the NameNode starts.
The other starts the decommission process for the hosts, which takes place if the value is first seen after a Hadoop dfsadmin -refreshNodes.
This file should contain one hostname or IP address per line, with standard Unix line endings.
Modify the hadoop-site.xml file by adding, or updating the following block:
When the process is complete, you will see a line in the NameNode log file like the following for each entry in the file:
Deleted File Recovery It is not uncommon for a user to accidentally delete large portions of the HDFS file system due to a program error or a command-line error.
Unless your configuration has the delete-to-trash function enabled, via setting the parameter fs.trash.interval to a nonzero value, deletes are essentially immediate and forever.
If an erroneous large delete is in progress, your best bet is to terminate the NameNode and secondary NameNodes immediately, and then shut down the DataNodes.
Use the procedures described earlier to recover from the secondary NameNode that has the edit log modification time closest to the time the deletion was started.
The fs.trash.interval determines how often the currently deleted files are moved to a date-stamped subdirectory of the deleting user’s .Trash directory.
Files that have not had a trash checkpoint will be under the .Trash/current directory in a path that is identical to their original path.
Troubleshooting Hdfs failures The previous section dealt with the common and highly visible failure cases of a server process crashing or the machine hosting a server process failing.
This section will cover how you can determine what has happened when the failure is less visible or why a server process is crashing.
There are a number of failures that can trip up an HDFS administrator or a MapReduce programmer.
In the current state of development, it is not always clear from Hadoop’s behavior or log messages what the failure or solution is.
NameNode Failures The NameNode is the weak point in the highly available HDFS cluster.
As noted earlier, currently there are no high-availability solutions for the NameNode.
The NameNode has been designed to keep multiple copies of critical data on the local machine and close in time replicas on auxiliary machines.
Out of Memory The NameNode keeps all of the metadata for the file system in memory to speed request services.
The NameNode also serializes directory listings before sending the result to requesting applications.
The memory requirements grow with the number of files and the total number of blocks in the file system.
If your file system has directories with many entries and applications are scanning the directory, this can cause a large transient increase in the memory requirements for the name server.
I once had a cluster that was using the Filesystem in Userspace (FUSE) contrib package to export HDFS as a read-only file system on a machine, which re-exported that file system via the Common Internet File System (CIFS) to a Windows server machine.
The access patterns triggered repeated out-of-memory exceptions on the NameNode.
If the DataNodes are unreliable, and they are dropping out of service and then returning to service after a gap, the NameNode will build a large queue of blocks with invalid states.
This may consume very large amounts of memory if large numbers of blocks become transitorily unavailable.
For this problem, addressing the DataNode reliability is the only real solution, but increasing the memory size on the NameNode can help.
Increase	the	memory	available	to	the	NameNode,	and	ensure	the	machine	has	sufficient real memory to support this increase.
Ensure	that	no	directory	has	a	large	number	of	entries.
Alter	application	access	patterns	so	that	there	are	not	large	numbers	of	directory listings.
As a general rule, pin the full memory allocation by setting the starting heap size for the JVM to the maximum heap size.
Even so, catastrophic failures or user errors can result in data loss.
The NameNode configuration will accept a comma-separated list of directories and will maintain copies of the data in the full set of directories.
This additional level of redundancy provides a current time backup of the file system metadata.
The secondary NameNode provides a few-minutes-behind replica of the NameNode data.
If your configuration has multiple directories that contain the file system image or the edit log, and one of them is damaged, delete that directory’s content and restart the NameNode.
If the directory is unavailable, remove it from the list in the configuration and restart the NameNode.
If all of the dfs.name.dir directories are unavailable or suspect, do the following:
Copy the contents of the fs.checkpoint.dir from the secondary NameNode to the fs.checkpoint.dir on the primary NameNode machine.
If there is no good copy of the NameNode data, the secondary NameNode image may be.
You may simply copy the file system image from the secondary NameNode to a file system image directory on the NameNode, and then restart.
As the imported data is older than the current state of the HDFS file system, the NameNode may spend significant time in safe mode as it brings the HDFS block store into consistency with restored snapshot.
No Live Node Contains Block errors Usually, if you see the no live node contains block error, it will be in the log files for your applications.
It means that the client code in your application that interfaces with HDFS was unable to find a block of a requested file.
For this error to occur, the client code received a list of DataNode and block ID pairs from the NameNode, and was unable to retrieve the block from any of the DataNodes.
This error commonly occurs when the application is unable to open a connection to any of the DataNodes.
This may be because there are no more file descriptors available, there is a DNS resolution failure, there is a network problem, or all of the DataNodes in question are actually unavailable.
This may be corrected by increasing the number of file descriptors available, as described earlier in this chapter.
An alternative is to minimize unclosed file descriptors in the applications.
I’ve seen DNS resolution failures transiently appear, and as a general rule, I now use IP addresses instead of hostnames in the configuration files.
It is very helpful if the DNS reverse lookup returns the same name as the hostname of the machine.
Write Failed If there are insufficient DataNodes available to allow full replication of a newly written block, the write will not be allowed to complete.
This may result in a zero-length or incomplete file, which will need to be manually removed.
DataNode or NameNode Pauses Through at least Hadoop 0.19.0, the DataNode has two periodic tasks that do a linear scan of all of the data blocks stored by the DataNode.
If this process starts taking longer than a small number of minutes, the DataNode will be marked as disconnected by the NameNode.
When a DataNode is marked as disconnected, the NameNode queues all of the blocks that had a replica on that DataNode for replication.
If the number of blocks is large, the NameNode may pause for a noticeable period while queuing the blocks.
The only solutions for this at present are to add enough DataNodes, so that no DataNode has more than a few hundred thousand data blocks, or to alter your application’s I/O patterns to use Hadoop archives or zip files to pack many small HDFS subblock-size files into single HDFS files.
The latter approach results in a reduction in the number of blocks stored in HDFS and the number of blocks per DataNode.
A simple way to work out how many blocks is too many is to run the following on a DataNode:
If it takes longer than a few hundred seconds, you are in the danger zone.
If it takes longer than a few minutes, you are in the pain zone.
Replace dfs.data.dir with an expanded value from your Hadoop configuration.
If your ls takes anywhere close to that, your cluster will be unstable.
With reasonable care and an understanding of HDFS’s limitations it will serve you well.
Through HDFS 0.19.0, the HDFS NameNode is a single point of failure and needs careful handling to minimize the risk of data loss.
Using a separate machine for your secondary NameNode, and having multiple devices for the file system image and edit logs, will go a long way toward providing a fail-safe, rapid recovery solution.
Monitoring your DataNode’s block totals will go a long way toward avoiding congestion collapse issues in your HDFS.
You can do this by simply running a find on the dfs.data.dir and making sure that it takes less than a couple of minutes.
Ensuring that your HDFS data traffic is segregated from your normal application traffic and crosses as few interswitch backhauls as possible will help to avoid network congestion and application misbehavior.
Ultimately, remember that HDFS is designed for a small to medium number of very large files, and not for transitory storage of large numbers of small files.
Organizations run Hadoop Core to provide MapReduce services for their processing needs.
They may have datasets that can’t fit on a single machine, have time constraints that are impossible to satisfy with a small number of machines, or need to rapidly scale the computing power applied to a problem due to varying input set sizes.
You will have your own unique reasons for running MapReduce applications.
To do your job effectively, you need to understand all of the moving parts of a MapReduce cluster and of the Hadoop Core MapReduce framework.
This chapter will raise the hood and show you some schematics of the engine.
This chapter will also provide examples that you can use as the basis for your own MapReduce applications.
Requirements for Successful MapReduce Jobs For your MapReduce jobs to be successful, the mapper must be able to ingest the input and process the input record, sending forward the records that can be passed to the reduce task or to the final output directly, if no reduce step is required.
The reducer must be able to accept the key and value groups that passed through the mapper, and generate the final output of this MapReduce step.
The job must be configured with the location and type of the input data, the mapper class to use, the number of reduce tasks required, and the reducer class and I/O types.
The TaskTracker service will actually run your map and reduce tasks, and the JobTracker service will distribute the tasks and their input split to the various trackers.
The cluster must be configured with the nodes that will run the TaskTrackers, and with the number of TaskTrackers to run per node.
The TaskTrackers need to be configured with the JVM parameters, including the classpath for both the TaskTracker and the JVMs that will execute the individual tasks.
There are three levels of configuration to address to configure MapReduce on your cluster.
From the bottom up, you need to configure the machines, the Hadoop MapReduce framework, and the jobs themselves.
We’ll get started with these requirements by exploring how to launch your MapReduce jobs.
Tip A Hadoop job is usually part of a production application, which may have many steps, some of which are MapReduce jobs.
Hadoop Core, as of version 0.19.0, provides a way of optimizing the data flows between a set of sequential MapReduce jobs.
This framework for descriptively and efficiently running sequential MapReduce jobs together is called chaining, and uses the ChainMapper and the ChainReducer, as discussed in Chapter 8
Launching MapReduce Jobs Jobs within a MapReduce cluster can be launched by constructing a JobConf object (details on the JobConf object are provided in this book’s appendix) and passing it to a JobClient object:
You can launch the preceding example from the command line as follows:
For this to be successful requires a considerable amount of runtime environment setup.
Hadoop Core provides a shell script, bin/hadoop, which manages the setup for a job.
Using this script is the standard and recommended way to start a MapReduce job.
This script sets up the process environment correctly for the installation, including inserting the Hadoop JARs and Hadoop configuration directory into the classpath, and launches your application.
This behavior is triggered by providing the initial command-line argument jar to the bin/hadoop script.
Hadoop Core provides several mechanisms for setting the classpath for your application:
You	can	set	up a fixed base classpath by altering hadoop-env.sh, via the HADOOP_CLASSPATH environment variable (on all of your machines) or by setting that environment variable in the runtime environment for the user that starts the Hadoop servers.
You	may	run your jobs via the bin/hadoop jar command and supply a -libjars argument with a list of JARs.
The	DistributedCache object provides a way to add files or archives to your runtime classpath.
Tip The mapred.child.java.opts variable may also be used to specify non-classpath parameters to the child JVMs.
In particular, the java.library.path variable specifies the path for shared libraries if your application uses the Java Native Interface (JNI)
If your application alters the job configuration parameter mapred.child.java.opts, it is important to ensure that the JVM memory settings are reset or still present, or your tasks may fail with out-of-memory exceptions.
The advantage of using the DistributedCache and -libjars is that resources, such as JAR files, do not have to already exist on the TaskTracker nodes.
The disadvantages are that the resources must be unpacked on each machine and it is harder to verify which versions of the resources are used.
When launching an application, a number of command-line parameters may be provided.
These JAR files will be staged into HDFS if needed and made available as local files in a temporary job area on each of the TaskTracker nodes.
You can use hadoop jar to launch an application, as follows:
When hadoop jar is used, the main method of org.apache.hadoop.mapred.JobShell is invoked by the JVM, with all of the remaining command-line arguments.
There are two distinct steps in the argument processing of jobs submitted by the bin/ hadoop script.
The first step is provided by the framework via the JobShell.
The arguments after jar are processed by the JobShell, per Table 5-1
The first argument not in the set recognized by the JobShell must be the path to a JAR file, which is the job JAR file.
If the job JAR file contains a main class specification in the manifest, that class will be the main class called after the first step of argument processing is complete.
If the JAR file does not have a main class in the manifest, the next argument becomes required, and is used as main class name.
Any remaining unprocessed arguments are passed to the main method of the main class as the arguments.
The second step is the processing of the remaining command-line arguments by the user-specified main class.
For example, one of my jobs required a shared library that handled job-specific image processing.
For example, using the command-line options -file libMyStuff.so would make libMyStuff.so available in the current working directory of each task.
Install	the	shared	library	on	every	TaskTracker	machine,	and	have	the	JVM	library loader path java.library.path include the installation directory.
The task JVM working directory is part of the java.library.path for a task, and any file that is symbolic-linked may be loaded by the JVM.
Caution If you are manually loading shared libraries, the library name passed to System.
MapReduce-Specific Configuration for Each Machine in a Cluster For simplicity and ease of ongoing maintenance, this section assumes identical Hadoop Core installations will be placed on each of the machines, in the same location.
The following are the MapReduce-specific configuration requirements for each machine in the cluster:
You	need	to	install	any	standard	JARs	that	your	application	uses,	such	as	Spring,	Hibernate, HttpClient, Commons Lang, and so on.
It	is	probable	that	your	applications	will	have	a	runtime	environment	that	is	deployed from a configuration management application, which you will also need to deploy to each machine.
The	machines	will	need	to	have	enough	RAM	for	the	Hadoop	Core	services	plus	the RAM required to run your tasks.
The	conf/slaves file should have the set of machines to serve as TaskTracker nodes.
You may manually start individual TaskTrackers by running the command bin/ hadoop-daemon.sh start tasktracker, but this is not a recommended practice for starting a cluster.
The hadoop-env.sh script has a section for providing custom JVM parameters for the different Hadoop Core servers, including the JobTracker and TaskTrackers.
As of Hadoop 0.19.0, the classpath settings are global for all servers.
The hadoop-env.sh file may be modified and distributed to the machines in the cluster, or the environment variable HADOOP_JOBTRACKER_OPTS may be set with JVM options before starting the cluster via the bin/start-all.sh command or bin/start-mapred.sh command.
The environment variable HADOOP_TASKTRACKER_OPTS may be set to provide per TaskTracker JVM options.
It is much better to modify the file, as the changes are persistent and stored in a single Hadoop-specific location.
The start-*.sh scripts will ssh to each target machine, and then run the bin/hadoop-daemon.sh start tasktracker command.
Using the Distributed Cache The DistributedCache object provides a programmatic mechanism for specifying the resources needed by the mapper and reducer.
The job is actually already using the DistributedCache object to a limited degree, if the job creates the JobConf object with a class as an argument: new JobConf(MyMapper.class)
You may also invoke your MapReduce program using the bin/hadoop script and provide arguments for -libjars, -files, or -archives.
The downloadable code for this book (available from this book’s details page on the Apress web site, http://www.apress.com) includes several source files for the DistributedCache examples: Utils.java, DistributedCacheExample.java, and DistributedCacheMapper.java.
Caution The paths and URIs for DistributedCache items are stored as comma-separated lists of strings in the configuration.
Any comma characters in the paths will result in unpredictable and incorrect behavior.
Adding Resources to the Task Classpath Four methods add elements to the Java classpath for the map and reduce tasks.
The first three in the following list add archives to the classpath.
The archives are unpacked in the job local directory of the task.
You can use the following methods to add resources to the task classpath:
JobConf.setJar(String jar): Sets the user JAR for the MapReduce job.
It is on the JobConf object, but it manipulates the same configuration keys as the DistributedCache.
The file jar will be found, and if necessary, copied into the shared file system, and the full path name on the shared file system stored under the configuration key mapred.jar.
JobConf.setJarByClass(Class cls): Determines the JAR that contains the class cls and calls JobConf.setJar(jar) with that JAR.
DistributedCache.addArchiveToClassPath(Path archive, Configuration conf): Adds an archive path to the current set of classpath entries.
This is a static method, and the archive (a zip or JAR file) will be made available to the running tasks via the classpath of the JVM.
The archive is also added to the list of cached archives.
The contents will be unpacked in the local job directory on each TaskTracker node.
The archive path is stored in the configuration under the key mapred.job.classpath.archives, and the mapred.job.classpath.archives.
If the path component of the URI does not exactly equal archive, archive will not be placed in the classpath of the task correctly.
Caution The archive path must be on the JobTracker shared file system, and must be an absolute path.
Only the path /user/hadoop/myjar.jar is correct; hdfs://host:8020/user/hadoop/myjar.jar will fail, as will hadoop/myjar.jar or myjar.jar.
DistributedCache.addFileToClassPath(Path file, Configuration conf): Adds a file path to the current set of classpath entries.
This is a static method that makes the file available to the running tasks via the classpath of the JVM.
The file path is stored under the configuration key mapred.job.classpath.files, mapred.cache.files.
If file is not exactly equal to the path portion of the constructed URI, file will not be added to the classpath of the task correctly.
Caution The file path added must be an absolute path on the JobTracker shared file system, and be only a path.
Distributing Archives and Files to Tasks In addition to items that become available via the classpath, two methods distribute archives and individual files: DistributedCache.addCacheArchive(URI uri, Configuration conf) and DistributedCache.addCacheFile(URI uri, Configuration conf)
Local file system copies of these items are made on all of the TaskTracker machines, in the work area set aside for this job.
Distributing archives The DistributedCache.addCacheArchive(URI uri, Configuration conf) method will add an archive to the list of archives to be distributed to the jobs.
The URI must have an absolute path and be on the JobTracker shared file system.
If the URI has a fragment, a symbolic link to the archive will be placed in the task working directory as the fragment.
The URI hdfs://host:8020/user/hadoop/myfile#mylink will result in a symbolic link mylink in the task working directory that points to the local file system location that myfile was unpacked into at task start.
The archive will be unpacked into the local working directory of the task.
The URI will be stored in the configuration under the key mapred.cache.archives.
Distributing Files This DistributedCache.addCacheFile(URI uri, Configuration conf) method will make a copy of the file uri available to all of the tasks, as a local file system file.
The URI must be on the JobTracker shared file system.
If the URI has a fragment, a symbolic link to the URI fragment will be created in the JVM working directory that points to the location on the local file system where the uri was unpacked into at task start.
The directory where DistributedCache stores the local copies of the passed items is not the current working directory of the task JVM.
This allows the items to be referenced by names that do not have any path components.
To pass a script via the distributed cache, use DistributedCache.addCacheFile( new URI invoked via ./script, use DistributedCache.addCacheFile( new URI("hdfs://host:8020/
If name has a leading slash, this method will search for it in each location in the classpath, and return the URI.
If the job passed a file into DistributedCache via the -files command or the file name component, with a leading slash, will return the URI.
The cache items will be the last items in the classpath.
This does not appear to work for files that are added via DistributedCache.addFileToClassPath.
The DistributedCache.addArchiveToClassPath(jarFileForClassPath, job) method actually stores the JAR information into the configuration.
Looking Up archives and Files The public static Path[]getLocalCacheArchives (Configuration conf) method returns a list of the archives that were passed via DistributedCache.
The paths will be in the task local area of the local file system.
It is possible that the file name portion of your archive will be changed slightly.
DistributedCache provides the following method to help with this situation:
This takes an original archive path and returns the possibly altered file name component.
The file name portions of the paths may be different from the original file name.
Finding a File or archive in the Localized Cache The DistributedCache object may change the file name portion of the files and archives it way to determine what the file name portion of the passed item was changed to.
In addition to the file name portion, the items will be stored in a location relative to the working area for the task on each TaskTracker.
Table 5-2 lists the methods provided in the downloadable code that.
These methods are designed to be used in the mapper and reducer methods.
Utils.makeRelativeName( name, conf)  Returns the actual name DistributedCache will use for the passed-in name.
Utils.findClassPathArchive( name, conf)  Returns the actual path on the current machine of the archive name that was passed via DistributedCache.addArchiveToClassPath.
Utils.findClassPathFile( name, conf)  Returns the actual path on the current machine of the file name that was passed via Dist ributeCacheAddFileToClasspath.
Utils.findNonClassPathArchive( name, conf)  Returns the actual path on the current machine of the archive name that was passed via DistributedCache.addCacheArchive.
Utils.findNonClassPathFile( name, conf)  Returns the actual path on the current machine of the file name that was passed via DistributedCache.addCacheFile.
Configuring the Hadoop Core Cluster Information The JobConf object provides two basic and critical ways for specifying the default file system: the URI to use for all shared file system paths, and the connection information for the JobTracker server.
These two items are normally specified in the conf/hadoop-site.xml file, but they may be specified on the command line or by setting the values on the JobConf object.
Setting the Default File System URI The default file system URI is normally specified with the fs.default.name setting in the hadoop-site.xml file, as it is cluster-specific.
Note The default value for the file system URI is file:///, which stores all files on the local file system.
The file system that is used must be a file system that is shared among all of the nodes in the cluster.
The Hadoop tools, examples, and any application that uses the GenericOptionParser class to handle command-line arguments will accept a –fs hdfs://NamenodeHostname:PORT command-line argument pair to explicitly set the fs.default.name value in the configuration.
This will override the value specified in the hadoop-site.xml file.
Here’s a sample command line for listing files on an explicitly specified HDFS file system:
You can also use the JobConf object to set the default file system:
Setting the JobTracker Location The JobTracker location is normally specified with the mapred.job.tracker setting in the hadoop-site.xml file, as it is cluster-specific.
Through Hadoop 0.19, there is not a standard for the PORT.
Many installations use a port one higher that the HDFS port.
Note The default value for the JobTracker location is local, which will result in the job being executed by the JVM that submits it.
The value local is ideal for testing and debugging new MapReduce jobs.
Here’s a sample command line explicitly setting the JobTracker for job control for listing jobs:
And here’s how to use the JobConf object to set the JobTracker information:
The class providing the map function must implement the org.apache.hadoop.mapred.Mapper interface, which in turn requires the interfaces org.apache.hadoop.mapred.JobConfigurable and org.apache.hadoop.
The Hadoop framework provides org.apache.hadoop.mapred.MapReduceBase from which to derive mapper and reducer classes.
In the utilities supplied with this book’s downloadable code is com.apress.hadoopbook.utils.MapReduceBase, which provides more useful implementations.
Note The interface org.apache.hadoop.io.Closeable will be replaced with java.io.Closeable in a later release.
This section examines the sample mapper class SampleMapperRunner.java, which is available with the rest of the downloadable code for this book.
When run as a Java application, this example accepts all of the standard Hadoop arguments and may be run with custom bean context and definitions:
The path and file name provided must be a path that can be opened from the current working directory.
Note If you are using the value local as the value of the mapred.task.tracker configuration key, using the DistributedCache object is less effective, as the task cannot change working directories.
Mapper Methods For the mapper, the framework will call three methods:
If the class is an instance of Configurable, newInstance will call the setConf method with the supplied configuration.
If the class is an instance of JobConfiguration, newInstance will call the configure method.
Any exceptions that are thrown during the construction or initialization of the instance are rethrown as RuntimeExceptions.
The framework may attempt to retry this task on another host if the allowable number of failures for the task has not been control the number of times a map task will be retried if the task fails.
If this method fails the Tasktracker will abort this task.
These casts are safe as they are checked by the framework * earlier in the process.
In this example, K2 is the map output key type, which defaults to the reduce output key type, which defaults to LongWritable.
V2 is the map output value key type, which defaults to the reduce output value type, which defaults to Text.
This method also instantiates a local instance of the key and value classes,
If the job is configured for running multithreaded map tasks, as follows, there may be.
The default number of threads for a multithreaded map task is ten.
The contents of the key object and the contents of the value object are valid only during.
The reduce key and value output classes are set by JobConf.setOutputKeyClass(clazz) and JobConf.setOutputValueClass(clazz)
You can explicitly configure the map output key and value classes, as follows:
If a map output class is set, the corresponding reduce input class is also set to the class.
If the map output key class is changed to BytesWritable, the Reducer.reduce’s key type will be BytesWritable.
Sample close method that sets the task status based on how * many map exceptions there were.
This assumes that the reporter object passed into the map method was saved and *  that the JobConf object passed into the configure method was saved.
If we have a reporter we can perform simple checks on the * completion status and set a status message for this task.
Use the Spring set bean to show we did get the values.
Ensure any HDFS files are closed here, to force them to be * committed to HDFS.
This example also logs to counters with the values received from the Spring initialization.
I found the Spring value-based counters useful while working out how to initialize map class member variables via the Spring Framework, as described after the discussion of the mapper class declaration and member fields.
It is also a good practice to instantiate member variables, or thread local method.
Having the TaskAttemptId available is also useful, as it is easy to determine if this is the map phase or the reduce phase of a job.
It is convenient to capture the output collector and the reporter into member fields so that.
Create a logging object or you will never know what happened in your * task.
They are so handy for almost any * interaction with the framework.
These are nice to save, but require a test or a set each pass through * the map method.
Remember that the map method is an inner loop that * may be called millions of times.
Initializing the Mapper with Spring Many installations use the Spring Framework to manage the services employed by their applications.
One of the more interesting issues is how to use Spring in environments where Spring does not have full control over the creation of class instances.
Spring likes to be in full control of the application and manage the creation of all of the Spring bean objects.
In the Hadoop case, the Hadoop framework is in charge and will create the object instances.
The examples in this section demonstrate how to use Spring to initialize member variables in the mapper class.
Listing 5-4 shows the bean file used in the Spring example.
Creating the Spring application Context To create an application context, you need to provide Spring with a resource set from which to load bean definitions.
Being very JobConf-oriented, I prefer to pass the names of these resources, and possibly the resources themselves, to my tasks via the JobConf and DistributedCache objects.
The example in Listing 5-5 extracts the set of resource names from the JobConf object, and if not found, will supply a default set of resource names.
This follows the Hadoop style of using comma-separated elements to store multiple elements in the configuration.
The set of resources names are unpacked and passed to the Spring Framework.
Each of these resources must be in the classpath, which includes the task working directory.
At the very simplest, the user may specify the specific Spring configuration files on the command line via the -files argument, when the GenericOptionsParser is in use.
The mapper class will need to determine the name of the file passed in via the command line.
For the example, set up the Spring initialization parameters on the application command line as follows:
If it comes before the jar argument, it will become a Java system property.
The configuration parameter mapper.bean.context tells the map task which bean file to load.
In the example, SampleMapperRunner looks up the configuration entry mapper.bean.context to determine which bean files to use when creating the application context.
This method picks up the application context from a file, that is in * the classpath.
Look up the context config files in the JobConf, provide a default value.
Using Spring to autowire the Mapper Class Once the Spring application context has been created, the task may instantiate beans.
The confusing issue is that the mapper class has already been instantiated, so how can Spring be forced to initialize/autowire that class?
The first is that the bean definition to be used must specify lazy initialization, to prevent Spring from creating an instance of the bean when the application context is created.
The second is to know the bean name/ID of the mapper class.
The example in Listing 5-6 makes some assumptions about how application contexts and task beans are named, and can be easily modified for your application.
This will fail if Spring weaves a wrapper class for AOP around * the configure bean.
Construct a bean name for this class using the configuration * or a default name.
In Listing 5-6, a base name is constructed for looking up information in the configuration via the following:
The example builds a context file name key, which will be mapper.bean.context in the case of a map, to look up the application context information in the configuration.
If a value is found, it is treated as a comma-separated list of bean resource files to load.
The application context is loaded and saved in the member variable applicationContext:
A default bean file is used if no value is found.
A bean name key mapper.bean.name, with a default value of mapper.bean.name, is looked up in the configuration.
This is the bean that will be used to configure the task.
An autowire-capable bean factory is extracted from the application context via the following:
The following line actually causes Spring to initialize the task:
The code must ensure that Spring did not return a delegator object when it was initializing the task from the bean definition:
Note This example does not handle the case where Spring returns a delegator object for the task.
Partitioners Dissected A core part of the MapReduce concept requires that map outputs be split into multiple streams called partitions, and that each of these partitions is fed to a single reduce task.
The reduce contract specifies that each reduce task will be given as input the fully sorted set of keys.
The entire partition is the input of the reduce task.
For the framework to satisfy this contract, a number of things have to happen first.
The outputs of each map task are partitioned and sorted.
The partitioner is run in the context of the map task.
The Hadoop framework provides several partitioning classes and a mechanism to specify a class to use for partitioning.
The actual class to be used must implement the org.apache.
Note that both the key and the value are available in making the partition choice.
The key (or a subset of the key) is used to derive * the partition, typically by a hash function.
The total number of partitions * is the same as the number of reduce tasks for the job.
Get the partition number for a given key (hence record) given the total * number of partitions i.e.
The key and value will be streamed into the partition number that this function returns.
Each of these map local partition files is sorted in key.
For each reduce task, the framework will collect all the reduce task’s partition pieces from each of the map tasks and merge-sort those pieces.
The output of a reduce task will be written to the part-XXXXX file, where the XXXXX corresponds to the partition number.
KeyFieldBasedPartitioner, which provides a way to partition by parts of the key.
The HashPartitioner Class The default partitioner, org.apache.hadoop.mapred.lib.HashPartitioner, simply uses the hash code value of the key as the determining factor for partitioning.
Listing 5-8 shows the actual code from the default partitioner used by Hadoop.
The partition number is simply the hash value of the key modulus the number of partitions.
The resulting number has modulus the number of reduce tasks applied, % numReduceTasks, and the result returned.
This produces a positive number between 0 and one less than the number of partitions.
The TotalOrderPartitioner Class The TotalOrderPartitioner, org.apache.hadoop.mapred.lib.TotalOrderPartitioner, relies on a file that provides the class with range information.
With this information, the partitioner is able to determine which range a key/value pair belongs in and route it to the relevant partition.
Note The TotalOrderParitioner grew out of the TetraSort example package.
Jim Gray introduced a contest called the TeraByteSort, which was a benchmark to sort one terabyte of data and write the results to disk.
The code is included with the Hadoop examples as bin/hadoop jar hadoop-*-examples.jar terasort in-dir out-dir.
Building a range table The org.apache.hadoop.mapred.lib.InputSampler class is used to generate a range partitioning file for arbitrary input sets.
This class will sample the input to build an approximate range table.
This sampling strategy will take no more than the specified number of samples total from the input.
The user may specify a maximum number of input splits to look in as well.
The actual number of records read from each input split varies based on the number of splits and the number of records in the input split.
The Hadoop framework controls how the input is split based on the number of input files, the input format, the input file size, and the minimum split size and the HDFS block size.
Let’s look at a few examples of running InputSampler from the command line.
The input is loaded from the directory csvin, and is parsed by the KeyValueTextInputFormat class.
The input splits are examined in the order in which they are returned by InputFormat.
The input splits are sampled in a random order, and the records from each split read are sequentially.
If the 1,000 samples are not selected after processing the recommended number of splits, more splits will be sampled.
The index is set up for 15 reduce tasks, and the input comes from csvin.
The frequency parameter defines how many records will be sampled.
The input comes from csvin, and the index is written to csvout.
Using the totalOrderpartitioner Once an index is generated, a job may be set up to use the TotalOrderPartitioner and the index.
The	partitioner	must	be	set	to	TotalOrderPartitioner in the JobConf object via conf.se tPartitionerClass(TotalOrderPartitioner)
The	partitioning	index	must	be	specified	via	the	configuration	key total.order.partitioner.path:
The	sort	type	for	the	keys	must	also	be	specified.
If	the	binary	representation	of	the keys is the correct sorting, the Boolean field total.order.partitioner.natural.order should be set to true in the configuration.
If the binary representation of the keys is not the correct sort, the Boolean field total.order.partitioner.natural.order must be set to false.
If the binary representation of the key is the correct sort order, a binary trie (an ordered tree structure; see http://en.wikipedia.org/wiki/Trie) will be constructed and used for searching; otherwise, a binary search based on the output key comparator will be used.
Here’s an example of how to put all this together:
In this example, csvin is the input file, and csvout is the index file.
The csvout file was set up for 15 reduce tasks, and requires the comparator rather than binary comparison.
The KeyFieldBasedPartitioner Class The KeyFieldBasedPartitioner, org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner, provides the job with a way of using only parts of the key for comparison purposes.
The primary concept is that the keys may be split into pieces based on a piece separator string.
The separator string is defined by the configuration key map.output.key.field.separator and defaults to the tab character.
It may be set to another string, str, as follows:
This is functionally equivalent to using the String.split(Pattern.quote(str)) call on each key and treating the resulting array as if indexes were one-based instead of zero-based.
Referencing individual characters within the pieces is also one-based rather than zerobased, with the index 0 being the index of the last character of the key part.
Note In addition to the one-based ordinal position within the key piece, the last character of the key piece may also be referenced by 0
The key pieces to compare are specified by setting the key field partition option, via the following:
The str format is very similar to the key field-based comparator.
The Javadoc from Hadoop 0.19.0 for KeyFieldBasedPartitioner provides the following.
Defines a way to partition keys based on certain key fields (also see KeyFieldBasedComparator)
The key specification supported is of the form -k pos1[,pos2], where, pos is of.
There is also the facility to select a start and stop point within an individual key.
Note If your key specification may touch the last key piece, it is important to terminate with the last character of the key.
Otherwise, the current code (as of Hadoop 0.19.0) will generate an ArrayIndexOutOfBoundsException as it tries to use the missing separator string.
The Reducer Dissected instances of the output key and output value objects apply to the reducer as well.
The default is to define a group as all values that share a key.
Common uses for reduce tasks are to suppress duplicates in datasets or to segregate ranges and order output of large datasets.
The identity reducer simply outputs each value in the iterator.
Performs no reduction, writing all input values directly to the output.
It is generally recommended that you do not make a copy of all of the value objects, as there may be very many of these objects.
Note In one of my early applications, I assumed that there would never be more than a small number of values per key.
It is possible to simulate a secondary sort/grouping of the values by setting the output value grouping.
To do this requires the cooperation of the OutputComparator, OutputPartitioner, and OutputValueGroupingComparator.
By default, the input key and value types are the same as the output key and value types, and are set by the conf.setOutputKeyClass(class) and conf.setOutputValueClass(class) methods.
If the map output keys must be different, using conf.setMapOutputKeyClass(class) and conf.setMapOutputValueClass(class) will also change the expected input key and value for the reduce task.
Used in building the textual representation of the output key and values.
Examine each of the values that grouped with this key.
Used in building the textual representation of the output key and values.
In the body of the example in Listing 5-10, each value that is passed in is examined and aggregated:
Examine each of the values that grouped with this key.
Finally, the output key and value are constructed with the aggregated data:
The example that runs this reducer also uses an output grouping comparator that groups the records in sets of ten.
The following is the core code in SimpleReduce that sets up the job that runs SimpleReduceTransformingReducer:
Force the reduce to take text as the output value class, * instead of the default.
The following command will run the SimpleReduce job (your output will vary slightly):
The actual output is the key, the average value, the number of values averaged, the minimum value, the maximum value, and the difference between the minimum and the maximum.
Now you can print the job output (key, average, count, min, max, difference), as follows:
A Reducer That Uses Three Partitions A variant of the SimpleReduce.java example, called TotalOrderSimpleReduce.java (available with the rest of this chapter’s downloadable code), uses three partitions, rather than just one.
This example demonstrates how to use the InputSampler class and the TotalOrderPartitioner.
In this example, the grouping operator groups by multiples of ten in the key space.
The TotalOrderParititioner selects a random sample of the keys and creates three groups that are roughly even in size given the sample of keys.
There is no guarantee that an entire group of keys will not be split into multiple partitions.
This application also requires a custom InputFormat, LongLongTextInputFormat, as the input key and the reduce key must be of the same type for the InputSampler.
In the previous version, the map input keys are Text and the reduce input keys are LongWritable.
Listing 5-11 shows the core of the LongLongTextInputFormat, the RecordReader.next method.
Delegated next, read the textual values from the the data source * and convert them into LongWritables.
The code in Listing 5-12 sets up the JobConf object for the TotalOrderParitioner.
As the keys are long values, they are binary comparable.
The call to runInputSampler computes the partitioning index and stores it in the file TotalOrderSimpleReduce.index.
Force reduce to take text as the output value class, instead of the default.
Ensure that all keys go to 3 reduce to demonstrate order based partitioning.
The code in Listing 5-13 runs the InputSampler to compute and store the index in indexFile.
The assumption here is that the JobConf object conf is already correctly set up with the InputPaths and InputReader.
The sampling strategy is to randomly sample the records with a 0.1% chance that any record is chosen.
Your results will differ, as random data generation and selection are occurring.
The purpose of a combiner is to reduce the volume of data that must be passed to the reducer from a map task by summarizing output records that share the combiner will be called with each output key and all of the output values that share that key.
The output of the combiner is what will be sent over the network to the actual reduce task for the job or written to the final output directory, if there is no reduce task configured.
When all of the map task input has been processed, these partitions are sorted, and each for each unique key in the partition, and the values will be the set of values that share that key.
The output of the combiner will replace that set of original map outputs, ideally with fewer records or smaller records.
This is suitable for jobs that are producing summary information from a large dataset.
Caution The combiner must not change the key values, as the map outputs are not re-sorted after the combiner runs.
The reduce phase requires the map outputs to be sorted by key.
It is common for the same class that is used in the reduce task to be used for the combiner.
The combiner must only aggregate values, in a manner that is suitable for processing by the actual reducer.
The	actual	reducer	has	the	larger	job	of	producing	the	final	job	output.
Problems	occur	when the reducer is modified to provide some change in the job output, and the person doing the modification is unaware that the reducer is also used as a combiner.
It is very important that the combiner class not have side effects, and that the actual reducer be able to properly process the results of the combiner.
Tip It not always simple to build a correct combiner.
If a job output has problems, try running the job use it as a combiner; instead, write a separate object to combine the map outputs.
The classic example of using a combiner is the org.apache.hadoop.examples.WordCount example.
This MapReduce job reads a set of text input files and counts the frequency of occurrence of each word in the input files.
The map phase outputs each word in the file as a key, with the count of 1
There will be one output record for each word in the file.
The combiner will aggregate these into a set that contains one output record per unique word in the input, and the value is the number of times the word appeared in the input.
Unless the writer has such a large vocabulary that no word is used more than once, the combiner will greatly reduce the number of records to be processed by the reduce phase.
When the map task has completed and the partitions are sorted, the combiner may run over the partitions and aggregate values, reducing the total number of key/value pairs that must go over the network to the reduce task.
For example, suppose the map partition dataset originally contained the following:
After the combiner has completed, the map partition dataset would contain these keys and values:
It is fairly simply to shoot yourself in the foot with a combiner.
The combiner must not cause the loss of any information that is needed by the actual reducer.
The classic example of this is a reducer that computes the average of the values for each key.
If that reducer is also used as a combiner, the information on the number of records involved computing the average will be lost, and the reduce tasks will see only the average values for each key; the final result will be the average of the averages, instead of the actual average.
Combiners also must be idempotent, as they may be run an arbitrary number of times by the Hadoop framework over a given map task’s output.
File Types for MapReduce Jobs The Hadoop framework supports text files, binary (sequence) files, and map files, which are actually a pair of sequence files.
Let’s take a closer look at each of these file types.
Text Files The Hadoop framework supports a number of textual input files and output files.
TextInputFormat: This class reads each line of the input split and returns a record composed of the line number as a LongWritable key, and the line itself as a Text value.
The workhorse class that actually produces the key/value pairs is org.apache.
KeyValueTextInputFormat: This class reads each line and splits the line into a key/ value pair on a tab character.
The separator may be configured by setting the configuration key key.value.separator.in.input.line.
If there is no separator found, the value will be an empty string.
NLineInputFormat: This format is ideal for using the input data as control information.
It guarantees that each input split will be N lines long, with one split being the remaining lines.
The configuration key mapred.line.input.format.linespermap controls the number of lines of input per map task.
Under the covers, this uses org.apache.hadoop.mapred.LineRecordReader to read the input data and produce LongWritable, Text key/value pairs.
MultiFileInputFormat: This is an abstract class that provides a way for a single task to receive multiple input files as the task’s input split.
There is substantial time involved in setting up and starting a task, as well as collecting the results.
If the input split is small, a substantial portion of the job runtime may be in the setup and teardown of tasks.
The developer is responsible for implementing provides an example of a RecordReader that handles reading from multiple files.
It basically calls the toString method on each key and value, producing a single-line key SEPARATOR value ASCII newline for each output record.
The SEPARATOR is specified by the value of the configuration key apred.textoutputformat.separator, which defaults to TAB.
If the value is null, no SEPARATOR and no value will be emitted.
The end-of-record character is hard-coded as an ASCII newline character.
MultipleTextOutputFormat: This format allows you to write output records to different files based on the key and value.
The Java source to this class is located in src/test/org/apache/hadoop/mapred/ TestMultipleTextOutputFormat.java in your Hadoop distribution.
Using MultipleTextOutputFormat, the user has the option of interceding in the selection of an output file for each output key/value pair in several different ways by overriding different methods.
For	map-only	jobs,	a	portion	of	the	input	file	path	may	be	included	in	the	output path, by setting the value of the configuration key mapred.outputformat.
The default is no components of the input file path are used.
The value +1 worth of components from the right side of the input file are inserted in the output file path before the file name.
You	can	change	the	final	file	name	or	leaf	name	via	the	String generateLeafFileName(String name) method.
The leaf name is normally the part-XXXXX, where the XXXXX corresponds to the reduce ordinal number, or the map ordinal number if this is a map-only job.
You	can	change	the	path	to	the	output	file	via	the	String generateFileNameForKeyValue(K key, V value, String name) method.
You can construct arbitrary paths out of the key, value, and name.
The example in Listing 5-17 produces an output file name of the first letter of the key, a dash, and the partition number.
Caution It is critically important to minimize the number of HDFS files that are opened.
HDFS, through at least Hadoop 0.19.0, is designed for small numbers of very large files.
Opening many small files will bring your cluster to its knees, and may result in catastrophic failure of your job, as well as your HDFS.
It is very easy to open hundreds of thousands of files with MultipleOutputFormats.
Sequence Files Sequence files are a binary format for storing sets of serialized key/value pairs.
Sequence files support compression, encapsulate the key and value types, and provide validity checksums.
They are an ideal format to use for data that is expensive or complex to parse.
SequenceFileInputFormat: The basic workhorse, this format supports splitting and provides the key and value types.
If the input file is a map file (described in the next section), the data file is read.
SequenceFileAsBinaryInputFormat: This format returns the raw key and value bytes.
SequenceFileAsTextInputFormat: This format returns the key and value as text.
It calls the toString method on the key and value classes and returns the key/value pair as Text,Text.
SequenceFileInputFilter: This format returns only specific records from the sequence file.
It provides the static void setFilterClass(Configuration conf, Class filterClass) method, which supplies a class that is used to determine which records are returned by the next(key,value) method on the reader.
The FilterClass must implement the SequenceFileInputFilter.Filter interface and provide a method boolean accept(Object key)
RegexFilter.setPattern(Configuration conf, String regex) provides the regular expression to filter keys.
PercentFilter.setFrequency(Configuration conf, int frequency) provides the way of accepting one record in frequency records.
SequenceFileOutputFormat: This format writes the serialized key/value records as output.
The key and value types must be BytesWritable, and these raw bytes are written as the records.
Map Files Map files are a pair of sorted sequence files.
If a map file named mymap is created, there will be a directory mymap in HDFS, and two files in mymap: index and data.
The data sequence file contains the key/value pairs as records, where the records are sorted in key order.
The index sequence file is key/location information, where location is the location in data where the first record containing a key is located.
Map files provide a way to find a particular key, or region of a sorted file, without having to read the entire file.
The HBase project (http://hadoop.apache.org/hbase) provides a persistent distributed hash table stored in HDFS, using map files as the underlying storage.
When a map file is specified as a job input, the data file is used as the actual input.
There is not a MapFileInputFormat class; the SequenceFileInputFormat class is used.
The path specified is the path to the directory containing the index and data files.
SequenceFileInputFormat will use the data file as the input source.
Tip For best performance, it is strongly suggested that all key lookups be performed in the sort order of the underlying map file.
For the MapFileOutputFormat, the value of the configuration key io.map.index.interval determines how many records are written to the data sequence file between writes to the index sequence file.
The default is one index entry for every 128 records.
Map files provide the following methods for looking up key/value pairs.
Writable get(WritableComparable key, Writable val): Gets the value for key.
WritableComparable getClosest(WritableComparable key, Writable val): Gets the closest match to the key, searching as seek.
WritableComparable getClosest(WritableComparable key, Writable val, final method, unless before is true—in which case the key before is returned.
Compression The Hadoop framework supports several types of compression and several compression formats.
The framework supports the gzip, zip, sometimes LZO, and bzip2 compression codecs.
The framework will transparently compress and uncompress most input and output files.
Input files are uncompressed when the input file name has a suffix that maps to one of the known codecs, as shown in Table 5-3
It is incompatible with the Apache license and has been removed from some distributions.
I sincerely wish that this will be resolved and that native LZO becomes a standard part of the Hadoop distribution.
Codec Specification The Hadoop framework supports a number of codecs, with native implementations for a smaller number.
LzoCodec may not be available in some releases due to licensing issues.
The list of codecs is stored in the configuration under the key io.compression.codecs.
If your environment requires additional codecs, the glue interface is org.apache.hadoop.
You would then add the class name to the list of codecs in the io.compression.codecs value.
The selection of a compression codec is a choice between speed and compression rate.
Sequence File Compression Sequence files are binary record-oriented files, where each record has a serialized key and serialized value.
The Hadoop framework supports compressing and decompressing sequence files transparently.
The framework will transparently compress at the record level or the block level.
The key io.seqfile.compression.type controls the record- or block-level compression for sequence files.
In general, block-level compression is recommended, because it provides greater data reduction (at the expense of individual key access)
The compression overhead is less, and the compression ratio is much greater.
For sequence files that are being used as input to a map or reduce phase, block-level compression is ideal.
Sequence files that were written using transparent compression may be divided into multiple input splits by the framework.
Many sites will set the default to BLOCK in their hadoop-site.xml file, as follows:
Map Task Output The intermediate map task outputs are a set of sequence files, one per reduce task.
As these files must be transferred across the network, a low-overhead compression type, such as gzip or LZO,	can	provide	a	substantial	reduction	in	network	traffic	for	little	CPU	cost.
The	blog	entry at http://blog.oskarsson.nu/2009/03/hadoop-feat-lzo-save-disk-space-and.html has some interesting	information	about	compression	CPU	and	size	reductions	for	different	Hadoop codecs.
Note I have spent some time running the same job with different compression codecs and RECORD or BLOCK set for compression, to determine which combination gave the overall performance for the job.
Map output block-level compression may be specified by the job or in the site configuration.
If compressed, map output, destined for a reduce task, is always BLOCK compressed.
Listing 5-18 provides an XML block suitable for inclusion in the conf/hadoop-site.xml file to make LZO compression the default for the map task outputs.
Listing 5-19 demonstrates configuring a cluster to always use compression for final output files, and if the final output file is a sequence file, to use BLOCK compression.
JAR, Zip, and Tar Files The Hadoop framework knows how to unpack JAR, zip, and tar files, but this is only automatically done for archives passed via the DistributedCache object The class org.apache.hadoop.
Summary The Hadoop Core framework provides a rich set of tools to support a variety of use cases.
As with most powerful tools, using them effectively requires training and experience.
This chapter has provided a solid foundation for configuring jobs to run successfully and building classes that will actual perform the work for the job.
The effective use of counters in the map and reduce methods provides both the application writer and the organization with metrics for job performance.
The DistributedCache object provides a way of distributing required data to all of the tasks, without needing to have the data already available on the TaskTracker nodes.
You can choose from a variety of input and output formats.
The use of compression can greatly reduce the wall clock runtime of a job, as can the use of a combiner.
The KeyFieldBasedComparator and KeyFieldBasedPartitioner classes allow you to implement a secondary sort via the OutputValueGroupingComparator.
You	also	know how to use MultipleTextOutputFormat, and the potential problems it can bring.
It is now time to have fun writing MapReduce jobs!
Once you have developed your MapReduce job, you need to be able to run it at scale on your cluster.
This chapter will cover how to recognize that your job is having a problem and how to tune the scaling parameters so that your job performs optimally.
The framework provides several parameters that let you tune how your job will run on the cluster.
Most of these take effect at the job level, but a few work at the cluster level.
With large clusters of machines, it becomes important to have a simple monitoring framework that provides a visual indication of how the cluster is and has been performing.
Having alerts delivered when a problem is developing or occurs is also essential.
Finally, you’ll get some tips on what to do when your job isn’t performing as it should.
This chapter is focused on tuning jobs running on the cluster, rather than debugging the jobs themselves.
Tunable Items for Cluster and Jobs Hadoop Core is designed for running jobs that have large input data sets and medium to large outputs, running on large sets of dissimilar machines.
The framework has been heavily optimized for this use case.
Hadoop Core is optimized for clusters of heterogeneous machines that are not highly reliable.
The HDFS file system is optimized for small numbers of very large files that are accessed sequentially.
The data stored in HDFS is generally not considered valuable or irreplaceable.
The service level agreement (SLA) for jobs is long and can sustain recovery from machine failure.
Users commonly get into trouble when their jobs input large numbers of small files, output large numbers of small files, or require random access to files.
Another problem is a need for rapid access to data or for rapid turnover of jobs.
Hadoop Core does not provide high availability for HDFS or for job submission, and special care must be taken to ensure that required HDFS data can be recovered in the event of a critical failure of the NameNode.
Behind the Scenes: What the Framework Does Each job has a number of steps in its execution: the setup, the map, the shuffle/sort, and the reduce.
The framework sets up, manages, and tears down each step.
Note The following discussion assumes that no other job is running on the cluster and that on submission, the job is immediately started.
On Job Submission The framework will first store any resources that must be distributed in HDFS.
These are the resources provided via the -files, -archives, and -libjars command-line arguments, as well as the JAR file indicated as the job JAR file.
If there are a large number of resources, this may take some wall clock time.
The XML version of the JobConf data is also stored in HDFS.
The replication factor on these resource items is set to the value stored in the configuration under the key mapred.submit.replication, with a default value of 10
The framework will then examine the input data set, using the InputFormat class to determine which input files must be passed whole to a task and which input files may be split across multiple tasks.
The framework will use the parameters listed in Table 6-1 to determine how many map tasks must be executed.
Input formats may override this; for instance, NLineInputFormat forces the splits to be made by line count.
The parameters in Table 6-1 are used to compute the actual split size for each input file.
The input format for the input file is responsible for indicating if the underlying file the input files.
For inputs that can be split, three things are computed before the actual split size is determined: the goal size, which is the total input size divided by JobConf.
A	split	will	be	no	smaller	than	the	remaining	data	in	the	file	or	minSize.
A	split	will	be	no	larger	than	the	lesser	of	the	goalSize and the blockSize.
Tip Through at least Hadoop 0.19.1, compressed files may not be split.
In general, a cluster will have the mapred.map.tasks parameter set to a value that approximates the number of map task slots available in the cluster or some multiple of that value.
The ideal split size is one file system block size, as this allows the framework to attempt to provide data locally for the task that processes the split.
The end result of this process is a set of input splits that are each tagged with information about which machines have local copies of the split data.
The splits are sorted in size order so that the largest splits are executed first.
The split information and the job configuration information are passed to the JobTracker for execution via a job information file that is written to HDFS.
Some jobs require that the input files not be split.
The simplest way to achieve this is to set the value of the configuration parameter mapred.min.split.size to Long.MAX_VALUE: JobConf.
Map task Submission and execution The JobTracker has a set of map task execution slots, N per machine.
Each input split is sent to a task execution slot for execution.
Sending tasks to a slot that is hosted on the machine that has a local copy of the input split data minimizes network I/O.
If there are spare execution slots, and map speculative execution is enabled, multiple instances of a map task may be scheduled.
In this case, the results of the first map task to complete will be used, the other instances killed, and the output, including the counter values, removed.
When map speculative execution is not enabled, only one instance of a map task will be run at a time.
The TaskTracker on the machine will receive the task information, and if necessary, unpack all of the DistributedCache data into the task local directory and localize the paths to that data in the JobConf object that is being constructed for the task.
With speculative execution for map tasks disabled, the only time more than one instance of a map task will occur in the job will be if the task is retried after failing.
Caution The framework is able to back out only counter values and output files written to the task output directory.
Any other side effects of killed speculative execution tasks or failed tasks must be handled by the application.
The TaskTracker picks a map runner class based on the content of the key mapred.
Its choices are the standard MapRunner, which runs a single thread; the MultithreadedMapRunner, which runs mapred.map.multithreadedrunner.threads (the default.
A child JVM is allocated to run the mapper class, and the map task is started.
The output data of the map task is partitioned and sorted by the output partitioner class and the output comparator class, and aggregated by the combiner class, if one is present.
The result of this will be N sequence files on disk: one for each reduce task, or one file if there is no reduce task.
Each time the map method is called, an output record is emitted, or the reporter object is interacted with, a heartbeat timer is reset.
If this timeout expires, the map task is considered hung and terminated.
If a terminated task has not failed more than the allowed number of times, it is rescheduled to a different task execution slot.
A failing task may have a debugging script invoked on it if the value of the configuration key mapred.map.task.debug.script is the path to an executable program.
The script is invoked with the additional arguments of the paths to the stdout, stderr, and syslog output files for the task.
See this book’s appendix, which covers the JobConf object, for details on how to configure a debugging script for failing tasks.
When a task finishes, the output commit class is launched on the task output directory, to decide which files are to be discarded and which files are to be committed for the next step.
The class name is stored in the configuration under the key mapred.output.committer.class and has the default class FileOutputCommitter.
If less than the required number of tasks succeed, the job is failed and the intermediate output is deleted.
The TaskTracker will inform the JobTracker of the task’s success and output locations.
Merge-Sorting The JobTracker will queue the number of reduce tasks as specified by the JobConf.
The JobTracker will queue these reduce tasks for execution among the available reduce slots.
The TaskTracker that receives a reduce task will set up the local task execution environment if needed, and then fetch each of the map outputs that are destined for this reduce task.
The number of pieces that are fetched at one time is configurable.
The value stored in the configuration under the key mapred.reduce.parallel.copies determines how many fetches are done in parallel.
A number of parameters control how the merge-sorting is done, as shown in Table 6-2
This parameter often causes jobs to run out of memory on small memory machines.
The reduce output is written to the local file system.
On successful completion, the output commit class is called to select which output files are staged to the output area in HDFS.
If more than the allowed number of reduce tasks fail, the job is failed.
Once the reduce tasks have finished, the job is done.
Writing to hDFS There are two cases for an HDFS write: the write originates on a machine that hosts a DataNode of the HDFS cluster for which the write is destined, or the write originates on a machine that does not host a DataNode of the cluster.
In both cases, the framework buffers a file system block-size worth of data in memory, and when the file is closed or the block fills, an HDFS write is issued.
The write process requests a set of DataNodes that will be used to store the block.
If the local host is a DataNode in the file system, the local host will be the first DataNode in the returned set.
The set will contain as many DataNodes as the replication factor requires, up to the number of DataNodes in the cluster.
The replication factor may be set via the configuration key dfs.replication, which defaults to a factor of three, and should never be less than three.
The replication for a particular file may be set by the following:
The block is written to the first DataNode in the list, the local host if possible, with the list of DataNodes that are to be used.
On receipt of a block, each DataNode is responsible for initiating the transfer of the block to the next DataNode in the list.
This allows writes to HDFS on a machine that hosts a DataNode to be very fast for the application, as they do not require bulk network traffic.
Cluster-Level Tunable Parameters The cluster-level tunable parameters require a cluster restart to take effect.
Some of them may require a restart of the HDFS portion of the cluster; others may require a restart of the MapReduce portion of the cluster.
These parameters take effect only when the relevant server starts.
Server-Level parameters The server-level parameters, shown in Table 6-3, affect basic behavior of the servers.
In general, these affect the number of worker threads, which may improve general responsiveness of the servers with an increase in CPU and memory use.
The variables are generally configured by setting the values in the conf/hadoop-site.xml file.
It is possible to set them via command-line options for the servers, either in the conf/hadoop-env.sh file or by setting environment variables (as is done in conf/hadoop-env.sh)
Quite often, the operating system-imposed limit is too low, and the administrator must increase that value.
The value 64000 is considered a safe minimum for medium-size busy clusters.
Caution A number of difficult-to-diagnose failures happen when an application or server is unable to allocate additional file descriptors.
Java application writers are notorious for not closing I/O channels, resulting in massive consumption of file descriptors by the map and reduce tasks..
Caution Hadoop Core uses large numbers of file descriptors in each server.
Rarely is the system default of 1,024 sufficient for the Hadoop servers or Hadoop jobs.
Most installations find that a minimum limit of 64,000 is required.
If you see errors in your log files that say Bad	connect	ack	with	firstBadLink, Could	not	obtain	block, or No	live	nodes	contain	current	block, you must increase the file descriptor limit for your Hadoop servers and jobs.
The larger this value, the fewer individual blocks will be stored on the DataNodes, and the larger the input splits will be.
The DataNodes through at least Hadoop 0.19.0 have a limit to the number of blocks that can be stored.
After this size, the DataNode will start to drop in and out of the cluster.
If enough DataNodes are having this problem, the HDFS performance will tend toward full stop.
When computing the number of tasks for a job, a task is created per input split, and input splits are created one per block of each input file by default.
There is a maximum rate at which the JobTracker can start tasks, at least through Hadoop 0.19.0
The more tasks to execute, the longer it will take the JobTracker to schedule them, and the longer it will take the TaskTrackers to set up and tear down the tasks.
The other reason for increasing the block size is that on modern machines, an I/O-bound task will read 64MB of data in a small number of seconds, resulting in the ratio of task overhead to task runtime being very large.
A downside to increasing this value is that it sets the minimum amount of I/O that must be done to access a single record.
If your access patterns are not linearly reading large chunks of data from the file, having a large block size will greatly increase the disk and network loading required to service your I/O.
The DataNode and NameNode parameters are presented in Table 6-4
The trash is used only for deletions done via the hadoop	dfs	-rm series of commands.
If specified, only the hosts in this file are permitted to connect to the NameNode.
If the file does not exist, no hosts are blacklisted.
If a set of DataNode hostnames are added to this file while the NameNode is running, and the command hadoop	dfsadmin	-refreshNodes is executed, the DataNodes listed will be decommissioned.
Any blocks stored on them will be redistributed to other nodes on the cluster such that the default replication for the blocks is satisfied.
It is best to have this point to an empty file that exists, so that DataNodes may be decommissioned as needed.
The interval in seconds that the NameNode  300 interval  checks to see if a DataNode decommission has.
If this value is 0, no access times are maintained.
Setting this to 0 may increase performance on busy clusters where the bottleneck is the NameNode edit log write speed.
Larger values allow more DataNodes to fail before blocks are unavailable but increase the amount of network I/O required to store data and the disk space requirements.
Large values also increase the likelihood that a map task will have a local replica of the input split.
This may  67108864 be too small or too large for your cluster, depending on your job data access patterns.
Increasing this may increase DataNode throughput, particularly if the DataNode uses multiple separate physical devices for block storage.
The  3600000 block report does a scan of every block that is stored on the DataNode and reports this information to the NameNode.
This is commonly greatly increased in busy and large clusters.
A copy will be kept in each loca- dfs/name, tion.
If  in /tmp by this data is lost, your entire HDFS data set is lost.
Ideally, this should hold multiple locations on separate physical devices.
If this is lost, your last few minutes of changes will be lost.
This list will be used in a round- tmp.dir}/ robin fashion for storing new data blocks.
Jobtracker and tasktracker tunable parameters The JobTracker is the server that handles the management of the queued and executing jobs.
The TaskTrackers are the servers that actually execute the individual map and reduce tasks.
A  local value of local means to run the job in the current JVM with no more than 1 reduce.
If the configuration specifies local, no JobTracker server will be started.
The number of server threads for handling 10 count  TaskTracker requests.
If this value is true, a JobTracker will attempt false recover  to restart any queued or running jobs that.
The basic block size used for writes to the 3145728 history.block.size  history file.
The number of jobs to be kept in the 100 completeuserjobs.maximum  JobTracker history.
The maximum number of tasks allowed for a -1 per.job  single job.
The maximum number of tasks a job can Unlimited taskScheduler.
Determines whether job status results are false jobstatus.active persisted to HDFS.
The number of hours that job status infor- 0 jobstatus.hours mation is kept.
If specified, only the hosts in this file are permitted.
If multiple directories are provided,  tmp.dir}/ the usage is spread over the multiple direc- mapred/local tories.
If the space available in the directories 0 minspacestart  specified by mapred.local.dir falls below.
This prevents tasks from failing due to lack of temp space.
The 0 value should be changed to something reasonable for your jobs.
The number of msec without a heartbeat 600000 interval  that a TaskTracker may go without reporting, before being considered hung and being killed.
This is used  Unlimited for processes started by the org.apache.
The framework uses this to launch external subprocesses, such as the pipes jobs and the external programs of streaming jobs.
The rate in msec that virtual memory use 5000 taskmemorymanager.
The maximum amount of virtual memory a -1 maxmemory  task and its children may use before the.
A task over its memory limit is sent a 5000 procfsbasedprocesstree.
If the task has not exited within sleeptime-before-sigkill  this time in msec, a SIGKILL is sent.
This should either be 1 (if there is only one CPU) or roughly one less than the number of CPUs on the machine.
This parameter needs to be tuned for a particular job mix.
This value is really a function of the CPU and I/O bandwidth available to the machine.
It needs to be tuned for the machines and job mix.
For multihomed TaskTracker nodes, report default interface  this interface’s IP address to the JobTracker.
If not default, this value is the name of a network interface, such as eth0
For multihomed TaskTracker nodes, use default nameserver  this address for DNS hostname resolution.
If your system has many reduce execution slots, the default may be too small.
Per-Job Tunable Parameters The framework provides rich control over the way individual jobs are executed on the cluster.
Table 6-7 shows the tunable parameters for the file system.
Nor- file:/// mally it will be set to hdfs://NamenodeHostname: NameNodePort.
In general, if writes are being retried, there is a problem with the HDFS or machine configuration.
The task-tunable parameters directly control the behavior of tasks in the cluster.
Only those parameters that directly control core functions are listed in Table 6-8
Many of the parameters are detailed in this book’s appendix, which discusses the JobConf object.
If it is a relative path, it will be relative to the task’s local working directory.
Whether idle map task slots will be used to true execution  set up execution races for executing identical.
This will consume more cluster resources and may offer faster job throughput.
This must be false if your map tasks have side effects that the framework cannot undo or have real costs.
Enable the use of unused reduce task execu- true speculative.execution  tion slots to try a task in multiple slots, to see.
This will consume more cluster resources and may offer faster job throughput.
This must be false if your reduce tasks have side effects the framework cannot undo or have real costs.
This  10 needs to be tuned on a per-job basis.
There is no automatic mechanism in the framework to clean these directories if this is set to false.
This is usually a significant win for jobs with large output.
See this  0-2 book’s appendix for how this may be set.
See this  0-2 book’s appendix for how this may be set.
The number of failures of a task before skip 2 start.skipping mode is engaged.
This must be false for streaming jobs or jobs that buffer records before reducing.
If the value is exactly none, no records will be written.
If set to anything else, it becomes the directory where skipped records are written.
The framework will attempt to narrow down the region to skip to this size.
The number of key/value set groups sur- 0 skip.groups rounding a bad record group that may be skipped by the reduce task.
Monitoring Hadoop Core Services To be able to detect incipient failures, or otherwise recognize that a problem is developing or has occurred, some mechanism must be available to monitor current status, and if possible provide historical status.
The Hadoop framework provides several APIs for allowing external agents to provide monitoring services to the Hadoop Core services.
This allows for the use of JMX-aware applications to collect information about the state of the servers.
The default configuration provides for only local access to the managed beans (MBeans)
To enable remote access, after determining a port for JMX use, alter the conf/hadoop-env.sh file (shown in Listing 6-1) and change the JMX properties being set on the servers.
The string -Dcom.sun.management.jmxremote enables the JMX management bean services in the servers.
The string is a JVM argument and passed to the JVM at start time on the command line.
See the Sun-supplied documentation for configuring access control and remote access, at http://java.sun.com/javase/6/docs/technotes/ guides/jmx/index.html.
Nagios: A Monitoring and Alert Generation Framework Nagios  (http://www.nagios.org) provides a flexible customizable framework for collecting data about the state of a complex system and triggering various levels of alerts based on the collected data.
A service of this type is essential for your cluster administration and operations team.
This example assumes that you understand how to construct the JMX password file and access control file.
To enable JMX monitoring on Hadoop, add the following lines to hadoop-env.sh:
Ganglia by itself is a highly scalable cluster monitoring tool, and provides visual information on the state of individual machines in a cluster or summary information for a cluster or sets of clusters.
Ganglia provides the ability to view different time windows into the past, normally one hour, one day, one week, one month, and so on.
Caution Due to some limitations in the Ganglia support in Hadoop through at least Hadoop 0.19.1, the configuration requirements are not as simple as Ganglia configuration normally is.
Ganglia is composed of two servers: the gmetad server, which provides historical data and collects current data, and the gmond server, which collects and serves current statistics.
The Ganglia web interface is generally installed on the host(s) running the gmetad servers, and in coordination with the host’s httpd provides a graphical view of the cluster information.
In general, each node will run gmond, but only one or a small number of nodes will also run gmetad.
For Hadoop reporting to work with Ganglia, the configuration changes shown in Table 6-9 must be made in the conf/hadoop-metrics.properties file.
Each cluster must also be allocated a unique cluster name.
The cluster name is referred to as CLUSTER in this section.
The UDP port for reporting is referred to as PORT, and for simplicity, the multicast port will be identical to PORT.
Also the multicast port unique to CLUSTER, which all gmond servers in CLUSTER will listen and transmit on.
The default of 239.2.11.71 is acceptable as long as each CLUSTER uses a unique PORT.
Note MULTICAST:PORT must be unique per CLUSTER, but generally MULTICAST is left at the default value, so PORT becomes the unique value per cluster.
The gmond on HOSTNAME will need to be configured to listen on the non-multicast UDP port of PORT.
Many enterprise-grade switches will need to have multicast enabled for each CLUSTER’s MULTICAST:PORT.
All nodes in the cluster will have the gmond server configured with the cluster name parameter set with the cluster’s unique name.
One node in the cluster, traditionally the NameNode or a JobTracker node, is configured to also accept non-multicast reporting on a port, commonly the same port as the multicast reception port.
This host will be considered the Ganglia cluster master, and its hostname is the value for HOSTNAME.
This host is also the host used in the /etc/gmetad.conf file.
The conf/hadoop-metrics file needs to be altered as shown in Listing 6-2
The HOSTNAME and PORT must be substituted for the actual values.
This file must then be distributed to all of the Hadoop conf directories and all Hadoop servers restarted.
All of the Hadoop servers will now deliver metric data to HOSTNAME:PORT via UDP, once every 10 seconds.
The gmetad server that will collect metric information for the cluster will need to be instructed to collect metric information about CLUSTER from the master node via a TCP connection to HOSTNAME:PORT.
The following is the configuration line in the gmetad.conf file for CLUSTER:
The Ganglia web interface will provide a graphical view of the clusters, as shown in Figure 6-1
The Ganglia web view of a running set of clusters.
When tuning jobs, Ganglia provides a wonderful interface to determine when your job is fully utilizing a cluster resource.
Determining which resource is fully utilized, tuning the appropriate configuration parameters for that resource, and then rerunning the job will allow you to optimize your job’s runtime on your cluster.
Chukwa: A Monitoring Service Chukwa’s goal is to provide extract, transform, and load (ETL) services for cluster logging data, thereby providing end users with a simple and efficient way to find the logging events that are actually important.
Chukwa uses HDFS to collect data from various data providers, and MapReduce to analyze the collected data.
The instance in Hadoop 0.19.0 appears to be currently optimized for the collection of data from log files, and then run a scheduled MapReduce job over the collected data.
FailMon: A Hardware Diagnostic Tool The FailMon framework attempts to identify failures on large clusters by analyzing data collected from the Hadoop logs, the system logs, and other sources.
The FailMon tools stem from a larger IBM effort to improve the operational reliability of large installations by predicting failures and taking corrective action before the failure occurs (see (https://issues.apache.
This is a very early technology and is expected to evolve rapidly.
The FailMon package consists primarily of data collection tools with MapReduce jobs to perform analysis of the collected data.
Tuning to Improve Job Performance The general goal for tuning is for your jobs to finish as rapidly as possible using no more resources than necessary.
This section covers best practices for achieving optimum performance of jobs.
Speeding Up the Job and Task Start If the job requires many resources to be copied into HDFS for distribution via the distributed cache, or has large datasets that need to be written to HDFS prior to job start, substantial wall clock time can be spent copying in the files.
For constant resources, it is simplest and quickest to make them available on all of the cluster machines and adjust the TaskTracker classpaths to reflect these resource locations.
The disadvantage of installing the resources on all of the machines is that it increases administrative complexity, as well as the possibility that the required resources are unavailable or an incorrect version.
The advantage of this approach is that it reduces the amount of work the framework must do when setting up each task and may decrease the overall job runtime.
Table 6-10 provides a checklist of items to look for that affect write performance and what to do when the situations occur.
Source machine CPU  The CPU is maxed out, the com- Change the compression or change utilization pression level is too high, or the  the number of threads.
Source machine network  Saturation of the outbound net- Increase the number of transfer work connection with traffic for  threads or provide a higher-speed HDFS network connection.
Per DataNode network input  If it is not saturated, more writes  Increase the number of simultacould be delivered to this DataNode  neous threads writing or reduce the.
DataNode I/O wait I/O contention on a DataNode  Add more independent locations to dfs.data.dir or add more DataNodes.
If you have a large number of files that need to be stored in HDFS prior to the task start, such as might occur if your job needs to populate the job input directory, there are several things you may try, in varying combinations:
It	may	be faster to copy the files from a machine that hosts a DataNode, as all of the writes will first go to the local DataNode, and the application will not have to wait for the data to traverse the network.
The downside is that one replica of every block will end up on the local DataNode, greatly reducing the opportunity for data to be local to a map task.
The DataNode may also get unbalanced with respect to storage, compared to other DataNodes.
Ideally, bulk input of data to be used as input to a map task should be input from a host that does not also provide DataNode services, to ensure even distribution of the stored blocks across the DataNodes.
It	may	be	faster	to	run	the	copies	in	parallel.
The	limiting	factor	will	be	the	network speed or the local DataNode disk speed in the event the copy host is also a DataNode.
Create	an	archive	of	the	input	files,	so	that	fewer	files	need	to	be	created	in	HDFS.
The downside is that zip and tar archives must be processed whole by a map task and may not be split into pieces (at least through Hadoop 0.19.0)
Writing compressed sequence files, where the key/value pairs are of the type BytesWritable, will give you input that may be split and a reduction in file size.
If	you	have	large	volumes	of	data,	you	may	need	to	set	up	special	machines	with high-bandwidth network connections to the switching fabric that holds your DataNodes.
Each block being written is sent directly to a DataNode.
That DataNode will in turn send the block to the next DataNode in the chain and so on, until the required number of replicas are complete.
If	the	origination	machine	has	a	higher	bandwidth	connection	and	is	able	to	write	multiple blocks in parallel (via multiple open files) while the bandwidth to each DataNode will be capped by the DataNode network speed, the origination machine will be able to write to HDFS at a higher rate.
You may change the dfs.block.size parameter to issue larger or smaller writes.
You may decrease the dfs.replication parameter to reduce the overall HDFS write load, or increase it to increase the chance of local access by later MapReduce jobs.
Figure 6-2 illustrates how HDFS operations that your application issues are actually handled by the framework.
Implicit in Figure 6-2 is that the replication count is three.
From a monitoring perspective, you will want to monitor the network utilization on the upload machine and to a lesser extent on the DataNodes.
If you are using compression, you will want to monitor CPU utilization on the machines doing the compression.
You may also wish to monitor the disk-write rate on the DataNodes, to verify that you are getting a good write rate.
Since the incoming data rate is capped by the network rate, generally this is not a significant factor.
If you see pauses in the network traffic or disk I/O, it implies that a Hadoop server may be unresponsive, and the client is timing out and will retry.
In general, increasing the server threads (dfs.datanode.handler.count) and the TCP listen queue depth (ipc.server.listen.queue.size) may help.
It may be that the NameNode is not keeping up with requests, and in that case, increasing dfs.namenode.handler.count may help.
Optimizing a Job’s Map Phase The map phase involves dispatching a map task to a TaskTracker.
Once the TaskTracker has a task to execute, it will prepare the task execution environment by building or refreshing the DistributedCache data.
If you don’t have an existing child JVM that has been used for this job’s task and is within its reuse limit, start a new child JVM.
The TaskTracker will then trigger the start of the map task in the child JVM.
The child JVM will start reading key/value pairs from its input, executing the map method for each pair.
The output key/value pairs will be partitioned as needed and collected in the proper output format.
If there is a reduce phase, the output format will be on the local disk in a sequence file.
If there is not a reduce phase, the output will be in the job-specified output format and stored in HDFS.
Figure 6-3 shows a diagram of the job setup and map task execution.
The left side follows the actions of the JobTracker from job submission through executing the map tasks on the available TaskTrackers.
The right side follows the loop that a TaskTracker executes for map tasks.
Tasktracker$Child Set up to read input split from HDFS and write output to local file system.
The following are some items you can tune for the map phase:
Map task run time: Each map task has some setup and teardown overhead.
If the runtime of the map task is short, this overhead becomes the major time component.
If the runtimes of tasks are too long, a single task may hold the cluster for a long period, or retrying a failed task becomes expensive.
In general, less than a minute is usually too short, What is too long for a map task is job-specific.
The primary tuning point for this is the dfs.block.size parameter.
Increasing this parameter usually increases the split size and the task run time.
On a per-job or per-cluster basis, you may also change mapred.min.split.size.
It is better to use dfs.block.size, as the data is more likely to be local when the split size equals the HDFS file system block size.
TaskTracker node CPU utilization: If the map tasks are computationally intensive, a significant goal is to use all of the available CPU resources for that computation.
There are two methods for controlling CPU utilization for map tasks:
The	job	or	cluster	may	configure	the	use	of	MultithreadedMapRunner for the MapRunner via mapred.map.runner.class, and specify the number of execution treads via mapred.map.multithreadedrunner.threads.
The	cluster	may	specify	the	number	of	map	tasks	to	run	simultaneously	by	a	TaskTracker via mapred.tasktracker.map.tasks.maximum.
This may be done on the command line for any job that uses the GenericOptionsParser.
Data location: If the map tasks are not receiving their input split from a local DataNode, the I/O performance will be limited to the network speed.
Other than increasing the replication factor and trying to ensure that the input split size is the file system block size, there is little tuning to be done.
Child garbage collection: If there is significant object churn in the Mapper.map method, and there is insufficient heap space allocated, the JVM hosting the task may spend a significant amount of wall clock time doing garbage collection.
This is readily visible only via the Ganglia reporting framework or through the JMX MBean interface.
Figure 6-4 shows an example of a Ganglia report for a two-host cluster, where one host is having problems.
This would imply that the child JVM has been configured with insufficient memory.
At the current time, it is not possible to differentiate the garbage collection timing for the different server processes.
Ganglia report showing gcTime for a two-host cluster, where one host is in trouble.
In this case, it’s possible that increasing the child JVM memory limit, via mapred.child.java.opts, would be helpful.
In this 10-minute window, the same task was run twice.
The second time, it was run with twice as much memory per child JVM via mapred.child.java.opts.
Note how much less time was taken in garbage collection on the right side of the graph for cloud9 versus the left half of the graph.
Here are the command-line options to enable multithreaded map running with ten threads:
Tuning the Reduce Task Setup The reduce task requires the same type of setup as the map task does with respect to the DistributedCache and the child JVM working environment.
The two key differences relate to the input and the output.
The reduce task input must be fetched from each of the TaskTrackers on which a map task has run, and these individual datasets need to be sorted.
The reduce output is written to HDFS, unlike with the map task, which has output to the local file system.
As you can see from Figure 6-5, there are several steps for a reduce task, each of which has different constraints, as follows:
The	JobTracker	can	launch	only	so	many	tasks	per	second;	this	is	something	that	will change after Hadoop 0.19.1
The	tuning	parameters	for	the	map	task	with	respect	to	job	setup	apply	equally	to	the reduce task.
The	framework	must	fetch	all	of	the	map	outputs	for	the	reduce	task,	from	the TaskTrackers that have them.
The	data	to	be	fetched	may	be	large	enough	that	the	network	transfer	speed	becomes a bounding issue.
There are several ways to reduce the size of the map output files, which can greatly speed up this phase.
Some care needs to be used, as some of the compression options may slow down the shuffle and sort phase.
The simplest thing is to specify a combiner class, which will act as a mini-reduce phase in each map task (as described in Chapter 5)
This works very well for aggregation jobs, and not so well for jobs that need the full value space in the reduce task on which to operate.
Many sites will enable map output file compression, via the Boolean value mapred.compress.map.output, in the hadoop-site.mxl file.
Conceptually, RECORD is better for the map output, as there will be a fair bit of reading through the files during the shuffle and sort phases.
This is something that will have to be tried on a per-job basis.
The other issue is that, at least through Hadoop 0.19.0, there is only one setting for this parameter, which affects all SequenceFiles.
Note There are a number of parameters that control the shuffle and merge.
Choosing the number of reduce tasks to run per machine and per cluster is the final level of tuning.
A major determinant here is how the output data will be used, and that is application-specific.
With reduce tasks, I/O, rather than CPU usage, is usually the bottleneck.
If the DataNodes are coresident with the TaskTrackers, the reduce tasks will always have a local DataNode for the output.
This will allow the initial writes to go at local speed, but the file closes will block until all the replicas are complete.
It is not uncommon for jobs to open many files in the reduce phase, which generally causes a huge slowdown, if not failure, in the HDFS cluster, so the job will take a significant amount of time to finish.
The following are some tuning points for the reduce phase:
Shuffle/sort time: The shuffle and sort cannot complete until all of the map output data is available.
If this is an issue, you can try the following:
Change	your	algorithm	so	that	less	data	needs	to	pass	to	the	reduce	phase.
Try more reduce tasks, to reduce the volume of data that each reduce phase must sort.
Network saturation: The pull of the map outputs should just saturate your network.
If the reduce tasks are timing out while trying to fetch outputs, increase the tasktracker.http.threads.
If the network is saturated, enable compression, reduce the number of map tasks, improve the combiner class, or restructure the job to reduce the data passed to the reduce phase.
Note I once had a job where part of the value associated with each key was a large block of XML data that was unused by the reduce phase.
Modifying the map to drop the XML data provided a tenfold improvement.
Actual reduce time: You may find that the time to actually reduce the data, after the shuffle and sort are done, is too long.
If you are using MultipleOutputFormat, ensure that the number of files being created is small.
If many small files must be created, write them as a zip archive.
The Ganglia gmetric value FilesCreated will give you an idea of the rate of HDFS file creation.
Write time: The write time may be too long if the volume of data or the number of files are large.
Pack multiple files into zip files or other archive formats.
Note I had one job that needed to create many tens of thousands of small files.
Writing the files as a zip archive in HDFS resulted in a hundredfold speed increase.
Overall reduce phase time: If the reduce phase is very long, you may want to tailor the number of reduce tasks per job and per machine.
The job may specify the number of reduce tasks to run, but at least through Hadoop 0.19.1, the number of reduce tasks per TaskTracker is fixed at start time.
If your cluster will run a specific set of jobs, with experimentation, you may find a reasonable number for the cluster-level parameter, and given that, identify a specific value for the number of reduce tasks for each job.
Addressing Job-Level Issues One of the more interesting things to see is a cluster going almost idle while a job is running.
This usually happens because a small number of tasks have not finished.
This can happen with the map tasks or the reduce tasks.
With multithreaded map tasks, the key or keys passed to one thread can sometimes take much longer to finish, and the task needs to wait for one or a small number of threads to complete, leaving the machine mostly idle.
In one particular job, there was a large variance in the time it took to process a key: some keys took three hours, and others three seconds.
If the long-running keys came late in an input split, the task would end up running one thread and idle six of the processors on the machine.
The only solution for this was to reorder the input keys so that the long-running keys came first in the splits, or to abandon the longrunning keys after a set elapsed run time, and reprocess all of the long-running keys in an additional job later.
Dealing with the Job tail The Hadoop standard is for very large jobs, spread over many machines, such that the time of one or two tasks is small compared to the run time of the job.
These organizations must tune their jobs so that the clusters are well utilized.
The job tail really comes down to either a small number of reduce tasks taking much longer than others, either because the partitioning of the key space is very uneven or the duplicate keys fall unevenly in the partitions.
The net result is that some reduce tasks have substantially more work to do.
Tuning the number of reduce tasks so they fall evenly on your reduce slots may also help.
Having one reduce task start after all the rest of the reduce tasks have finished can drastically increase the job runtime.
Summary This chapter detailed how jobs are run by the Hadoop Framework and how MapReduce application writers and cluster administrators can tune both jobs and clusters for optimal performance.
The NameNode, JobTracker, DataNodes, and TaskTrackers have a number of start time parameters that directly affect how jobs are executed by the cluster and the overall run time of the jobs.
The execution of a job is performed in several steps: setup, map, shuffle/sort, and reduce.
It’s possible to do some tuning to improve performance in each step.
This chapter discussed several tools for monitoring clusters and jobs.
Ganglia is the tool I prefer for tuning and general dashboard-level awareness, and Nagios is the one I use for operational support.
Using these tools enables rapid recognition of problems with jobs and clusters and provides insight into what parameters may need to be tuned, before the CEO calls you into the front office to explain why the mission-critical jobs haven’t been running successfully.
Ganglia also provides the informative, pretty graphs that higher-level management like so much.
Is it working?” will be addressed by writing unit tests.
The agile programming model suggests starting with unit tests and building the code afterwards.
I generally try to follow this model, although under pressure, I have shifted to just writing the code, usually to my later regret.
Testing the lower-level APIs in your applications is a known skill set.
This chapter will cover unit tests that actually run MapReduce jobs at a small scale.
There are several ways to determine what is happening in a running program.
The first level is through examining the log messages or other job output data.
This requires detailed understanding of the output and may not provide sufficient information to isolate the problem.
An alternative is to put custom code into the application to trigger different behavior or logging around the area of code that is in question.
The most comprehensive method is to attach to the running application with a debugger and step through the execution of the code.
This chapter will cover interactive debugging, using the Eclipse platform to provide the graphical interface.
Unit Testing MapReduce Jobs MapReduce jobs, by their very nature, don’t lend themselves to the traditional unit testing model.
The common approach is to verify that all of the lower-level APIs are working correctly through their own unit tests.
Next, you build small test datasets that have known outcomes and run them on a simulated MapReduce cluster, and then examine the output.
If a real cluster is available for testing, the test run time will be shorter, but the tests must be coordinated among the cluster users.
The unit tests covered here are built on the Hadoop basic test case class org.apache.
This class provides a basic JUnit 3 test base that will start and stop a mini-HDFS with two DataNodes and a NameNode, and a mini-MapReduce cluster with two TaskTrackers and a JobTracker.
This is a complete cluster, with web consoles for the JobTracker and the NameNode.
All of the servers will run on the local machine, and the ports will be chosen from the free ports on the machine.
The virtual cluster will be established and torn down for each test that any derived classes execute.
In the archive that contains a Hadoop release are a number of prebuild release-specific JAR files.
These JAR files are named in the form hadoop-major release-minor release-component name.jar.
The standard Hadoop JARs are found in the root directory of the installation.
The root directory will contain JAR files for Ant, Core, examples, test, and tools.
Each component may have an associated lib directory containing JAR files on which the component depends.
By convention, the lib directory is located in the same directory as the component JAR.
In this chapter, I refer to the JARs as hadoop-rel-component.jar, where component is replaced with the actual component name, such as hadoop-<rel>-core.jar for the Hadoop Core JAR.
Requirements for Using ClusterMapReduceTestCase The ClusterMapReduceTestCase class, like all Hadoop Core classes, makes strong assumptions about the runtime environment.
For Hadoop Core unit tests, and for running standard jobs, the Hadoop Ant environment and the bin/hadoop script configure the runtime environment for the unit test or job, respectively.
The unit tests that developers write to run in their workspace, or those created for build automation tools, do not generally have this luxury and must set up the runtime environment directly.
The ClusterMapReduceTestCase starts a virtual Hadoop cluster on which the tests are run.
If the cluster does not start successfully, the test case will fail, without exercising the classes being tested.
A startup failure is commonly due to a configuration issue, either with the runtime classpath or server or cluster configuration file.
In particular, the NameNode and the JobTracker use Jetty to provide web servers for their web UIs.
The error messages relating to the Jetty web server start failures do not provide sufficient information for the novice to resolve the configuration problem.
For developers running the tests from their IDE, it is not uncommon to load the Hadoop source code into the workspace, in place of using hadoop-<rel>-core.jar.
Most of the Hadoop classes require configuration information that is provided in the distribution’s config/hadoop-default.xml file.
A copy of hadoop-default.xml is also bundled into the hadoop-<rel>-core.jar.
When this configuration information is absent, the virtual cluster behavior is unpredictable.
The following are the requirements for using ClusterMapReduceTestCase in a unit test:
Include this JAR in the build path of your project and in the runtime classpath for the JUnit execution environment.
Include this JAR in the build path of your project and in the runtime classpath for the JUnit execution environment.
Include the needed JARs in the build path of your project and in the runtime classpath for the JUnit execution environment.
It is often simpler to just include all of the JARs in the lib directory.
This must be defined at test case start time, or by using the call System.setProperty("hadoop.log.dir", "path"), before the first call to during cluster setup.
It may be specified by the following argument on the JVM command line:
Alternatively, you can call the following before the first call to ClusterMapReduceTestCase.
Let’s look at some ways to check whether the requirements for using ClusterMapReduceTestCase have been met.
For unit testing, the entire classpath, including the Hadoop JARs, were folded together.
All of a sudden, the unit tests started failing with an exception thrown by Jetty.
The Saxon JAR was in the classpath before the Jetty JAR, so it was being used to deliver the XML parsers, and the parser was not validating.
A couple of system properties control the XML parser that applications will get:
This is the Sun JDK default and works well with Jetty.
Validated XML is good, and these are parsed only at job start time.
These properties may be set by passing them as arguments to the JVM via the setup for the test.
Hadoop Core servers rely on Jetty to provide web services, which are then available for internal and external use.
If the Jetty JAR is missing from the classpath, the servers will not start.
The Hadoop Core Jetty configurations also require the JARs that are in the lib/jetty-ext directory of the installation.
If they are not present in the classpath of the unit test, nondescriptive failure-to-start error messages will be generated.
In these listings, only the relevant exception lines are shown; the stack traces have been removed to aid clarity.
In this case, various required configuration files may be missing, resulting in unexpected failures.
The MiniDFSCluster creates the directories for HDFS storage in the path build/test/data/ dfs/data, or build\test\data\dfs\data under Windows.
Listing 7-8 shows the error message that results if the directories cannot be deleted.
The typical reason for the failure is that a prior instance of MiniDFSCluster is still running.
By default, this virtual cluster starts six server processes: one NameNode, one JobTracker, two DataNodes, and two TaskTrackers.
If the NameNode or the JobTracker did not start, the tests cannot be run.
If the HDFS fails to start, the MapReduce portion is not started.
The NameNode is kind enough to actually report that it is up, as in this example log line:
The DataNodes’ state must be deduced from the log messages.
The two DataNodes are started after the NameNode is started, and each has a Starting line followed by a final line that indicates the BLOCKREPORT_INTERVAL.
If the lines containing BLOCKREPORT_INTERVAL are missing, the DataNode did not start.
As with the HDFS portion, the JobTracker informs you directly that it is running, but the TaskTracker status must be deduced from the logs.
Listing 7-10 shows the log lines that show the TaskTracker is running.
If the hadoop.log.dir system property is unset, a subprocess of the virtual cluster may crash and leave the test case in limbo.
The Eclipse framework provides a decent way to run individual or class-based Hadoop.
Occasionally, some state can get lost, particularly in the Windows environment, and the virtual cluster will fail to start.
The indication of this will be a series of messages stating that a connection attempt has failed.
Simpler Testing and Debugging with ClusterMapReduceDelegate When running tests, it is very helpful to be able to interact with the test cluster HDFS, to access the Web GUIs of the various servers, and to examine the log files.
This book’s downloadable code includes a class com.apress.hadoop.mapred.test.ClusterMapReduceDelegate that provides a wrapper around the Hadoop Core test framework class ClusterMapReduceTestCase.
This delegate class provides a JUnit 4-friendly way to build test classes, and exposes information useful to understanding what is happening in your test.
All the test case classes discussed here extend the class ClusterMapReduceDelegate.
Core Methods of ClusterMapreduceDelegate Table 7-1 lists the core methods that any unit test interacting with Hadoop will use.
In particular, all JobConf objects used by the test cases and classes being tested must be children of the.
Any test case that needs to create files in HDFS to use for the interactions.
It will then start the virtual cluster, with NameNode, and a JobTracker.
This method will throw an exception or possibly hang if the virtual cluster does not start.
NameNode and DFS configuration objects to log at level info.
Configuration  If you or your test need to Returns the virtual cluster’s HDFS.
Configuration  If you or your test need to inter- Returns the configuration object used.
JobConf object’s used by your  correctly configured for the virtual test cases.
Configuration parameters for Interacting with Virtual Clusters Several core parameters are needed for the tester and the test cases, when interacting with the virtual cluster.
Table 7-2 details the parameter names, how to get their values, and what to do with them.
When debugging test cases, it is very useful to know where the log files are being written, and the web addresses for the NameNode and JobTracker.
For accessing files in the virtual cluster’s HDFS, the HDFS file system URL must be available.
Writing a Test Case: SimpleUnitTest The sample SimpleUnitTest test case simply starts a cluster, writes a single file to the cluster HDFS, and reads that file back, verifying the contents are correct.
The full code for this example is in the file SimpleUnitTest.java of package com.apress.hadoopbook.examples.ch7 in the downloadable code for this book.
Normally, the Logger would be from the class being tested to better enable control over the logging levels.
In this sample test case, there is no class being tested, so a logger is created.
This simple unit test exists to demonstrate the creation and teardown of a * virtual cluster and the writing of a test case that uses the created cluster.
The test also makes two JUnit assertNotNull checks, to verify that the cluster configuration information is available.
If there are any failures, an exception should be thrown.
The JUnit framework will catch the exception and mark the test set as failed.
It also will set a small those parameters are currently unset.
This method starts the cluster and performs simple validation of the working ➥ state of the cluster.
Under some failure cases usually related to incorrect CLASSPATH ➥ configuration, this method may never complete.
Verify that there is a JobConf object for the cluster.
It is essentially the finally clause for the test class.
This stops the cluster, and would perform any needed cleanup.
The test has the standard stylized framework you will see in all of the sample code.
Any code that allocates objects that hold system-level file descriptors is done in a try block.
The try block has a finally clause where the system-level file descriptors are closed.
This pattern, if rigorously applied, will greatly reduce job failures when the jobs are running at large scales.
A very simple unit test that uses the virtual cluster.
The test case writes a single file to HDFS and reads it back, verifying ➥ the file contents.
Create our test file and write our test string to it.
The writeUTF ➥ method writes some header information to the file.
With HDFS the file really doesn't exist until after it has been * closed.
Open it and read the data back * and verify the data.
Our traditional finally when descriptors were opened * to ensure they are closed.
The test grabs the FileSystem object out of the base class using the following call:
This ensures that the file operations will be on the virtual cluster’s HDFS.
As a double layer of paranoia, the following line verifies that the file system is in fact an.
The following line will create testFile, if there is not an existing file by that name.
The following line writes the string testData with a small header to testFile.
At least through Hadoop 0.19.1, files do not really become available until after they are.
The following line will trigger an exception if fs.exists(testFile) is not true.
This verifies that the file exists in the file system.
At this point in the test case, the file has been created and is known to exist.
The test case will now open the file and read the contents, verifying that the contents are exactly what was written.
To open the file, rather than to create it, the following line is used:
The first line in the next snippet reads back one UTF8 string, and the next line verifies that the expected data was read.
My favorite initial testing tool is the PiEstimator example in the hadoop-<rel>examples JAR, which is the class for which this unit test is built.
The PiEstimator class, as it stands, is not unit test-friendly, and very little information can be extracted.
The only thing that can be done to verify the result is to examine the estimated value of pi.
As is common in unit tests, this test case declares that it is in the same package as the class under test:
The PiEstimator test class started life as a copy of SimpleUnitTest,java.
Turn down the cluster logging to filter the noise out.
Turn up the logging on this class and the delegate.
The NameNode web GUI is not known to work correctly with Hadoop 0.19.0 under the virtual cluster.
The method under test couple of key pieces of information to enable the test runner to interact with the virtual cluster services.
The JobTracker URL will let you interact with the running or finished job, examine the task outputs, and look at the job counters.
The HDFS URL will let you interact directly with the file system to view the data files.
Listing 7-19 constructs a new JobConf object out of the test default configuration all test cases of that virtual cluster instance.
It is actually the JobConf object returned by the TaskTrackers to send all task output to the console of the process that submitted the job.
This is set using the configuration parameter jobclient.output.filter to ALL.
Tip It is always wise to ensure that the JAR that contains your MapReduce classes is part of the classpath for tasks, and the conf.setJarByClass(PiEstimator.class) call ensures that.
The configuration parameter jobclient.output.filter specifies what output, if any, from the tasks are printed on the console of the job submitter.
The default value is FAILED, and only failed tasks have their output printed.
Make all task output come to the console of the unit test.
Ensure that hadoop- -examples.jar is pushed into the DistributedCache * and made available to the TaskTrackers.
The PiEstimator instance needs to be created and configured, as shown in Listing 7-20
Create the PiEstimator object and initialize it with our conf object.
This completes the walk-through of a unit test that invokes a MapReduce job.
Running the debugger on MapReduce Jobs There are several basic strategies for running a MapReduce job under the debugger.
Here, we’ll start with simplest and move to the more complex methods.
When running MapReduce jobs under a debugger, it is important to drastically increase the value of the configuration key mapred.task.timeout.
When you are single-stepping through a map or reduce task, it is common for more than 10 minutes to pass.
If the value has not been lengthened, the task you are debugging will be killed.
You can set the task timeout length to a large long value via the bin/hadoop jar command line, using -Dmapred.task.timeout, after the main class specification, if the job uses the GenericOptionsParser that the ToolRunner class provides.
Running an Entire MapReduce Job in a Single JVM The normal process of job submission involves the JobClient class storing all of the relevant information about the job in HDFS, and then making an RPC call to the JobTracker to submit the job for execution.
The JobClient determines the address of the JobTracker through the configuration value of the configuration key mapred.job.tracker.
This configuration key may have the value of local, in which case the entire MapReduce job will be run in the JVM that is submitting the job.
The cause of most of the restrictions is that the map and reduce tasks do not run in their own JVMs.
There is no way to change the JVM working directory, classpath, or other command-line configured options.
A significant result of this is that the DistributedCache behavior is very different.
If your job relies on the DistributedCache, this method will not be a good debugging choice.
The number of reduce tasks is limited to zero or one.
If your job requires more than one reduce task, this method will not be a good debugging choice.
The example in this section uses our old friend PiEstimator, the example that comes with Hadoop Core, as the MapReduce job.
The classpath must include the Core JAR, the example JAR (since the test case comes from this JAR), and all of the JARs from lib and lib/jetty-ext, as shown in Figure 7-1
You must explicitly specify all of the JARs required for your job here.
In this example, Eclipse is not configured to load the native compression codec libraries.
You also need to set up the following arguments, as shown in Figure 7-4:
Once the directory /tmp/pidebug has been made, the Run button in the bottom-right corner of the Run Configurations window will be active.
Click this button just to verify that the job will work in this setting.
And is in the process of commiting mapred.LocalJobRunner: Generated 1 samples.
And is in the process of commiting mapred.LocalJobRunner: Generated 1 samples.
The final line, with the estimated value of pi, indicates that the environment is correctly configured.
For Figure 7-5, a breakpoint was set in the map task, and the job launched via the Debug As Java Application menu item.
The job is configured to use the local file system for all working storage, and you may examine the files using the normal file system tools.
In this case, with the PiEstimator test stopped in the first line of the first map task, you can see that the working directory in the local file system contains a number of interesting files.
The file names that end in .crc are checksum files written out by the framework to provide data integrity checking.
It is prepared by the class JobClient as part of the job submission process.
The PiEstimator class writes out SequenceFiles for its input, and these binary files may be examined with the command-line Hadoop tool via bin/hadoop dfs -fs file:/// -text /tmp/pidebug/ test-mini-mr/in/part1
Let’s examine the contents of part0 of the input to demonstrate this.
Debugging a Task Running on a Cluster Hadoop Core, as of version 0.19.0, does not provide any tools to specify which tasks of a job to enable Java debugging services on, nor does Hadoop Core provide a way to indicate on which host and port such a task might be listening for remote debugging connections.
To debug tasks running on a cluster, the JVM parameters for the task have remote debugging enabled via the command-line arguments.
The core issue is to arrange for the JVM of the server or task that is to be debugged to have the additional command-line arguments that enable the Java Platform Debugger Architecture (JPDA) servers.
Table 7-3 describes the parameters to the JPDA debugging agent, agentlib:jdwp, that must be enabled in the task JVM to allow connections from debuggers.
The program receives two arguments: the transport, dt_socket, and the allocated port.
This program may be used to provide notification that the task has engaged the debugger agent and the port it is listening on for a debugger connection.
At this point, any program defined by launch would be executed.
Program execution will be stopped, while the agent waits for a debugger connection.
This section will consider only using the debugger configured for TCP/socket-based transport, à la remote debugging, in Eclipse.
The configuration parameter that needs to be set is mapred.child.java.opts, which may be set at the job level.
The JPDA invocation arguments need to be added to the value for this parameter.
The parameters that are most relevant to the user are suspend and address.
The for connections on the wildcard address of the local machine.
The port that is allocated will be printed on the standard output.
This precluded specifying a fixed port as part of the address parameter.
If only a single task will run at a time, a port may be specified.
It is best to have only the option suspend set to y for all of the task JVMs, isolate this option to a single machine in the cluster, to avoid requiring extensive interaction with each task, at minimum, connecting to the task in the debugger to resume execution.
The JPDA arguments will be passed via the command line to the child JVM, and JVM reuse will be explicitly disabled to avoid complications.
Figure 7-6 shows the Eclipse setup for a remote debugging session.
The Source tab has been configured identically to the earlier source configuration shown in Figure 7-3, with the hadoop/src directory and its subfolders.
Then you click the Apply and Debug buttons, to save this session and connect to the task.
Figure 7-8 shows the PiEstimator task in the debugger, stopped at a breakpoint in the.
This is configured per task and needs to be determined from task log files.
To determine the port to connect to, if no launch program has been provided, requires finding the per-task stdout log file.
At present, this step is manual and requires issuing shell commands on one of the cluster machines.
This is available via the JobTracker web interface or via the command-line tool:
These values are what you would put into the Host and Port fields of the Remote Debugging configuration dialog box.
The slaves.sh command will execute its command-line arguments as a shell command on each machine in the conf/slaves file, the output of these commands will have the generating host’s IP address prefixed to the output lines.
Let’s see an example of determining the ports and hosts of the suspended tasks:
Rerunning a Failed Task The IsolationRunner provides a way of rerunning a task out of a failed job.
Normally, the framework immediately removes all local task specific data when a task finishes.
Configuring the Job or Cluster to Save the task Local Working Directory Two configuration keys provide some control over the ability to rerun a task.
The value of the configuration key keep.task.files.pattern is a Java regular expression, which is matched against task files.
Any task that matches this pattern will not have its task files removed.
To save the results of all map tasks, a pattern *_m.* would work.
The other option is to set the value of the configuration key keep.failed.tasks.files to true.
Any task that fails will not be subject to cleanup.
Determining the Location of the task Local Working Directory The root directory set for the local working areas is stored in the configuration under the key mapred.local.dir, and may be a comma-separated list of directories.
Running the find command on this set of directories looking for files named job.xml will present a set of candidate tasks to be run via the IsolationRunner.
It demonstrates how to run it so that the map task local file space is left in intact.
Then you find the job.xml files that can be run via the IsolationRunner and run one of them in a way that will enable the use of the debugger.
Here’s how to put it all together for the IsolationRunner:
Once the child JVM is configured and waiting, Eclipse must be configured to connect to it.
Figure 7-9 shows the Eclipse Debug Configuration window for setting up a remote debugging connection.
The Host field must be filled in with the host on which the JVM is running, and the Port field filled in with the value from the Listening for transport dt_socket at address: XXXXX output line from the JVM.
Figure 7-10 shows Eclipse connected to the running JVM of the PiExample test case.
Eclipse setup to connect to port 54990 on the specified host.
This chapter has provided you with the techniques needed to build unit tests for your MapReduce jobs.
This chapter also demonstrated three ways to run MapReduce applications under the Eclipse debugger.
This will enable you to understand problems that occur only at scale on your large clusters, as well as explore how things work directly on your local development machine.
This chapter discusses techniques for handling larger jobs with more complex requirements.
In particular, the section on map-side joins covers the case in which the input data is already sorted, and the section on chaining discusses ways of adding additional mapper classes to a job without passing all the job data through the network multiple times.
The traditional MapReduce job involves providing a pair of Java classes to handle the map and reduce tasks: reading a set of textual input files using KeyValueTextInputFormat or SequenceFileInputFormat, and writing the sorted results set out using TextOutputFormat or SequenceFileOutputFormat.
The framework will schedule the map tasks if possible so that each map task’s input is local, and provides several ways of reducing the volume of output that must pass over the network to the reduce tasks.
This is a good pattern for many (although not all) applications.
There are other options available to Hadoop Core users, either by changing the pattern of the job or by providing the ability to use other languages, such as C++ or Perl and Python, for mapping and reducing.
Streaming: Running Custom MapReduce Jobs from the Command Line The streaming API allows users to configure and submit complete MapReduce jobs using the command line.
As an added bonus, streaming provides the ability to use external programs as any of the job’s mapper, combiner, or reducer.
The job is a traditional MapReduce job, with the framework handling input splitting, scheduling map tasks, scheduling input split pairs to run, shuffling and sorting map outputs, scheduling reduce tasks to run, and then writing the reduce output to the Hadoop Distributed File System (HDFS)
In the following example, we will demonstrate how to run a simple streaming job to sort all the input records of a dataset using MapReduce.
The argument informs the bin/hadoop script that the streaming JAR is to be used.
The use of this input format requires the output key format be set to Text via -D mapred.output.
Hadoop streaming provides the user with the ability to use arbitrary programs for a job’s map and reduce methods.
The framework handles a streaming job like any other MapReduce job.
The job might specify that an executable be used as the map processor and for the reduce processor.
Each task will start an instance of the applicable executable and write an applicable representation of the input key/value pairs to the executable.
The standard output of the executable is parsed as textual key/value pairs.
The executable being run for the reduce task will given an input line for each value in the reduce value iterator, composed of the key and that value.
The following example uses /bin/cat as the mapper and a Perl program to produce line counts of distinct lines from the input set.
The argument -file /tmp/wordCount.pl causes the file /tmp/wordCount.pl to be copied into HDFS and then made available to the map and reduce tasks in their current working directory.
The argument -reducer "/usr/bin/perl -w wordCount.pl" causes the Perl program wordCount.pl to be used to perform the reduce.
I had a large dataset composed of many input files in one compression format, and the data needed to be compressed in a different format.
The author ran a streaming job, with the map executable set to /bin/cat and the minimum input split size set to Long.MAX_VALUE, and enabled map output compression of the required type.
In a few minutes the cluster had uncompressed and recompressed the data files.
The following streaming example takes input from the directory words, uses /bin/ cat as the map executable, has no reduce phase, and compresses the job output using the GzipCodec.
The IdentityMapper could have been used just as easily, but the use of /bin/cat is just plain fun.
Note The streaming API explicitly forces the output key/value classes to be text.
If Java classes are used for the mapper, combiner, or reducer, the InputFormat used must produce text key/value pairs, or the mapred.map.output.key.class and mapred.map.output.value.class configuration key/values must be explicitly set to the class names of the key/value classes via -D mapred.map.output.key.
JyThoN: a way of iNTeRaCTiNg wiTh Java CLaSSeS iN PyThoN.
There are additional language constructs that allow the addition of arbitrary Java classes into the namespace of the Jython applications.
There are also additional primitive operators for interacting with native Java types, and a transparent translation between the Java String class and the Python string class.
The Hadoop Core distribution provides a Jython example MapReduce application in src/examples/ python/WordCount.py.
People have good results having Python applications used by Hadoop streaming.
Streaming Command-Line Arguments The streaming command-line interface provides a rich set of command-line arguments for controlling the execution of your streaming job.
It may be given multiple times to provide multiple input paths.
The directory must not exist prior to job start, and will be created by the framework for the job.
The executable will be stored in the current working directory of the task.
The class name of the handler that will JobConf.setInputForm hadoop.
TextInput There is special handling for the fully Format  qualified class names of TextInputFormat,
The default TextInputFormat is most efficient because the individual input lines are not split into key/value pairs.
This is currently used only in the framework for the org.apache.hadoop.
How key/value pairs are split and joined in a streaming job.
Using -inputreader org.apache.hadoop.streaming.StreamXmlrecordreader The -inputreader command-line flag is an unusual input format handler that provides two core features.
The keys that are emitted by the StreamXMLRecordReader class contain only the text that is between a beginning and an ending marker, inclusive.
Any text in the file that is not between a beginning and an ending marker is ignored.
When run on an Ant build file, the following example produces a sorted list of the target target> values.
This text will include any line separator sequences that are present in the original file in the block.
It is best to use StreamXmlRecordReader with a Java-based mapper because the key/value pairs may contain line separators.
There is some ability to control how much read ahead is done when looking for a match end.
The framework will look ahead only in lookahead bytes, which by default is equal to twice maxrec bytes, or 50,000
The value is interpreted as a regular expression if slowmatch is true.
The value is interpreted as a regular expression if slowmatch is true.
The record reader will look forward only the maximum of maxrec or lookahead bytes for the end of a CDATA block.
The record reader will look forward only the maximum of maxrec or lookahead bytes for the end of a CDATA block.
For more information, look at the excellent tutorial on using streaming at the Hadoop Core web site: http://hadoop.apache.org/core/docs/current/streaming.html.
Using Pipes Hadoop Core provides a set of APIs for use by other languages that allow a reasonably rich interaction with the Hadoop framework.
The C++ interface lends itself to usage by Simplified Wrapper and Interface Generator (SWIG) to generate other language interfaces.
The usage of the pipes APIs are outside of the scope of this book (refer to the wordcountsimple.cc example in your distribution and the tutorial in the Hadoop wiki: http://wiki.
Using Counters in Streaming and Pipes Jobs The framework monitors the standard error stream of the mapper and reducer processes.
Any line read from the standard error stream that starts with the string reporter: is considered by the framework as an interaction command.
As of Hadoop 0.19.0, there are two commands honored: counter: and status:
The default value is reporter:, but the value of stream.stderr.reporter.prefix will be used if set.
A counter record should be emitted for each input record, for each output record, for each record that is invalid, and for each crash or exception when possible.
The more detail about the job provided by the counters, the more understandable the job behavior is.
A line output to the standard error stream, of the form reporter:counter:UserCounters,InputLines,1, will increment a counter InputLines in the counter group UserCounters.
Note The value specified for the stream.stderr.reporter.prefix configuration key is the entire prefix string, the framework will use that exact string as the prefix, and the text that comes afterward must be counter:group,counter,increment.
The colon character is not added by the framework as a separator.
Using the reporter:counter:group,counter,increment Command A line of the form reporter:counter:group,counter,increment is converted by the framework into a call on the Reporter object of the following:
The parameter increment must be a whole number between LONG.MIN_VALUE and Long.
The group and counter parameters must not have the comma character in them.
Using the reporter:status:message Command A line of the form reporter:status:message is converted by the framework into a call on the Reporter object of the following:
The library provides application writers with a set of methods for interacting with HDFS.
The methods in turn use JNI to actually interact with an embedded Java Virtual Machine (JVM) which actually interacts with HDFS.
A Linux i386 version is provided in the distribution in the directory libhdfs.
If you need to build a custom version of the library.
The command to cause libhdfs to be compiled is the following:
If the libhdfs property is not set, Ant will not compile libhdfs.
Tip Any application using libhdfs must have a set CLASSPATH environment variable that includes the hadoop-<rel>-core.jar file and the JARs in the lib directory of the Hadoop distribution, and a shared library loading path that includes the libjvm.so shared library from the Java Development Kit (JDK)
If they are not preset, the embedded JVM will either fail to launch or the class loader of the embedded JVM will not be able to load the Hadoop classes required to provide HDFS file service.
It allows arbitrary programs to access data that is stored in HDFS.
The SourceForge project FUSE, http://fuse.sourceforge.net/, provides a set of APIs that allow programs written to those APIs to be mounted as host-level file systems.
There is no prebuilt version of fuse-dfs bundled into the distribution.
Note The fuse-dfs compilation environment will compile only for the i386 OS architecture.
The fuse-dfs package requires a modern Linux kernel with the FUSE module, fuse.ko, loaded.
To actually mount an HDFS file system, the environment variables listed in Table 8-4 must be set correctly.
The  set up the runtime environment for OS compilation architecture of  the fuse_dfs program.
In Hadoop 0.19.0, the FUSE mounts produced corrupted directory listings.
The fuse_dfs_wrapper.sh script makes some assumptions that are not generally applicable and may not work for most installations without modification.
The core configuration requires that the LD_LIBRARY_PATH environment variable include the directories that libjvm.so and libhdfs.
Cut from bin/hadoop, to ensure classpath is the same as running installation.
The following command will mount a read-only HDFS file system with debugging on.
The fs.default.name for the file system being mounted is hdfs://cloud9:8020
The mount point for the file system is /mnt/hdfs, and the arguments after the /mnt/hdfs are passed to the FUSE subsystem.
These are reasonable arguments for mounting an HDFS file system:
It is possible to set up a Linux system so that an HDFS is mounted at system start time by updating the system /etc/fstab file with a mount request for an HDFS file system.
A candidate line for use in /etc/fstab is to mount an HDFS file system at system initialization time.
The mount script, for the /etc/fstab entry in Listing 8-5, would be passed four arguments.
And the script could be placed in /bin (see the script bin_fuse_dfs in the examples)
The sorting stage requires data to be transferred across the network and also requires the computational expense of sorting.
In addition, the input data is read from and the output data is written to HDFS.
The overhead involved in passing data between HDFS and the map phase, and the overhead involved in moving the data during the sort stage, and the writing of data to HDFS at the end of the job result in application design patterns that.
Many processes require multiple steps, some of which require a reduce phase, leaving at least one input to the next job step already sorted.
Having to re-sort this data may use significant cluster resources.
The following section goes into detail about a variety of techniques that are helpful for special situations.
Chaining: Efficiently Connecting Multiple Map and/or Reduce Steps New in Hadoop 0.19.0 is the ability to connect several map tasks together in a chain.
Prior to the chaining feature, the user was forced to either construct large map methods or run multiple jobs as a pipeline, with all the additional I/O overhead.
Figure 8-2 provides a graphical depiction of the flow of key/value pairs through a job that uses chaining.
The chaining feature constructs a pipeline, internal to the task, which feeds each key/value pair from each output.collect to the map method of the next mapper in the chain.
The map task may be a chain, and the reduce task may have a chain as a post processor.
This allows for the construction of simple mapper classes that do one thing well, as well as the ability to rapidly modify a chain to support additional or different features.
Note At least through Hadoop 0.19.0, it is not possible to run the chain mapper through the streaming APIs.
Configuring for Chains There are two possible chains that can be established for a job: the map task can be a chain or the reduce task can have a chain.
The framework serializes the key/value into the output format for the particular task, and the.
During job configuration, when a mapper is being added to a chain, the style of key/value passage is specified, either by value or by reference.
Passing by reference eliminates a serialization and deserialization for the key/value, a.
Note If pass by reference is enabled, some level of verification needs to be in place to ensure that no use of the key/value object is made after a call to output.collect or that no mapper in the chain that receives the key or value reference modifies the contents.
Any compliance failures in this implicit contract will cause difficulties in diagnosing problems.
This is especially difficult because the configuration for pass by reference by the fact that the mapper class might be unaware about being part of a chain.
At least as of Hadoop 0.19.0, the chaining code does not explicitly check the runtime types of the key/value pairs being passed between elements in the chain.
The types are checked only during the job configuration phase.
It is recommended that these custom configurations be light configurations, which have only the special parameters for that item.
For a map task, the chain will have only mapper items.
For a reduce task, the chain will have a leading reducer item and then some number of mapper items.
At task runtime, a JobConf object is made for each item.
This JobConf object is constructed by making a copy of the localized task JobConf object and then copying each key/value pair out of the per map configuration into the copy.
This modified copy is then passed to the configure method of the item.
Configuring Mapper tasks to be a Chain A mapper task is either a normal map task or a chain.
The configuration of one excludes the been configured will disable the chain.
The framework provides a class, org.apache.hadoop.mapred.lib.ChainMapper, which appends the specified mapper class to the end of the current chain mapper task chain.
Class<? extends Mapper klass false The mapper class to be run.
JobConf mapperConf true  The configuration object that provides custom configuration data for this mapper instance at mapper runtime.
The input and output classes will be stored in this object.
Any keys present will override the corresponding values in the task’s localized JobConf object.
Any custom parameters may then be set on the mapperConf object before object will be initialized.
This * precedence is in effect when the task is running.
Configuring the reducer tasks to Be Chains Configuring the reducer phase is very similar to the configuration of the mapper phase with one additional requirement: the job configuration step must make a call to ChainReducer.
Class<? extends  klass	 false The reducer class to be run.
JobConf reducerConf true  The configuration object that provides custom configuration data for this reducer instance at reducer runtime.
The input and output classes will be stored in this object.
Any keys present will override the corresponding values in the task’s localized JobConf object.
Class<? extends  klass false The mapper class to be run.
JobConf mapperConf true  The configuration object that provides custom configuration data for this mapper instance at mapper runtime.
The input and output classes will be stored in this object.
Any keys present will override the corresponding values in the task’s localized JobConf object.
The provided code in ChainMappingExample and ChainMappingExampleMapperReducer provides a simple example of chain mapping that is structured to help you understand the order of events in your chain.
The maps and the reduce have a particular id to help distinguish them.
The actual ordering information has to be extracted from the job log.
Table 8-10 demonstrates running the ChainMappingExample and details the exact sequence of the method invocation on the mapper and reducer classes.
The assumptions are that the hadoopprobook and commons-lang JARs are in the current working directory.
The construct 2>&1 forces the standard error output to go to the same descriptor as the standard output.
Map-side Join: Sequentially Reading Data from Multiple Sorted Inputs In a traditional MapReduce job, the framework sorts all data for a reduce task before presenting the keys sequentially to the reduce task.
If the input data is already sorted, traditional MapReduce requires that the full map shuffle and sort process take place before the reduce task receives the sorted keys.
Map-side joins provide a way for a map task to receive keys in sequential order and to receive all the values associated with each key (very similar to a reduce task)
The map task reads the data directly from HDFS and no reduce is needed, which greatly reduces cluster loading.
The map-side join provides a framework for performing operations on multiple sorted datasets.
Although the individual map tasks in a join lose much of the advantage of data locality, the overall job gains due to the potential for the elimination of the reduce phase and/or the great reduction in the amount of data required for the reduce.
Caution There are several constraints on when map-side joins may be used, and the cluster loses capability to manage data locality for the map tasks (see Table 8-11)
The author has used map-side joins extensively in large-scale web crawls to eliminate recently crawled URLs from the set of freshly harvested URLs being prepared for fetching.
As of Hadoop 0.19.0, the join package supports full inner and outer joins.
All joins are full table scans at present; one optimization currently missing from the join package is the capability to use the indexes supplied with org.apache.hadoop.io.MapFile to skip over unneeded records in datasets.
In the following section, the term dataset is used to refer to one join item in the set of elements being joined.
The dataset can be an actual dataset or the result of a join.
Note As of at least Hadoop 0.19.0, the joins handle only keys that implement WritableComparable and values that implement Writable.
The join framework has not been updated to handle arbitrary key/value classes.
A dataset is the set of input splits that an InputFormat will produce when given the name as an argument.
The goal is to force the InputFormat not to split individual data files, thereby ensuring that each returned split contains the entirety of a single reduce task output, or partition.
The directory and the partition files it contains is a dataset.
Using the join package imposes the following limitations on your application.
All datasets must be  The sort ordering of the data in each dataset must be identical for sorted using the same  datasets to be joined.
All datasets must be par- A given key has to be in the same partition in each dataset so that all titioned using the same  partitions that can hold a key are joined together.
The number of parti- A given key has to be in the same partition in each dataset so that all tions in the datasets  partitions that can hold a key are joined together.
The InputFormat must  The OutputPartitioner class returns a partition number for each return the input splits in  key, which determines the reduce task each key is assigned to.
The file name is the string part-, followed by a 0 padded five-digit number, which is the reduce output partition.
At split time, no information is readily available to determine what partition number the split was originally a part of, so the ordinal number in the InputSplit array, returned by the returned by an InputFormat.getSplits call, if that key could be present in another dataset, it would be present only in the Nth split returned by that dataset’s InputFormat.getSplits call.
Note The map-side join has no simple way to discover what reduce partition a split was created as.
The InputFormat’s split routine is called with the minimum split size set to Long.MAX_VALUE, under the assumption that this will cause each split returned to be one complete input partition.
The map-side join splits, or partitions in the same partition order (i.e., any given single index slice through arrays of splits will return a set of splits in which all the keys in each set belong to the same partition)
If this assumption of equivalent ordering is incorrect, the behavior of the map-side join will be incorrect, and this failure will be detectable only by examining the output data.
For each dataset specified in the join, the input splits of the dataset are collected.
If the number of input splits returned by each dataset’s InputFormat is not identical, the framework throws an exception of the form IOException("Inconsistent split cardinality from child N, Y/Z") where N is the ordinal number of the dataset, per the input specification; Y is the expected number of splits or partition; and Z is the number of splits provided by the Nth dataset’s InputFormat.
For each single index slice of the InputSplit arrays, a WrappedRecordReader is constructed.
ComposableRecordReader and provides the standard RecordReader function of next(K key, V value)
The set of ComposableRecordReaders that are to be used for a particular join are bundled into a JoinRecordReader, which also implements the interface ComposableRecordReader.
The basic JoinRecordReader.next(key, value) method returns the keys of the entire set of keys present in the WrappedRecordReaders in OutputComparator order.
The value is a TupleWritable object, which contains each value associated with the key across the set of WrappedRecordReaders, and information about which WrappedRecordReader the value originated in.
A JoinRecordReader can have any ComposableRecordReader implementer as one of its inputs; by default, they are WrappedRecordReaders and JoinRecordReaders.
Each map task is given a JoinRecordReader from the outermost join as the task input record reader and receives the key/value sets of the join one by one in the map method.
In a simple case, this JoinRecordReader will have N WrappedRecordReaders from slice N of the original InputSplit arrays.
The default outer join behavior will receive each key in the input split set, in the sort order with all the values for that key.
The map method behaves very much like a traditional reduce.
Joins can be made on direct input datasets or on the results of joining input datasets; arbitrary deep nesting of this joining structure is supported.
The map method will be called with a key/value set only if every dataset in the join contains the key.
The TupleWritable value will contain a value for every dataset in the join.
The map method will be called for every key in the set of datasets being joined.
The TupleWritable value will contain values for only those datasets that have a value for this key.
The override join is unusual in that the there will only ever be one value passed to the map method.
In the inner and other joins there will be a set of values passed to the map method.
The override join maps a call to the map method with each key in the input split set and with that single value from the rightmost input split or join that has a value for the key.
The use of this join style requires that you order your input datasets (from least to most.
The join framework provides a mechanism for defining additional operators.
The configuration key mapred.join.define.YOUR_OPERATOR must be set to the class name of a class that implements the ComposableRecordReader interface.
The string YOUR_OPERATOR in the key definition must be replaced with the name of the custom join operation.
YOUR_OPERATOR can then be passed as the op parameter to the compose methods that accept op, and used anywhere that the predefined operators, inner, outer, and override, are used.
Details of a Join Specification A join specification is an operator and a set of data sources.
The predefined operators are inner, outer, and override to correspond with the join types.
A data source is either a table statement or a join specification.
Table 8-12 provides examples of several join data source specifications.
A data source located in or at the path textSource KeyValueTextInputFormat,"textSource")  that contain records that are to be parsed by the.
A data source composed of the inner join of the data SequenceFileAsTextInputFormat, in sequence file format at or in sequence, and the "sequence"),tbl(org.apache.hadoop.
The key/value mapred.KeyValueTextInputFormat, classes read from sequence are converted into Text.
A composite data source composed of a nested mapred.SequenceFileAsTextInputFormat, inner join of sequence and textSource, joined with "sequence"),tbl(org.apache.hadoop.
For example, suppose that a join of two tables is made.
The map method will be called twice with key 1; once with a tuple a,c; and once with a tuple b,c.
Composing a Join Specification full join specification or build the input specification for a particular dataset in the join.
Two of the methods construct a full join specification and are used when all the datasets within the join have the same InputFormat.
These two methods differ only in accepting String or Path objects for the dataset locations.
The third is used to construct a table statement for a dataset that includes a specified InputFormat and requires the application developer to aggregate the results into a full join specification.
This method produces a table statement from an input format class object and a path to a dataset.
The fully qualified class name of inf will be used in the returned table statement.
It is commonly used when building a join statement from input datasets that have different input formats.
The resulting string can be stored in the configuration under the key mapred.join.expr or used as a nested join within another join statement.
This method is identical to the String variant except that Path objects instead of String objects provide the table paths.
Building and running a Join There are two critical pieces of engaging the join behavior: the input format must be set to CompositeInputFormat.class, and the key mapred.join.expr must have a value that is a valid join specification.
Optionally, the mapper, reducer, reduce count, and output key/value classes may be set.
The mapper key class will be the key class of the leftmost data source, and the key classes of all data sources should be identical.
The mapper value class will be TupleWritable for inner, outer, and user-defined join operators.
For the override join operator, the mapper value class will be the value class of the data sources.
In Listing 8-7, note that the quote characters surrounding the path names are escaped.
The map method for the inner and outer join has a value class of TupleWritable, and each call to the map method presents one join result row.
The TupleWritable class provides a number of ways to understand the shape of the join result row.
Writable get(int i) The ordinal number  Returns the value object that the dataset has of a dataset.
The application will need to make a copy the contents need to exist past the next call to.
Only the top-level datasets are counted, even if the dataset is the result of many nested joins.
This method is used to provide an index limit for loops through the values using has and get.
Note A dataset may provide a null value to a join result record if the dataset is composed only of keys.
The user has two choices here, there is an iterator * and a get(i) size option.
The down side of the iterator is you don't know what table * the value item comes from.
Note, get returns the same object initialized // to the data for the current get valuesOutputCount++;
The iterator must always return the same number of➥ values as a loop monitoring has(i)";
The Hadoop Core framework provides a package for performing data aggregation jobs.
This package may conceptually be thought of as Hadoop streaming for statistics.
The analogy is incomplete because some code must be written to use the aggregation services.
The aggregation services are provided by classes that implement the interface org.apache.hadoop.
The framework provides a set of aggregator services (see Table 8-14 for descriptions of the predefined aggregator services)
The user can define the custom aggregator (see Listing 8-15)
The aggregation framework manages the mapper, combiner, and reducer; and the aggregation service produces the correct key/value pairs to pass forward.
The user is responsible for parsing the input record and invoking the aggregate service with the record key and count; the record and count are the traditional map task output key/value pairs.
Quite often, the key has no meaning for the job and is simply a label for the end user.
The count must the textual representation of an object that the aggregator service expects: a number for DoubleValueSum, a whole number for the LongValue series, an arbitrary string for the StringValue series, and a whole number for UniqueValueCount and ValueHistogram.
DoubleValueSum  Computes the sum of input  DoubleValueSum Label The number to accumulate  - DoubleValueSum: values.
The behavior is  LabelTAB37 expected to be doubles and    identical to LongValueSum.pl, are summed.
A single out-   so the LongValueSum.pl put record per reduce.
LabelTAB37 are expected to be longs, and the max value is output.
LongValueMin  Computes the minimum in- LongValueMin Label The number to challenge the - LongValueMin: put value.
The  LabelTAB3 are expected to be longs, and    behavior is essentially identhe min value is output.
A    tical to LongValueMax, so the single output record per    LongMax.pl example is used.
LongValueSum  Computes the long sum of in- LongValueSum Label The number to add to the LongSum.pl LongValueSum: put values.
LabelTAB37 expected to be longs, and the sum is output.
StringValueMin  Computes the lexically least  StringValueMin Label String to challenge the cur- - StringValueMin: input value.
UniqValueCount  Computes the set of unique  UniqValueCount Object as a Ignored; 1 is acceptable.
Any new objects encountered in a map or reduce task past this value are discarded.
ValueHistogram  Computes a histogram of  ValueHistogram The object The count of times the object LongHistogram.pl ValueHistogram: the occurrence counts of the  as a string.
The code that the user must supply can be supplied as a streaming mapper or via a Java class.
Aggregation Using Streaming The user-supplied code must take an input record and return an aggregator record.
Listing 8-9 provides a sample Perl mapper that computes the sums of input files that are sets of long values.
Each reduce task will have a single output value, the key will be the string SUM, and the value will be the sum of all of the long values routed to that reduce task.
The streaming command that was used is in Listing 8-10
An input file with white space separated whole numbers must be in /tmp/numbers, and the sums will be placed in /tmp/numbers_sum_output.
Note The reducer is defined as aggregate for the streaming job in Listing 8-10
If an error shown in Listing 8-11 happens, it generally means that an unrecognized aggregator id has been output by the mapper.
Aggregation Using Java Classes A Java application that wants to use the Aggregation services must provide a class that implements the class ValueAggregatorDescriptor.
The framework provides a base class ValueAggregatorBaseDescriptor that can be extended.
The command-line arguments accepted in args are listed in Table 8-15
Required None  The input directory or file to load input records from.
As with any MapReduce job, this directory must not exist prior to job start and will be created by the framework for the job.
Optional textinputformat  May be textinputformat or seq, indicating that the records in argument 0, input, are to be handled using TextInputFormat or SequenceFileInputFormat.
Optional None An XML file to load as configuration data.
Optional Empty String  The suffix to append to the job name, which is initialized to ValueAggregatorJob:
Specifying the ValueAggregatorDescriptor Class via Configuration Parameters The Hadoop test class TestAggregates provides an example of specifying the ValueAggregatorDescriptor class via the configuration instead of using ValueAggregatorJob.
The configuration data causes the class AggregatorTests to be used.
The text UserDefined tells the framework that this is a userdefined class.
The parameter aggregator.descriptor.num tells the framework how many definitions there are.
The value is the two-part text string, UserDefined, and the fully qualified class name, with a comma separating the parts.
Input records are passed this order to each of the defined classes.
The framework does not have an example for defining a custom value aggregation service.
Such a service would need to implement the ValueAggregator interface, and jobs using the custom service would have to provide an implementation of ValueAggregatorDescriptor.
Side Effect Files: Map and Reduce Tasks Can Write Additional Output Files The Hadoop Core framework assumes that individual map and reduce tasks can be killed with impunity, which allows the use of speculative execution and retrying of failed tasks.
The framework achieves this by placing the task output in a per-task temporary directory that is deleted if the task fails or is killed, or committed to the job output if the task succeeds.
Prior to Hadoop release 0.19.0, this per-task directory was available under the task configuration key mapred.output.dir.
As of Hadoop 0.19.0, this directory is a function of the OutputCommitter the job is using.
The default OutputCommitter is the FileOutputCommitter, which stores the task local output directory in the configuration key mapred.work.output.dir, and a getter is defined as FileOutputFormat.getWorkOutputDir(JobConf conf)
The FileOutputCommitter class will move all files and directories from a successful tasks work output directory to the job output directory.
Tip Side effect files should have job unique names; the method FileOutputFormat.getUniqueName (conf,name) produces unique names.
If fs is a FileSystem object for the job output directory, and conf ( FileOutputFormat.getWorkOutputDir(conf), FileOutputFormat.getUniqueName(conf, with a base name of side_effect_file and return an FsDataOutputStream object to the opened file.
Tasks that want to create additional output files directly can create them in the temporary output directory.
Tasks can create files in this directory, and the files will be part of the final job output when the tasks succeed.
The OutputCommitter actually commits the files to the actual job output directory.
This data, commonly called dirty data, is often not perfectly compliant with the data specification.
It might also be the case that some input records, while compliant, are unanticipated.
These data records can cause a map or a reduce task to hang, crash, or otherwise complete abnormally.
By default, the framework will retry the failed task, and the entire job will be terminated if the task does not complete after a number of attempts.
Operationally it is not desirable to have a long running job terminated if only a small number of records are causing problems.
New in Hadoop 0.19.0 is the ability to specify that a job can succeed even if a specified number of records cannot be processed.
The framework also allows the job to specify what percentage of the map tasks and what percentage of the reduce tasks must succeed for the job to be considered a success.
In some applications, there is a threshold for good enough that is less than 100%
In an application the author worked with, there was a piece of legacy code that would catastrophically crash every few thousand records.
Due to a variety of business reasons, it was decided not to attempt to fix the legacy application, but instead to just accept those failures.
In the real world of large-scale data processing, often the individual data records are not valuable, and the time value of the transformation result of the dataset is high.
In these situations it is acceptable to accept some failing records and or some failing tasks and then let the job complete.
Dealing with Task Failure The Hadoop framework provides four different mechanisms for dealing with task failure:
At	the	highest	level,	the	JobTracker	keeps	track	of	the	number	of	tasks	that	have	failed on a particular TaskTracker node on a per-job basis.
If this number crosses a threshold, mapred.max.tracker.failures, that TaskTracker is blacklisted from executing further tasks for the job.
The	next	level	is	the	standard	method	that	most	users	are	familiar	with:	to	retry a failed tasks a number of times, mapred.map.max.attempts, for map tasks and mapred.reduce.max.attempts for reduce tasks.
If any task fails more than the respective number of times, the job is terminated.
A task isn’t actually considered failed by the JobTracker until it has used up all of its retry attempts.
The	framework	also	allows	the	job	to	specify	what	percentage	of	the	tasks can fail before the job is terminated.
This is normally 0%, but the parameters mapred.max.map.failures.percent and mapred.max.reduce.failures.percent control the allowed failure percentage.
The	job	may	also	specify	that	bad	record	skipping	is	enabled,	as	described	in	the	next section.
Skipping Bad Records You can enable bad record skipping by setting mapred.skip.map.max.skip.records and/or mapred.skip.reduce.max.skip.groups to a positive nonzero value.
The actual value specified is the size of the record block that is acceptable to lose.
The smaller the number, the more work the framework might need to do to minimize the dropped records.
The configuration parameter mapred.skip.attempts.to.start.skipping determines how many times a task can fail before skip processing is enabled.
Skip processing requires that the framework keep track of what record is being processed by the task.
For streaming jobs and for jobs that consume multiple records to work on groups of records, the framework cannot track the records; the application developer has to assist the framework in this tracking.
For maps and reduces, respectively, there are two configuration parameters and two counters that the application must manage:
The respective parameter must be set to false in the job configuration.
A binary search is used to locate the failing record group within the task.
It appears that this search is exhaustive and will continue until the number of task failures is exceeded.
For small values of these configuration parameters, increasing the number of task retries is required.
Tip The number of retries is controlled by the configuration parameters mapred.map.max.attempts.
This feature provides somewhat dedicated resource pools, queuing priority, and pool-level access control.
In the public documentation, a  resource pool is referred to as a queue, so that term will be used in this document as well.
A queue has priority access to a specified percentage of the overall cluster task execution slots.
When a cluster has unused task execution slots, a job in a queue can use the idle slots, even though these slots are over the queue’s priority capacity.
If a job with priority access to these resources is started, the over-priority allocation task slots will be reclaimed as needed within a specified time interval by killing the tasks executing on them.
A queue may have an explicit list of users allowed to submit jobs to it.
The Capacity Scheduler may also have a list of users allowed to manage the queues.
Enabling the Capacity Scheduler To enable the Capacity Scheduler, the following parameter must be placed in the hadoop-site.
As of Hadoop 0.19.0, the Capacity Scheduler JAR is not part of the default runtime classpath.
Adding this JAR to the HADOOP_CLASSPATH by amending the conf/hadoop-env.sh script is sufficient.
Each queue that the cluster administrator defines must have a configuration block in the hadoop-site.xml file.
Listing 8-16 defines one queue, one-small-queue, with user jason and group wheel given submission and control permissions.
These values could be in hadoop-site.xml, but the suggested location is in capacity-scheduler.xml.
Figure 8-3 shows the JobTracker web interface for this queue set.
This is the set that may submit jobs to one-small-queue</description> users and the list of groups.
This is the set that may kill or change the priority of other users jobs.</description>
The sum of the percentage cluster capacity for all queues must not exceed 100, or the JobTracker will not start, and there will be an exception in the log file:
Summary The Hadoop framework provides a powerful set of tools to enable users to run more than standard MapReduce jobs.
This chapter covers a number (but by no means all) of the features.
Hadoop is under active development, and new features are being introduced on a regular basis.
The Hadoop streaming and aggregator features are powerful and provide the user command-line tools for performing data analysis on large datasets.
Chain mapping provides a way to maintain code simplicity and reduce overall data flow through the system by allowing multiple mapper classes to be applied to the data for a job.
Map-side joins provide databasestyle joins that can drastically speed up jobs that process bulk data that is already sorted.
There are also a number of features that have become their own Apache projects  (see Chapter 10)
On the Hadoop Core mailing list, a user was wondering about the way to handle a specific style of range query with MapReduce.
The application had a search space and incoming search requests.
In this chapter, we’ll look at a similar setup, as follows:
The	search	space	dataset	has	the	key	range begin, range end and the value search space data.
For simplicity’s sake, let’s assume that ranges in the search space do not overlap.
The	search	request	dataset	has	the	key	value and the value search request data.
The	result	set	for	a	value	that	is	between	range begin and range end has the key value and the value search request data, search space data.
How do you solve this problem with a traditional MapReduce application? That’s the focus of this chapter.
There are a couple of overall design goals, and the weights of the different factors will vary by installation and by job.
In today’s environment, there is an intense pressure to get processes up quickly and evolve them.
Given agile business practices and tight budgets, rapid evolution becomes the norm.
This practice means that there will be little design time, and the application will be modified, possibly by multiple teams, over a medium to long period of time.
Design Goals Our overall goal is to have a job that runs reliably and fast.
To achieve reliability, we aim for simple code, and implement monitoring to be informed when the algorithms being used are no longer suitable for the scale or patterns of data.
Given that this application is going to evolve rapidly, and eventually be modified, perhaps by different people, each piece of code needs to be simple and clear.
This is in direct opposition to the requirement that the map and reduce methods be treated as the deeply nested inner loops that they are and carefully optimized.
The data is expected to be real-world, dirty, and to change over time.
Wherever possible, the application must handle malformed records in a graceful manner and report on the malformed rate.
To achieve good performance, the job must minimize underuse of the hardware, by managing how the data is split, partitioned, and compressed and by tuning the number of tasks run per node.
To avoid having the network speed become the limiting factor, the transform.
The output of the job, shown in Table 9-1, will be a modified common log format with the IP address, the network range, and the network name, in place of the original IP address, for those search requests for which a network was found.
Note The last two octets of all IP addresses in the log files have been randomized.
Design 1: Brute-Force MapReduce The brute-force MapReduce pattern is generally the quickest to get going and the simplest to manage.
The downside is that these jobs quickly become bound by the network speed and the sorting speed for the cluster.
In a brute-force MapReduce, the only time you have ordered data is in the reduce step.
This forces all of the data to flow through to the reduce task.
There is also the additional complexity that you have multiple record types, which need to be distinguished at reduce time.
The overriding constraint here is ensuring that any given search request record finds all records that it is in range of in the search space.
A Single Reduce Task If a single reduce task is used, all search request records are guaranteed to be in the same partition as their respective search space records.
Table 9-2 defines the comparator behavior for the three cases the comparator will encounter.
The input plan for the reduce method is to receive individual records and to manage the join behavior by maintaining memory about previous records.
This adds complexity to the reduce method and increases the risk of out-of-memory conditions.
To enable the framework to do the aggregation would require having redundant data in the records; the end range would need to be in the value of the search space records.
This requirement is driven by the fact that the OutputCompartor object receives only the key.
A simplification that results from this decision is that, in the first pass, using Text is acceptable for the key and value, as the records may be distinguished lexically.
In a future step, as a performance optimization, we will implement a key class that provides a WritableComparator that handles our keys at the byte level rather than at the object level.
Using the byte-level comparator for a complex key opens the door to the key format and the comparator getting out of sync, introducing the possibility of errors.
Note Having Text objects for the key and value greatly simplifies the initial debugging of the jobs, as the data can be readily examined by eye.
Key Contents and Comparators For simplicity in this pass, we are going to use the same object, Text, for the keys for both datasets, and Text for the values.
To do this, a simple encoding must be defined that allows the origination dataset to be determined easily from the text of the key.
If there is a way to do this without needing to write a custom comparator, the job can be up and running very quickly.
For the stock comparator to work, the keys must lexically compare an order that the reduce method understands and can process with minimal complexity.
If all IP addresses are encoded as a zero-padded, fixedlength hexadecimal string, the primary lexical ordering issue is addressed.
This leaves a single issue: lexically, keys for the search requests will sort before a search space key that has a begin range value equal to the key of the search request.
In the best of all possible worlds, search request keys would appear in the sorted output, after the search space key that opens the range for the request.
The search space key may simply be the begin range and end range values, with a separator character.
There are many simple tools for splitting strings based on a separator character.
This has the advantage that if a lexically larger character is used as a suffix for the search request keys, the search request keys will sort after the search space key that defines the relevant range.
This can be quickly tested by running a small sample dataset through a streaming job to verify that the data compares the way we expect.
A test dataset will be prepared from an Apache log file, with the Perl command in Listing 9-1
The code in this section takes the first field of the access log, commonly an IP address, and converts it to an unsigned integer, which is then printed as an eight-character-wide hexadecimal number, with a semicolon (;), as a suffix.
A fake range is generated by printing that original value, without the semicolon, with a number ten higher, with a colon (:) separating them.
Notice that the output ordering is exactly the reverse of what our application needs.
Note Listing 9-1 is structured to run from within the Cygwin environment, in the examples directory, on a Windows installation.
Adjust the paths and file names as needed for your local installation.
In the command shown in Listing 9-1, a dataset was prepared with converted IP addresses from an Apache log file.
Listing 9-2 runs a streaming job to see how the records will actually be sorted by the default comparator.
Listing 9-2 is structured to run from the Hadoop installation directory.
And is in the process of commiting mapred.TaskRunner: Task 'attempt_local_0001_m_000000_0' done.
A Helper Class for Keys Key management is critical for this job, and to help avoid introducing errors later in the application life cycle, a helper class for keys will be provided.
The initial version needs to be able to validate, pack, and unpack keys to and from the Text objects.
The Hadoop framework creates a runtime environment for the tasks of the job.
In the TaskTracker’s local working area, the path set defined by the configuration key, mapred.local.dir, a directory tree is built for the job, which contains the unpacked DistributedCache items, a file job.xml that contains the job configuration, a shared directory for all tasks of the job, and a working directory for the task to be run.
An instance of the configuration date is created, and the per-task information modified by adding per-task parameters and adjusting the paths of configuration parameters that have been unpacked into the job or task working areas.
The bulk of this localization process is handled by TaskTracker.localizeJob.
The following parameters are added or modified for a task as of Hadoop 0.19.0:
A Java system property of the same name is also set.
All task attempts for this task will have the same value for this key.
The framework will make multiple attempts to complete a task.
This value for this key holds the ID of the current attempt instance.
For a map task, this is the ordinal number of the task.
For a reduce task, it is both the ordinal number of the reduce task and the result of Partitioner.getPartition( K,V, numPartitions), which will be identical for all key/value pairs passed to this reduce task.
In our example, four classes are associated with key handling:
These parameters have default values of semicolon (;) and colon (:), respectively.
They may be any pair of characters, as long as the range separator character sorts first.
In Listing 9-3, the key is converted to a String and examined to see if it is one of the two patterns that are accepted.
All IP addresses will be encoded as eight hexadecimal digits.
If the key is a search request, there will be one IP address and a trailing searchRequestSuffix character only, forcing the string to be only nine characters in length.
If the key is a search space item, there will be two IP addresses, with a rangeSeparator character between them only, forcing the string to be seventeen characters in length.
The IP addresses are converted into long values via Long.valueOf(address,16)
The String.substring method is used for extracting the actual IP address data from the raw string.
If a valid search request or search space definition is found, the helper object is marked address.
The setToRaw method, in Listing 9-4, is used to create and store a value in a key object that correctly encodes either a search request or a search space.
If the helper object is not valid, nothing is done, and no indication of this is made.
Changing this behavior requires rearchitecting the application to provide a visible trace of this error; logging it is not likely to be sufficient.
A StringBuilder and Formatter are ThreadLocal instance variables, making this class thread-safe.
This is done as a small efficiency and a protection against the day when the helper is used in a multithreaded map task.
Note It is reasonable to assume that anything written to the log by a task will never have been seen by a human being unless something is visibly wrong with the job.
The Mapper With the plan for the comparator handled, it is time to design the mapper.
For	the	search	requests,	the	mapper	must	accept	Apache	log	files	and	extract	a	key from the line in the key format, passing the rest of the line as the value.
The	search	space	items	will	be	stored	as	straight	text,	with	a	tab	(\t) separating the range from the data.
The mapper may distinguish between the two records either from the input file name or by the length of the key.
As a demonstration of using chain mapping, our mapper is going to run a chain to process the incoming values.
The first element in the chain will take action only if the incoming record does not look like a search request or search space key, but instead looks like an Apache log file record.
This mapper will transform the record into a search request.
The next map in the chain will perform validity checking on the keys.
Note In the next version, the example will use org.apache.hadoop.mapred.lib.MultipleInputs, and have the search space dataset be in a SequenceFile.
For simplicity of debugging, this version uses text records only.
This demonstrates our standard practice of having a counter, named TOTAL INPUT.
This provides a clear indication of how the job is going.
The helper object parses a string that is either a search request or a search space, returning true if the key was recognized.
In this preamble, if the helper can parse the key, it is just passed forward.
As a general rule, we log per-key data only at level debug, as the logging volume will be very large.
In Listing 9-6, the key was not recognized as a prepared key and is assumed to be an Apache log line.
If the input separator for the TextInputFormat happens to be a single space:
Note If the input format happens to not be KeyValueTextInputFormat, the configuration key changes in KeyValueTextInputFormat, or the default value changes, this code will fail silently.
In Listing 9-7, the default case of a raw log line is handled.
This code does make the assumption that the keyValueSeparator computed in Listing 9-6, is correct.
A complete line is assembled in sb, and then parsed.
The IP address is assumed to be the first text in the line and to be terminated by an ASCII space character.
This code accepts only IPv4 addresses in the format of four dot-separated octets.
Once the correct key and new value are produced, they are output.
The use of chain mapping actually reduces the efficiency of the task, but it is nice to have a demonstration.
For paranoia sake, reassemble the log line and split it ourselves * on the first space.
At this point, all keys are assumed to be valid, and this map verifies that.
Several counters are kept to help with sort and long-term monitoring of the job.
The Combiner The combiner is often one of the more complex pieces of a MapReduce job, and it’s usually given the least thought.
What is the correct behavior for encountering duplicate keys in the map output? For simple aggregation jobs, this is straightforward.
In our case, we have two different types of keys, and what to do for a duplicate in either case is unclear.
The first proposal would be to use a TextArrayWritable, and just keep all of values.
This doesn’t provide much of a space saving, compared to just not running a combiner.
A combiner should provide either a significant reduction in I/O volume or a significant reduction in resource use for the reduce phase.
If a custom comparator were written, a combiner might make sense.
In the type of MapReduce application we are working on here, a combiner that suppresses duplicate key/value pairs could be helpful.
In our constructed example, we know there are no exact duplicates.
The Reducer Each reducer task will need to receive a stream of key values, where the range statements will be first in the sorting order.
This forces the reducer class to maintain state information about which ranges have been seen, and the value of those ranges.
This prior range information is bounded, and ranges may be flushed when the end range value is less than the current input.
As an added bonus, the reduce task is also run as a chain, with a postprocessing map that converts the encoded key formats back into dot-separated octet format.
The actual reduce task is performed by ReducerForStandardComparator.java, shown in Listing 9-9
At this point, any invalid key is an indication that something has gone very wrong—data corruption at some level, given the level of verification performed on the keys in earlier steps.
We keep a queue of networks, ordered by the network endof-address range.
If the current key is a search request and the current key is larger than the end of a network’s address range, the network is removed from the active queue.
The call activeRanges.deactivate(searchRequest) clears any networks from the activeRanges queue that can no longer be matched.
If the current key is a search space key, it is added to the set of active ranges, via the following:
At this point, each network in activeRanges is a match.
A network’s end range is guaranteed to be larger than the search request key, and due to our comparator’s ordering of the keys, the network begin range must be less than or equal to our search request.
These are used to construct the actual output key and output value.
The key will be the original log record IP address, followed by the network begin and end addresses.
For ease of parsing, these will be separated by an ASCII tab character.
The value is simply the network name, ASCII tab, and the rest of the original log line.
The Driver The driver, shown in Listing 9-11, builds on our base class, utils/MainProgramShell.java, and defines only a small number of methods.
This example relies on there being only a single reduce task, as the default partitioner will cause this job to fail.
In our next design iteration, we will write a custom partitioner.
The values for input and output are set by the use of the command-line flags --input and --output, respectively.
The setup follows the general rule for using the chain, and allocates dummyConf to use as the private configuration object for the chained map and reduce tasks.
The framework serializes the contents in each call to the ChainMapper methods, making it safe to clear dummyConf and reuse it.
The map and reduce methods used do not modify the passed-in key or value objects; therefore, the chaining framework is being formed to pass keys and values by reference.
The second-to-last argument, false, in the ChainMapper.addMapper and ChainMapper.setReducer methods forces this behavior.
All of the mappers and reducers expect Text objects for the input key and value, and output Text.
In an updated version of chaining, in which the key and value objects implement WritableComparable and Writable, passing TextKeyHelperWithSeparators objects for the key would probably be significantly more efficient.
The Pluses and Minuses of the Brute-Force Design The biggest plus of this design is that it is simple and took about a day to put together.
The biggest disadvantages are that all of the data must pass through the mapper and be sorted, and that only a single reduce task may be used.
Given that the total number of networks is relatively bounded, if the incoming log records are batched in smaller sizes, this job will run reasonably well and reasonably fast.
Without a custom partitioner, this job cannot be made to run with multiple reduce tasks.
Design 2: Custom Partitioner for segmenting the Address space The biggest boost for the brute-force method would be to find a simple way to allow multiple reduce tasks.
The standard partitioner uses the hash value of the key, modulus the number of partitions as the partition number.
A simple strategy for this application might be to simply segment the IP address range.
There is no guarantee that the network ranges will fall cleanly on these segments.
There will need to be a mechanism to split search space keys into segmentappropriate boundaries during the job, while putting the full range in the output record.
Perhaps simply modifying the format for the search space records to allow for an original range to be part of the record will address this.
Note This partitioning method is still subject to uneven distributions of the key space resulting in a subset of reduce tasks running much longer.
To ameliorate this, the key space may be sampled and the partitioning table built using the sample data, in a manner similar to that done by the Hadoop terasort example.
The Simple IP Range Partitioner method, shown in Listing 9-12, simply takes the IP address of a search request key or the begin range address of a search space key and returns the partition for that record.
The original design supported a configurable table to ensure that the records were partitioned approximately evenly.
This required a tool to scan the records to generate a distribution map and code to load that map into the partitioner.
During the process of actually writing the code, the decision was made that if that feature is needed, it may be implemented later.
Instead, each partition gets an approximately even number or span of addresses out of the IPv4 space.
A TreeSet is used instead of simply maintaining an array of long values.
The array of long values would be faster and would greatly reduce object churn.
Find the bucket in ranges that is the lowest bucket * that is valued higher than begin.
The first step is to initialize the key helper and to determine if the key is actually a valid search space or search request key:
If the key is valid, the IP address of the search request record or the range begin address of the search space record is stored in begin.
Once begin is known, it may be looked up in the table, ranges, that maps addresses to reduce partitions.
The table is actually a TreeMap, and entry keys are the ending IP address of the partition.
This data structure allows the following line to provide the entry of the partition that the key/ value pair must go to:
The TreeMap method higherEntry returns the element in ranges where the entry key is closest to begin, while not being less than begin.
The value of that entry is the partition number for this key/value pair.
For debugging purposes, the entry is assigned to a local variable, partition.
The entry value could simply be returned at this point, but a little checking is done to verify that this key/ value pair is a search space record, where the end of the search space is also an address that will be in this partition.
No checking is made for the case where ranges.higherValue returns null, as it is assumed that the ranges table spans the full IPv4 address space range.
Now that we have a conf object we can initialize the * helper and build ranges, using the number of reduces.
The first step is to save a copy of the JobConf object into conf, our standard practice.
This class delegates to the TextKeyHelperWithSeparators class for any unrecognized input keys, and handles an extended form for search space keys that provides a way of splitting a search space key across multiple partitions and then assembling the resulting records later.
Each partition will span approximately rangeSpan addresses, defined as 4294967295L / numPartitions.
The application uses long values to avoid issues with sign extension, as Java does not provide an unsigned integer type.
The variable spanned contains the ending IPv4 address of the previous partition.
Each pass through the for loop adds rangeSpan to spanned defining the ending address of the next partition and increments the partition number:
These are currently added in order, which is not optimal for a TreeMap, as TreeMaps are stored as red-black trees and ordered insertion will result in an unbalanced tree.
Casting our gaze into the future, it seems unlikely that there may be more than small hundreds of reduce tasks and a rewrite might be planned to eliminate the use of TreeMap and simply use an array.
Here, I took a design expedience step that perhaps was not optimal given my later experience.
I decided to use the BruteForceMapReduceDriver (Listing 9-11), and allow more than one reduce task.
To achieve this, each search space record must be replicated so that any partition that could have matching requests each gets a copy of.
The concept is that an addition map will, for each incoming search space record, output a set of search space records such that each reduce partition that could receive a matching search request will receive one of the output search space records.
This addition map, RangePartitionTransformingMapper (shown later in Listing 9-16), will be added to the mapper chain.
If the passed in key is a regular search space key, * set the extended attributes for a spanning search space key.
The first portion of Listing 9-14 handles the setup and validation.
The calling convention requires that the caller pass in an initialized key helper (outsideHelper) and the value to output (OutputCollector)
The Reporter object (reporter) is used to log metrics check fails, an exception is thrown.
The key helper class for these spanned keys has two additional fields: the actual begin and end of the search space request.
The begin and end fields will now be fields for the address span of the partition for which the record is output.
As a quick recap, the search space key contains an IPv4 address range, represented as a beginning and ending address.
To enable multiple reduce tasks, the search space records must be available in each reduce task that could receive search requests that would match the search space record.
This allows the search space requests to be mixed into the job input with the search requests.
Each search space key is split into a set of search space keys, such.
Implicit is that each partition starts with the address after the prior partition and there is no overlap in address space between partitions.
The block of code in Listing 9-15 is the part of the spanSpaceKeys method that produces the per-partition keys.
The end range value is used as a convenience and should not be used in test.
The real end and real beginning are always the actual * begin and end of the search space request.
The real ranges are untouched, and the begin range is moved up * and the end range is just set in the loop.
If the newly adjusted begin range is past the end of our key's range, * there will be no more keys output.
We have to assume at this point * that it is not before the start of the partition.
This case indicates that the end of this partition span is past the end of * the real search space request.
The passed-in, parsed-input key is in outsideHelper, the working object is helper, and the actual begin and end addresses for the network are stored in the real begin (helper.
The helper, a PartitionedTextKeyHelperWithSeparators object, holds both the actual original search space key, using the realRangeBegin and realRangeEnd fields, and the begin and end address of the range within a partition, in the begin and end fields.
For each partition, the within that partition that this search space record will match, and the realRangeBegin and realRangeEnd fields will be untouched.
The variable spannedRanges is a subset of ranges that contains only partitions that have an end address larger or equal to the real begin range of the key, and equal to or less than the real end range of the key.
Put simply, spannedRanges contains the partitions that may contain addresses that would match the passed-in search space record.
The following loop examines each of the candidate partitions in ascending order of the partition end address:
The variable spanEnd contains the ending address for the current partition.
It is implicit in (the beginning address of the portion of the key that has not yet been output to a partition is.
When a per-partition key is to be output, the helper is set up with the correct end address for that partition.
The end address will either be the last address of the partition, spanEnd, or.
The begin field of helper is set to the address after the end of the.
The core loop is run once for each potential partition that this key may need to have a record placed.
The variable count keeps track of the number of records output, and span contains the information about the current partition, in particular the end address and the partition number.
There are a couple checks: one to see if the partition end addresses are.
The	remaining	portion	of	the	key	fits	entirely	in	the	current	partition,	span,
The	key	has	address	space	that	extends	past	the	end	of	span.
The end of the loop actually builds the Text object with the appropriate data, helper.
It initializes the key helper from the passed-in key, helper.getFromRaw(key), and for a valid search space key, calls the spanSpaceKeys method of SimpleIPPartitioner (search requests are just passed through as output)
The original concept was to take the search requests, feed them through the RangePartitioningTransformingMapper using RangePartitionTransformingMapper as a driver class, convert the search space records into a sorted and partitioned dataset, run another MapReduce job over the incoming search requests, and then perform a map-side join on the resulting datasets.
After working with the data for a short time, I realized that the search space was so small that it wasn’t worth the extra complexity or time to have an additional step for presorting the search space records.
I decided to simply add this mapper as part of the mapper chain, and read the search space records as input with the search request records.
The configuration changes to BruteForceMapReduceDriver are shown in the next section.
Helper Class for Keys Modifications The class PartitionedTextKeyHelperWithSeparators will be the new KeyHelper and will support carrying the original key data, so that the output records can be provided with the actual network range instead of that portion of the network range that fits in this partition.
A new record format needs to be designed that can carry the additional data.
The key format for the search space keys has been begin:end, where begin and end are the first and last addresses of the network, each an eight-digit hexadecimal number.
To allow partitioning, the search case keys must match keys in a particular partition.
My first idea on how to address this was to just have four values instead of two, with the same separator between each.
The full code for that version is in com.apress.hadoopbook.examples.ch9.PartitionedTextKeyHelperWithSeparators.java, available with the rest of the downloadable code for this book.
The code for the first design must be modified to examine a configuration parameter, range.key.helper, and instantiate the value as a class, defaulting to the TextKeyHelperWithSeparators class.
The existing mapper and reducer classes are modified to instantiate their KeyHelper class based on a configuration property, range.key.helper, defaulting to TextKeyHelperWithSeparators.
This leaves the old behavior intact, while allowing multiple reduce tasks.
In Listing 9-18, the configuration key range.key.helper is set to be our partitioning class by conf.setClass("range.key.helper", PartitionedTextKeyHelperWithSeparators.class, KeyHelper.class), and an additional map is placed in the chain, to span the search space keys:
If more that one reduce is to be run, the spanning partitioner must be used.
The reducer, ReducerForStandardComparator.java, does not need any changes, but the ActiveRanges class, which provides the hit method, does.
In Listing 9-19, we simplify it to make it aware of the PartitionedTextKeyHelperWithSeparators class, and in that case, to use the real begin and end ranges for a search space request, rather than the per-partition begin and end ranges.
If many types of keys are used, this method will quickly become excessively complex.
In this case, there is only one type of key, so we can defer that code cleanup to a future that may not come.
To provide a secondary sort of the final output, we have the classes DataJoinReduceOutput, DataJoinMergeMapper, and IPv4TextComparator.
This set of classes performs a map-side join on all of the reduce output partitions of BruteForceMapReduceDriver, producing a single sorted file as output.
The output uses the network begin, end, and name values as secondary sort keys.
These also provide an example of how to perform a merge-sort of any reduce task output efficiently using map-side joins.
Unlike a traditional map-side join, where each path item in the input is a table and the matching partXXXXX files of each input path are joined, each individual part-XXXXX file is taken as a table, and all of the part-XXXXX files are joined together.
This causes the map-side join to perform a streaming merge-sort on all of the input data files.
If there are exactly two parts and the first part is a class name that implements InputFormat, that input format is used for loading the directory name in parts[1]
If there is not exactly two parts, the original input is used with KeyValueTextInputFormat.
Basically, the input directory can be preceded by a class name and a colon, and the class will be used as the input format for loading files from that input directory.
This method examines inputPath, constructed from that passed-in path element.
In this case, the only items accepted have file names that match the regular expression ^part-[0-9]+$, our standard reduce output file format.
Rather than try to manage the map-side join table format, the following call builds the table format for the input file:
All of the individual table entries are aggregated in the ArrayList tables.
This by itself will merge-sort all of the input data into a single output file.
The new piece, the specialty sorting of the input records before the map method, is triggered by the following line:
The mapper, shown in Listing 9-22, provides a secondary sort by network for the matched requests.
These probably aren't needed * but are made only once.
For each table, check to see if it has a value for the key.
Each table is checked for a value (value.has(i)) and each table value (Writable the values are converted to Text objects when needed (outputText[valuesIndex]
Once any required sorting is completed, the records are output (output.collect( key, values[i] ))
It attempts to operate at the byte level and to minimize object allocation.
Compare to text objects that are IPv4 addresses in dotted octet notation.
The comparator in Listing 9-24 expects input lines of the form:
It will do a primary sort using the first IP address, secondary on the second IP address, and tertiary on the network name.
If at any point there is a parse failure, the element that the parse failed on is considered greater.
The parsing is deferred as long as possible in the hopes that it.
This code tries very hard to work at the byte level and not convert items back into strings.
Do the basic check on <code>a</code>, see if we find the first bit.
Do the basic check on <code>b</code>, see if we find the first bit.
Do the ip address comparison on the first IP, * if they are different, this routine is done.
Since we have longs and the result is int, a simple * subtraction may not work as the result may not be an int.
At this point both pairs of IP addresses are the same.
Listing 9-25 shows the commands used to generate the output.
The first command runs the BruteForceMapReduceDriver, passing in the JAR file included with the book examples, and specifies that ten reduce tasks are to be run:
Most of our later examples accept the arguments -v –deleteOutput, enabling verbose logging and causing the job output directory to be deleted if the directory exists.
The first output directory is range_join, which will be the input directory of the next command.
The second line runs the command DataJoinReduceOutput to take the ten partition files and produce a single file that is sorted in IP address order, with secondary sorts on the network begin and end addresses and the network name.
Design 3: Future Possibilities Two possibilities come to mind for this sample MapReduce job:
An indexed map file of search requests in the reduce task: For each search request key, the for the entire search space or a partitioned file—where the partition contains the networks would be used to find search space records that could match.
Map-side join of the presorted search requests and a presorted search space: This method requires presorting the search request records and the search space records, and then using the map-side join techniques discussed in Chapter 8 and the classes for working with the IP address described in this chapter.
Also, in both cases, the search space records can either be partitioned as the search request records are partitioned, or the entire search space be present in each task, in Google Bigtable style (see http://labs.google.
The partitioned case reduces the data volume that must be scanned.
Even with indexes, the amount of data that needs to be fetched from disk will be smaller in the partitioned case.
The downsides are that search space needs to be repartitioned if the number of reduce tasks for the search requests is changed, and there is additional (though small) code complexity to ensure that the correct search space map file is opened in each search request reduce task.
Both techniques lose the data being local for at least the search space records, and neither seem worth the bother at present, as it is not clear that there would be any performance gain.
They also require the search request records to be sorted, and the search space is expected to be relatively small.
In the process, you have seen a number of design decisions made that become invalid as understanding arrives.
The design and development process was deliberately oriented to provide initial functionality quickly so that this understanding could arrive sooner, rather than after a large and costly development cycle.
A number of the advanced features, such as chaining and map-side joins, were used in the application, and a partitioner and several comparators were written.
The tight coupling between the custom partitioner and the comparator allowed the application to perform range-based matching very efficiently using MapReduce techniques.
The techniques that you have learned will allow you to efficiently and effectively tackle very complex problems that do not appear to fit the MapReduce framework, but in fact are ideally suited for MapReduce.
Particularly in the rapidly evolving environment of today, you will never have time to build the perfect application—just an application that works for yesterday’s goals.
Someone else will come along later and modify the application until it meets the new goals.
Be kind to that person by leaving comments, testing, and keeping it simple.
People use Hadoop to solve many types of problems, and a number of teams have built packages on top of Hadoop Core to address an even larger scope of problems.
This chapter will walk through some of the many tools being built on top of Hadoop and one tool that can be built into Hadoop.
You’ll see a section on changes later in this chapter.)
This section will provide an overview of them and, when feasible, show a quick example of how to set up and use them (as well as what problems users might encounter)
I have little to no experience with most of the projects listed in this chapter, so the information in this chapter is gleaned from reading the project or company web site and/or trying the examples from a current release.
HBase: HDFS-Based Column-Oriented Table The project description describes HBase as the Hadoop database―an open source, columnoriented structured datastore based on the Google BigTable paper, http://labs.google.com/ papers/bigtable.html.
The earlier versions of HBase used the Hadoop MapFile as the underlying storage mechanism and managed updates by maintaining overlay MapFiles.
When there were sufficient updates, a merged file was reconstructed, and the overlays were discarded.
To speed access and distribute access, each individual MapFile is responsible for only a specific.
More recent versions of HBase also provide a memcached-based intermediate layer between the user and the MapFiles (http://www.danga.com/memcached/)
Prior to the addition of the memcached layer, HBase suffered terrible performance for random reads and writes, primarily because HDFS is not optimized for low latency random access.
HBase has a number of server processes, a single HBaseMaster that manages the HBase cluster and a set of HRegionServers, each of which is responsible for a set of MapFiles containing column regions.
HBase suffers terribly from the inability of applications to flush file data to storage before the file is closed, and a crash of any portion of the HBase servers or a service interrupting crash of HDFS will result in data loss.
In prior chapters there was a discussion of problems caused by applications or server processes attempting to exceed the system-imposed limit on the number of open files; HBase also has this problem.
The problem is substantially aggravated because each Hadoop MapFile is actually two files and a directory in HDFS, and each HDFS file also has a hidden checksum file.
Setting the per-process open file count very large is a necessity for the HBase servers.
A storage file format, HFile, is under development and due for Hbase version 0.20.0, and is expected to solve many of the performance and reliability issues.
HBase relies utterly on a smoothly performing HDFS for its operation; any stalls or DataNode instability will show up as HBase errors.
There are HDFS tuning parameters suggested in the troubleshooting section on the HBase wiki: http://wiki.apache.org/hadoop/ Hbase/Troubleshooting.
In particular, if the underlying HDFS cluster is experiencing a slow block report problem, https://issues.apache.org/jira/browse/HADOOP-4584, HBase is not recommended.
HBase servers, particularly the version using memcached, are memory intensive and generally require at least a gigabyte of real memory per server; any paging will drastically affect performance.
Java Virtual Machine (JVM) garbage collection thread stalls are also causing HBase failures.
HBase generally provides downloadable release bundles that track the Hadoop Core distributions.
Hive: The Data Warehouse that Facebook Built Hive provides a rich set of tools in multiple languages to perform SQL-like data analysis on data stored in HDFS.
The wonderful people at Facebook have contributed Hive to the Apache project.
As of the publication of this book, Hive is undergoing active development.
Compiled versions of Hive are part of the contrib subtree of the Hadoop Core distribution.
Cloudera, discussed later in this chapter, provides online training for Hive.
Setting Up and running hive The following four lines are required before attempting to start Hive (your installation might already have the /tmp and /user/hive/warehouse directories present):
The only issue I encountered when running Hive was a problem with a missing JAR because of an error I introduced into the conf/hadoop-env.sh file (see Listing 10-1)
The contrib/hive/bin/hive script sets HADOOP_CLASSPATH with the set of JARs that Hive requires and then invokes the bin/hadoop script to start the Hive command-line interpreter.
The examples listed in the wiki page http://wiki.apache.org/hadoop/Hive/ GettingStarted did not work particularly well for me (they might be updated by the time you read this chapter)
The language is named Pig Latin, and the Pig project provides a compiler that produces MapReduce jobs from a Pig Latin script.
Pig is not distributed with Hadoop Core, and is mature enough that the project has releases.
At the time of writing, Pig 0.2.0 has been released.
Pig also provides grunt, an interactive shell, for running Pig Latin commands directly.
Cloudera, listed later in this chapter, provides online training for Pig.
The setup is as simple as unpacking the distribution and setting the environment variable PIG_CLASSPATH to the directory that contains the hadoop-site.xml file that defines your cluster.
Its plan is to build libraries for the ten machine learning algorithms listed in http://www.cs.stanford.edu/ people/ang//papers/nips06-mapreducemulticore.pdf.
As of the time of writing, the first release, 0.1, has been made available for download.
The Taste project (a recommendation engine) has become a part of Mahout and is included in the 0.1 release.
Mahout requires Maven for operation, and it is not clear from the documentation how to run the examples, including the Taste examples, without Maven.
Mahout also provides a number of distributed clustering algorithms, including k-means, dirichlet, mean-shift, and canopy.
There are also two Bayesian classifiers: the naive and the complementary naïve.
An implementation of watchmaker is provided for building evolutionary algorithms and support for matrix and vector operations.
The project is intended to be used for large-scale numerical analyses and data mining.
The project will provide matrix-vector and matrix-matrix multiplication, linear equation solving, tools for working with graphs, data sorting, and methods of finding eigenvalues and eigenvectors.
ZooKeeper maintains a shared namespace that looks very similar to a hierarchical file system.
Each of these namespace entries may have data associated with it.
The entry data is accessed atomically, and changes are ordered.
In addition, ZooKeeper provides an ephemeral node, an entry that vanishes when the service holding the entry open disconnects.
The ephemeral nodes are used to establish service masters and sets of backup servers.
Ephemeral nodes are used to support redundant servers with hot failover.
ZooKeeper has been designed to be very reliable and very fast in environments in which data is primarily read.
The examples at http://hadoop.apache.org/zookeeper/docs/current/recipes.html provide ZooKeeper recipes for two-phase commit, leader election, barriers, queues, and locks.
Lucene: The Open Source Search Engine The Lucene project, http://lucene.apache.org/java/docs/, provides the standard open source package used for search engines.
The Lucene core provides the ability to take in documents in a variety of formats and build inverted indexes out of the terms found in the documents.
Lucene also provides a query engine that takes incoming queries, searches the indexes, and returns the documents that match.
Hadoop Core provides a contrib package that manages indexes that are stored in HDFS: contrib/index/hadoop-<rel>-index.jar.
The contrib package supports distributed indexes, shards, and unified indexes.
SOLr: a rich Set of Interfaces to Lucene The SOLR project, http://lucene.apache.org/solr/, is a stand-alone, enterprise-grade search service built on top of Lucene.
Katta: a Distributed Lucene Index Server The Katta project, http://katta.sourceforge.net/, describes itself as Lucene in the Cloud, a scalable, fault-tolerant, distributed indexing system capable of serving large replicated Lucene indexes at high loads.
Katta uses ZooKeeper to coordinate among the individual servers of the Katta cloud.
Katta supports storing shards on the local server file system, HDFS, and in Amazon’s S3
Katta also provides a distributed scoring service, allowing for the search results from multiple indexes to be merged together.
The core concept is that of defining a type in a text file and having a tool generate per-language APIs for accessing the data structure and for serializing and deserializing the data structure.
As of Hadoop 0.17.0, the framework supports using any type that provides serialization services as a key or a value.
Cascading: A Map Reduce Framework for Complex Flows Cascading, http://www.cascading.org/, describes itself as a rich API for handling complex scale-free workflows reliably on a MapReduce cluster.
The Cascading package allows the rapid wiring of components together into workflows that support flow control statements.
Cascading’s metaphor is that the incoming data flows through a series of functions and filters that allow the data to be split into multiple streams and then joined together again as needed.
An acyclic-directed graph is built by the framework, out of the functions and filters.
CloudStore: A Distributed File System CloudStore, http://kosmosfs.sourceforge.net/  (formerly known as the Kosmos file system), provides an alternative file system for use within a MapReduce cluster.
The Hypertable site is clear that the project is at a 0.9 release.
Currently, the core servers for Hypertable, the Master server and Hyperspace server, are single points of failure.
Hypertable does not provide ready-to-run distributions and must be built from source.
It provides a download link to allow you to try its software.
It also provides an in-database MapReduce that interoperates with SQL.
CloudBase: Data Warehousing The CloudBase project, http://cloudbase.sourceforge.net/, provides a high-performance, data warehousing system built on top of MapReduce, with an ANSI SQL API.
The project is developed by business.com to speed terabyte scale web log analysis.
The web site provides detailed instructions for running CloudBase instances on Amazon’s elastic compute (EC2) service.
Hadoop in the Cloud Sometimes you need additional compute resources for only a short time, you want to experiment with particular configurations, or you just don’t want to manage your own hardware.
Cloud service vendors provide the ability to spin up clusters of almost arbitrary size and capacities for short to long durations.
The best-known cloud server provider at the time of writing is Amazon, and there is direct support for running Hadoop in its cloud.
Amazon Amazon, http://aws.amazon.com, provides a large set of cloud computing services:
The	Elastic	Block	Store	(EBS),	http://aws.amazon.com/ebs/, provides persistent storage within EC2 and is ideal for longer-running HDFS clusters.
The	Elastic	MapReduce	service	provides	on-demand	Hadoop	clusters,	using	S3	as	the job input and output file system.
The one significant downside to Hadoop in the Amazon cloud is that there is no real data locality―something Hadoop works hard to achieve.
Caution Anything stored on an EC2 machine instance vanishes when the instance is shut down.
At the time of writing, the base was Hadoop 0.18.3, with important fixes and features back ported from later versions, including unreleased versions.
This is an ideal distribution for production use because it provides minimal API changes while providing bug fixes and some new features.
It provides free online basic Hadoop training at http://www.cloudera.com/hadoop-training-basic, Hive training at http://www.cloudera.com/hadoop-training-hive-introduction, and Pig training at http://www.cloudera.com/hadoop-training-pig-introduction/
There is also a session on using Eclipse with Hadoop at http://www.cloudera.com/blog/2009/04/20/configuringeclipse-for-hadoop-development-a-screencast/
Supported Distribution Cloudera provides a freely downloadable version of its distribution at http://www.cloudera.
The virtual machine has an Eclipse installation set up for use with its Hadoop distribution.
Note I used the Cloudera training virtual machine to work up some of the examples in this book.
The EC2 image has Hive and Pig installed and ready to use.
The principals are the Cascading project lead and the Katta project lead.
Our consultants’ experience does not end with Map Reduce patterns and Hadoop Distributed File System deployment models; but also spans over a wide set of related open.
Scale Unlimited also sponsors a live CD image of a Solaris installation with a three-node Hadoop cluster in zones (http://opensolaris.org/os/project/livehadoop/)
Note A live disk is a CD or DVD that boots as a running instance, not requiring any changes to the local machine’s hard disk.
An image is an .img file that most CD/DVD burner applications can burn directly to writable media.
At the time of writing, it is becoming clear that it is not ready for production use.
This section hopes to whet your appetite for these new features and help you plan for their arrival.
Vaidya: A Rule-Based Performance Diagnostic Tool for MapReduce Jobs Vaidya processes the log file data of previously run jobs and provides suggestions on how to improve performance.
How	evenly	the	data	is	partitioned	between	the	reduce	tasks.
Whether	map	task	failure	and	re-executions	are	affecting	the	overall	job	performance.
Whether	reduce	task	failure	and	re-executions	are	affecting	the	overall	job performance.
Whether	the	io.sort.space size is sufficient to prevent the map tasks outputs from being spilled to disk during the map-side sort phase.
Whether	substantial	data,	other	than	the	key/value	pairs,	is	being	read	from	HDFS	during the map or reduce tasks.
Service Level Authorization (SLA) The SLA package provides the access control lists for the control APIs of the various Hadoop Core servers, providing some assurance that any client connecting to a server with SLA enabled is an authorized client.
There are plans to bring in another LZO-like codec with a license the Apache Foundations will accept.
Zero-Configuration, Two-Node Virtual Cluster for Testing The class com.apress.hadoopbook.RunVirtualCluster in test/src of the examples will start and run a mini–Hadoop cluster that provides a near-full Hadoop Core installation.
To run it, change to a directory that will be used as the virtual cluster local storage, and run the following:
The cluster will be started, information about the web GUI URLs will be printed to stdout, and a configuration file that defines the relevant parameters for this virtual cluster will be written to the file saved_configuration.xml.
I find this particularly handy for debugging jobs when I am on the road because the HDFS data persists after the debugger has exited, and I can examine the job status via the web GUIs.
The only problem I have is that the per-task log files are not available via the web GUI, and the HDFS files are not available via the web GUI because of issues inside the Hadoop-supplied MiniMRCluster code.
The following command lists the files in the virtual HDFS:
This came into being when I was trying to work on the unit tests while on the road, using a machine with Windows XP as the host operating system.
The virtual clusters would periodically not start, and I became very frustrated.
I wrote this and after it started, it stayed running, and I could use it reliably for multiple tests.
The ability to examine the data files in HDFS and to interact with the web interfaces was a pleasant discovery.
Eclipse Project for the Example Code The example code was developed in Eclipse 3.4, and the project and class path files are part of the download, enabling you to load up, experiment with, and run the example code.
Many people and organizations are leveraging the power of Hadoop MapReduce and providing domain-specific package tools.
Distributed column-oriented databases are the current mantra of the scalable web services community; and HBase and Hypertable provide them.
Data mining, extracting, transforming, and loading without having to write custom MapReduce jobs are provided with Hive and Pig.
Machine learning and recognition are provided by Mahout and Hama, and distributed search is provided by the Katta project.
I am partial to the Cloudera Hadoop distribution because it has good support, back ported fixes, training, is free, and is responsive to community needs.
Try the various packages discussed in this chapter―explore and enjoy.
Everything in a job is controlled via the JobConf object; it is the center of the universe for a MapReduce job.
The framework will take the JobConf object and render it to XML; then all the tasks will load that XML when they start.
This section will cover all the relevant methods (as of Hadoop Core 0.19.0) and provide some basic usage examples.
Because the JobConf object is the primary interface between the programmer and the framework, I’ll detail all methods available to the user of a JobConf without distinguishing which methods come from the Configuration base class.
I suggest that you create and use only JobConf objects.
By default, a new JobConf object loads and merges the hadoop-default.xml and hadoop-site.xml files, as shown in Figure A-1
Virtual Machine (JVM) classpath and merged into the configuration data in the order added.
Configuration values that are loaded as resources are stored separately from the values that are set via setter calls.
The values that were loaded via resources are removed by a call to for a value, a value set by a setter call takes precedence over a value loaded from a resource.
Each configuration item is a name and value pair with an optional final parameter.
These parameters tell the Hadoop framework code how to contact the cluster, are defaults for various attributes, and allow for passing arbitrary values to the tasks.
The conf/hadoop-default.xml file has a list of most of the Hadoop Core framework parameters.
Other parameters are found only by reading the source code.
You can set arbitrary names for value pairs in the configuration, and these name-value pairs are made available to MapReduce tasks.
Values that are objects are serialized and then deserialized by each MapReduce task when tasks start.
The naming convention for configuration parameters is usually area.subarea.specific name.
The parameters that configure the distributed file system start with dfs, and the parameters that configure the MapReduce framework start with mapred.
How configuration data is loaded into the JobConf object and resolved.
In the job driver, the JobConf object is constructed with all the parameters for the job.
At job runtime, required data, the JobConf object, JAR files, archives, and other resources are stored in the Hadoop Distributed File System (HDFS) in a job-specific directory.
In the task, the JobConf object is reconstituted and localized, and it is given a set of directories between the paths defined in mapred.local.dir.
Any items that must be referenced from the local file system, such as the job JAR file or other items passed via the DistributedCache, are unpacked into these local directories and the path references to items in the configuration are adjusted to be the task local path.
The classpath for the JVM that the task will run in is also set up for the task to include the location on the local file system that the classpath resources were unpacked into.
JobConf Is a Properties Table The JobConf instances maintain a table of key/value pairs for all the configuration parameters.
The values are all stored as String objects and are serialized if they are objects.
At the lowest level, operations get a value for a key or store a value for a key.
Variable Expansion The JobConf object performs variable expansion on values when raw returned values have special text embedded in them.
The syntax is ${key}, which will be replaced by the value of key.
In the configuration files you will often see values in this form:
If there is no value found, the value is resolved against the configuration in the JobConf object.
This process continues until there are no items that are candidates for expansion or there are no items that can be expanded.
The first key examined is no.expansion; in Listing A-1, the value is defined as no.expansion Value, which is the result printed.
The expanded result is The value of no.expansion is no.expansion Value, showing that the ${no.expansion} was replaced by the value of no.expansion in the configuration.
The item for expansion.from.JDK.properties demonstrates that the key/value pairs in the System.properties are used for variable expansion.
Note that the actual system property value for java.io.tmpdir is used, not the value stored in the configuration for java.io.tmpdir, failed attempt to override a System.properties value for variable expansion.
The final example demonstrates that the variable expansion results are candidates for further expansion.
Simple class to demonstrate variable expansion * within hadoop configuration values.
This relies on the hadoop-core jar, and the * hadoop-default.xml file being in the classpath.
Get a local file system object, so that we can construct a local Path * That will hold our demonstration configuration.
Final Values The Hadoop Core framework gives you a way to mark some keys in a configuration file as final.
The stanza <final>true</final> prevents later configuration files from overriding the value specified.
The <final> tag does not prevent the user from overriding the value via the set method.
The example in Listing A-3 creates several XML files in the temporary directory: the first file, finalFirst, contains the declaration of a configuration key, final.
The second file, finalSecond, also defines final.first with the value This should not override found to be first final value.
Ensure that the output writer is closed even on errors.
Get a local file system object, so that we can construct a local Path * That will hold our demonstration configuration.
Add the additional file that will attempt to overwrite * the final value of final.first.
Constructors All code that creates and launches a MapReduce job into a Hadoop cluster creates a JobConf object.
This constructor should not be used because it doesn’t provide the framework with information about the JAR file that this class was loaded from.
This common use case constructor is the constructor you should use.
The archive that the exampleClass was loaded from will be made available to the MapReduce tasks.
The type of exampleClass is arbitrary; exampleClass is used only to find the classpath resource that the exampleClass was loaded from.
The containing JAR file will be made available as a classpath item for the job tasks.
The JAR is actually passed via the DistributedCache as a classpath archive.
Tip The task JVMs are run on different physical machines and do not have access to the classpath or the classpath items of the JVM that submits the job.
The only way to set the classpath of the task JVMs is to either set the classpath in the conf/hadoop-env.sh script or pass the items via the DistributedCache.
This constructor is commonly used when your application already has constructed a JobConf object and wants a copy to use for an alternate job.
The configuration in conf is copied into the new JobConf object.
It is very handy when unit testing because as the unit test can construct a standard JobConf object, and each individual test can use it as a reference and change specific values.
If your driver launches multiple MapReduce jobs, each job should have its own JobConf object, and the pattern described previously for unit tests is ideal to support this.
Construct a new JobConf object that inherits all the settings of the passed-in Configuration object conf, and make the archive that exampleClass was loaded from available to the MapReduce tasks.
Classes that launch jobs that may have unit tests or be called as part of a sequence of Hadoop jobs should provide a run method that accepts a Configuration object and calls this constructor to make the JobConf object for that class’s job.
This way, the unit test or calling code can preconfigure the configuration, and this class can customize its specific variables and launch the job.
Ensure that the archive that contains this class will be * provided to the map and reduce tasks.
Construct a JobConf object with configuration data loaded from the file that config is a path to.
Construct a JobConf object and load configuration values from the XML data found in the file config.
This constructor is used by the TaskTracker to construct the JobConf object from the job-specific configuration file that was written out by the Hadoop framework.
This method is identical to the no-argument constructor unless the loadDefaults value is false.
If loadDefaults is false, hadoop-site.xml and hadoop-default.xml are not loaded.
Methods for Loading Additional Configuration Resources The methods described in this section load an XML configuration file resource and store it in the JobConf parameter set.
The order in which these methods are called is important because the contents specified by the most recent call will override values supplied earlier.
If a specified resource cannot be loaded or parsed as valid configuration XML, a RuntimeException will be thrown unless quiet mode is enabled via a call to setQuietMode (true)
Each call to one of these methods results in the complete destruction of the configuration data that resulted from the loading and merging of the XML resources.
There are no changes made to the configuration parameters that have been created via the set methods.
The entire set of XML resources is reparsed and merged on the next method call that reads or sets a configuration parameter.
These resource items follow the same rules as with the hadoop-default.xml and hadoop-site.xml files, and a parameter in a resource object can tag itself as final.
In this case, resource objects loaded later may not change the value of the parameter.
If quietmode is true, no log messages will be generated when loading the various resources into the configuration.
If a resource cannot be parsed, no exception will be thrown.
If quietmode is false, a log message will be generated for each resource loaded.
If a resource cannot be parsed, a RuntimeException will be thrown.
The parameter is loaded from the current classpath by the JDK ClassLoader.getResource method.
The default configuration has two addResource( String name ) calls: one for hadoop-default.xml and the other for hadoop-site.xml.
Caution The first hadoop-default.xml file and the first hadoop-site.xml file in your classpath are loaded.
It is not uncommon for these files to accidentally be bundled into a JAR file and end up overriding the cluster-specific configuration data in the conf directory.
A problem often happens with jobs that are not run through the bin/hadoop script and do not have a hadoop-default.xml or hadoop-site.xml file in their classpath.
This method explicitly loads the contents of the passed-in URL, url, into the configuration.
This method explicitly loads the contents of file into the configuration.
Load the XML configuration data from the supplied InputStream in into the configuration.
Clear the current configuration, excluding any parameters set using the various set methods, and reload the configuration from the resources that have been specified.
If the user has not specified any resources, the default pair of hadoop-default.xml and hadoop-site.xml will be used.
This method actually just clears the existing configuration, and the reload will happen on the next get or set.
Basic Getters and Setters The methods in this section get and set basic types:
In	general,	if	the	framework	cannot	convert	the	value	stored	under	a	key	into	the	specific type required, a RuntimeException will be thrown.
If	the	value	being	retrieved	is	to	be	a	numeric	type,	and	the	value	cannot	be	converted to the numeric type, a NumberFormatException will be thrown.
For	boolean	types,	a	value	of	true is required for a true return.
For	values	that	are	class	names,	if	the	class	cannot	be	instantiated,	or	the	instantiated class is not of the correct type, a RuntimeException will be thrown.
There is no mechanism currently to escape a comma that must be a part of an individual item in a list.
Under the covers, all data is stored as a java String object.
All items stored are serialized into a String object, and all values retrieved are deserialized from a String object.
The user is required to convert objects into String representations to store arbitrary objects in the configuration and is responsible for re-creating the object from the stored String when retrieving the object.
This is the basic getter: it returns the String version of the value of name if name has a value or if the method returns null.
If the value is a serialized object, the results of the variable expansion may be incorrect.
Returns the raw String value for name if name exists in the configuration; otherwise returns null.
This is the method to use to retrieve serialized objects.
Stores the value under the key name in the configuration.
Any prior value stored under name is discarded, even if the key was marked final.
Many properties stored in the configuration are simple integers, such as the number of reduces, mapred.reduce.tasks.
If the underlying value for name is missing or not convertible to an int, the defaultValue is returned.
Stores the String representation of value in the configuration under the key name.
Many properties stored in the configuration are simple long values, such as the file system block size dfs.block.size.
If the underlying value for name is missing or not convertible to a long, the defaultValue is returned.
Stores the String representation of value in the configuration under the key name.
Some properties stored in the configuration are simple floating-point values.
You might want to pass a float value to the mapper or reducer, which would use this method to get the float value.
If the underlying value for name is missing or not convertible to a float, the defaultValue is returned.
Many properties stored in the configuration are simple boolean values, such as the controlling speculative execution for map tasks, mapred.map.tasks.speculative.execution.
If the underlying value for name is missing or not convertible to a boolean value, the defaultValue is returned.
The comparison is case sensitive, so a value of True will fail to convert, and the defaultValue will be returned.
Convert the boolean value to the String true or the String false and store it in the configuration under the key name.
The configuration supports storing basic types such as various numbers, boolean values, text, and class names.
It is two integer values, in which the second integer is larger than the first.
An individual range is specified by a String containing a -, a dash character that can also have a leading and trailing integer.
If the leading integer is absent, the first range value takes the value 0
If the trailing integer is absent, the second range value takes the value of Integer.MAX_VALUE.
Multiple ranges may be separated by ,: a comma character such as 1–5,7–9,13–50
As of Hadoop 0.19.0 it is used only to determine which MapReduce tasks to profile.
As of Hadoop 0.19.0 there is no corresponding set method, and the base set( String name, String value) is used to set a range The value has to be the valid String representation of a range or later calls to the getRange method for name will result in an exception being thrown.
The defaultValue must be passed in as a valid range.
String null may not be passed as the default value, or else a NullPointerException will be thrown.
This method looks up the value of name in the configuration, and if there is no value, the defaultValue will be used.
The resulting value will then be parsed as an IntegerRanges object and that result returned.
The JobConf and Configuration objects  (at least through Hadoop 0.19.0) handle parameters that are sets of String objects by storing them internally as comma-separated lists in a single String.
The JobConf and Configuration objects  (at least through Hadoop 0.19.0) handle parameters that are sets of String objects by storing them internally as comma-separated lists in a single String.
This method gets the value associated with name in the configuration, splits the String on commas, and returns the resulting array (see Listing A-7)
As of Java 1.5, variable argument lists are supported for method calls.
The declaration of the last parameter may have an ellipsis between the type and the name, type...name.
The caller can place an arbitrary number of objects of type in the method call, and the member method will receive an array of type with the elements from the caller’s call.
The JobConf and Configuration objects  (at least through Hadoop 0.19.0) handle parameters that are sets of String objects by storing them internally as comma-separated lists in a single String.
This method will get the value associated with name in the configuration and split the String on commas and return the resulting array (see Listing A-8)
If there is no value stored in the configuration for name, the array built from the defaultValue parameters will be returned.
Stores the set of Strings provided in values under the key name in the configuration, deleting any prior value (see Listing A-9)
The set of String objects defined by values is concatenated using the comma (,) character as a separator, and the resulting String is stored in the configuration under name.
ClassNotFoundException It attempts to load a class called name by using the JobConf customized class loader.
If the class is not found, a ClassNotFoundException is thrown.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
If that class loader is unavailable, the class loader used to load the Configuration.class is used.
If a class cannot be loaded for any entry in the list, a RuntimeException is thrown.
If name does not have a value in the configuration, this method returns the array of classes passed in as the defaultValue.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
If that class loader is unavailable, the class loader used to load the Configuration.class is used.
If name is present in the configuration, it attempts to load the value as a class using the configuration’s class loader.
If a value exists in the configuration and a class cannot be loaded for that value, a RuntimeException is thrown.
If name does not have a value, the class defaultValue is returned.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
If that class loader is unavailable, the class loader used to load the Configuration.class is used.
If a value exists in the configuration, and a class cannot be loaded for that value, a RuntimeException is thrown.
The loaded class must derive from or implement xface, or else a RuntimeException will be thrown.
If no value is present for name in the configuration, the defaultValue will be returned.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
If that class loader is unavailable, the class loader used to load the Configuration.class is used.
Class<?> xface) The class name for theClass is stored in the configuration under the key name.
If theClass does not derive from or implement xface, a RuntimeException is thrown.
Getters for Localized and Load Balanced Paths The framework provides the capability for multiple local directories to be specified for task temporary files.
Multiple locations are allowed to load balance the I/O over multiple devices.
The framework will attempt to select one location set either at random or in sequential order.
The ordering used will be given in the method description.
Each of the methods described in this section is called with a trailing path component that includes a final file name.
The return value will be the full path to the file; all the intermediate directory components will be constructed if needed.
The first found complete path that can be created or exists will be returned by the method.
The intermediate directories may be constructed in locations that do not allow the complete construction of the path.
If so, those intermediate directories that have been created will not be removed.
If no path can be constructed, an IOException will be thrown.
These throwing IOException paths are explicitly constructed on the local (host) file system.
The pseudorandom method does not guarantee that all possible path candidates will be tried; only that no more than the count of path candidate elements will be tried (as of Hadoop 0.19.0)
Also as of Hadoop 0.19.0, the method does not fail if the path candidate is a file, not a directory.
The goal is to return a resultant path composed of pathTrailer as the trailing component and one element out of the set of directories stored under dirsProp as the path leader.
This method uses the Hadoop LocalFileSystem object for all path operations.
The paths defined by dirsProp are searched in a pseudo-random order.
IOException This method is used to load balance access to a set of directories that reside on different devices.
The goal is to return a resultant path composed of pathTrailer as the trailing component and one element out of the set of directories stored under dirsProp as the path leader.
This method uses the java.io.File methods to create directory paths and test for directory existence.
The paths defined by dirsProp are searched in a pseudo-random order.
This method looks up the key mapred.local.dir in the configuration.
The value is expected to be a set of file system paths separated by commas.
If present, the value is split on comma characters and the resulting array of String objects is returned.
If there is no value present, a null is returned.
This is used by the TaskTracker to find the set of directories to use for.
The TaskTracker uses a round robin strategy to allocate a task directory for a new task.
This parameter is generally only used directly by the framework.
This method deletes all the directory trees stored in the configuration under the key mapred.
The value is parsed as a comma-separated list of paths.
This is used by the framework to clean up the local machine temporary areas on TaskTracker start and TaskTracker exit.
This method deletes subdir from all the directories that are stored in the configuration under the key mapred.local.dir.
The value is parsed as a comma-separated list of paths.
This is used by the framework to clean up the local machine temporary files for a particular task.
IOException This method looks up the key mapred.local.dir in the configuration and parses the value as a comma-separated list of local file system paths.
For each directory in the resulting list, an attempt is made, in pseudo-random order, to create the path portion of pathString, including any leading directory elements.
If after this creation attempt that directory exists, the file name portion of pathString is appended to the directory path and the resulting path is returned.
This method looks up the key job.local.dir in the configuration and returns the value.
The value will be the task-specific shared directory for each job on each TaskTracker.
This parameter is set only in the JobConf object passed to each task.
The returned value will be the path fragment taskTracker/jobcache/JobId/work, prefixed by one of the directories specified in the set of directories stored under the key mapred.local.dir.
This is used by the framework when setting up the per-job task environment on a TaskTracker node.
Tasks can use this method to find the path to the task-specific directory on the local file system, which may be used for temporary file storage.
Methods for Accessing Classpath Resources The framework provides a way for tasks to access resources from the task-specific classpath objects.
Returns the URL for the resource name, found by searching the configuration’s class loader.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
If that class loader is unavailable, the class loader used to load the Configuration.class is used.
String name) Returns the java.io.InputStream resulting from opening the URL for the resource name, found by searching the configuration’s class loader.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
If that class loader is unavailable, the class loader used to load the Configuration.class is used.
Caution This method does not look up the value of name in the configuration; name is the value passed to the class loader.
Returns the java.io.Reader resulting from opening the URL for the resource name, found by searching the configuration’s class loader.
By default, the class loader used to load the class is the class loader for the thread that initialized the JobConf object.
If that class loader is unavailable, the class loader used to load the Configuration.class is used.
This method does not look up the value of name in the configuration.
The name is used directly as a Java resource name.
This is approximately equivalent to new InputStreamReader the ClassLoader member variable of the configuration.
Caution getConfResourceAsReader does not look up the value of name in the configuration; name is passed directly to the class loader.
Methods for Controlling the Task Classpath These methods ensure that the objects referenced are distributed to the task nodes and made available in the classpath of the tasks.
The key mapred.jar is the JAR to use for the MapReduce job.
The mapred.jar key’s value is set by the setJar(String jar) method, the setJarByClass (Class cls) method, and the JobConf constructors that take a Class value as a parameter.
If the mapred.jar key has been set in the configuration, the value will be returned.
Stores the String jar, which should be the path to the JAR that contains the map and reduce classes for this job into the configuration under the mapred.jar key.
Any prior value stored under the key mapred.jar will be discarded.
This archive will be distributed to the task nodes and placed in the classpath for the map and reduce tasks.
Looks for the first JAR file in the classpath that contains class cls.
If found, stores the path to it under the mapred.jar key in the configuration.
If no JAR file is found that contains cls, a RuntimeException is thrown.
Methods for Controlling the Task Execution Environment These methods control the setup and cleanup of the individual task environment.
This method returns the value stored in the configuration under the key user.name.
This parameter is generally initialized to the name of the user that launched the job, but this is not enforced.
Caution Usernames may be overwritten with a different username by any user.
This is not a security feature, and Hadoop permissions are not a security feature.
Through at least Hadoop 0.19 any user may claim to be any other Hadoop user and act fully as if they are that user, including the removal of files or the scheduling of jobs.
There is way to prevent this; you have to trust the users who have access to your cluster because any user can override any Hadoop level permission restrictions placed on that user.
The value of user is stored in the configuration under the key user.name.
Stores the value of keep in the configuration under the key keep.failed.task.files.
This value configures the framework to save or not save the intermediate output files of tasks that fail.
It is set to true to when the task output is needed to debug a failing job.
Returns the value stored in the configuration under keep keep.failed.task.files converted to a boolean.
If no value is found, or the value is not exactly true or false, the value false is returned.
Stores a Java regular expression String pattern into the configuration under the key keep.
If the task id of a task matches this regular expression, its temporary files will not be removed The file names are written as *_[mr]_[jobid]_[tasknumber]
Returns the value stored in the configuration under the key keep.task.files.pattern.
The framework calls this in the TaskTracker before cleaning up temporary files after a task completes.
If the task id matches the pattern, the temporary files are not removed.
The user calls it and if it has not been called by job submission time, the JobClient object will initialize it to the current working directory of the process submitting the job.
The default file system is the file system defined by the configuration key fs.default.name.
If this value is unset, this method first sets the value for the key to the default working directory for the default file system.
The default file system is defined by the configuration parameter fs.default.name; for HDFS, the default working directory is /user/USERNAME.
Prior to Hadoop 0.19.0, a new JVM was created for each task run by the TaskTracker.
As of Hadoop 0.19.0, the TaskTracker has the capability to reuse the task JVM for additional tasks.
The configuration key mapred.job.reuse.jvm.num.tasks’s value is the number of times that a JVM may be reused.
This method stores numTasks in the configuration under the key mapred.job.reuse.jvm.num.tasks.
Looks up the value of mapred.job.reuse.jvm.num.tasks in the configuration and converts the value to an integer.
If the value does not exist or if the value cannot be converted to an integer, it returns 1
Prior to Hadoop 0.19.0, a new JVM was created for each task run by the TaskTracker.
As of Hadoop 0.19.0, the TaskTracker has the capability to reuse the task JVM for additional tasks.
The value stored in the configuration under mapred.job.reuse.jvm.num.tasks is the number of times to use a JVM for a task.
Methods for Controlling the Input and Output of the Job The methods described in this section are used to configure how the jobs’ input and output will be handled.
This includes how the input is parsed and presented to the framework, the compression of intermediate and final output, and how the output is written.
This method looks up the value of the key mapred.input.format.class in the configuration and instantiates a class of that name.
If the value is missing, a TextInputFormat.class will be returned.
If the class name cannot be instantiated, or if the instantiated class is not an instance of InputFormat, a RuntimeException will be thrown.
The returned class will be used by the framework to read the input data set for the job.
The key/value pairs that the class extracts from the input will be passed to the map method of the mapper class in the map tasks.
There will be one instance created per map task, and that instance will receive the input split for that map task as input.
An instance of this class will be instantiated in each map task to convert the input split data into a set of key/value pairs for the map method of the mapper class.
If theClass does not implement the interface InputFormat, a RuntimeException will be thrown.
This method looks up the value of the key mapred.output.format.class in the configuration and instantiates a class of that name.
If the value is missing, a TextOutputFormat.class will be returned.
If the class name cannot be instantiated or if the instantiated class is not an instance of OutputFormat, a RuntimeException will be thrown.
There will be one instance of this class created for each reduce task.
By default, one instance of this class is created for each map task.
This class transforms the key/value pairs passed to the output in the If theClass does not implement the OutputFormat interface, a RuntimeException will be thrown.
The framework provides a unique output directory for each task and stores this directory in the per-task configuration under the key mapred.output.dir.
As of Hadoop 0.18, this key is set via the FileOutputFormat.setOutputPath static method.
As of 0.19.0, the OutputCommitter object is used to process the files in the per-task temporary area on successful task completion, and is responsible for deciding which output files.
Prior to this, any files present were moved to the userspecified output path.
This method retrieves the value stored in the configuration under the key mapred.
If the retrieved value is null, the FileOutputCommitter class will be returned.
If the retrieved value is not null, the method will attempt to instantiate a class using the value as the class name.
If the class name cannot be instantiated or if the instantiated class is not derived from the class OutputCommitter, a RuntimeException will be thrown.
This method will store the class name of theClass in the configuration under the key mapred.output.committer.class.
If theClass does not implement the OutputCommitter interface, a RuntimeException will be thrown.
See getOutputCommitter for a description of what the OutputCommitter is used for.
This method stores the String equivalent of the value of compress in the configuration under the key mapred.compress.map.output.
If the stored value is true, the map output data that will be consumed by the reduce phase will be compressed, using either the default compression codec or the codec specified by the method setMapOutputCompressorClass.
During a Hadoop job that has a reduce phase, the map phase produces intermediate output that will be further processed by the framework.
This output will eventually become the input to the reduce phase.
This output may be compressed to reduce transitory disk space requirements and network transfer requirements.
To enable map output compression when the job will not have a reduce phase, the call FileOutputFormat.setCompressOutput(conf, true) must be made.
Having the map output compressed can save substantial time because the amount of data that must traverse the network between the map and the reduce phase may be substantially reduced.
Having the job output compressed may also save substantial time because the amount of data to be stored in HDFS may be substantially reduced, greatly reducing the amount of network traffic for the replicas.
This method returns the value stored in the configuration under the key mapred.compress.
If the value is unset or is not one of true or false, the value false will be returned.
If this value is true, map task output that will be reduced will be compressed using the compression defined for SequenceFiles.
This method stores the class name of codecClass in the configuration under the key mapred.
An instance of this class will be used to compress the map task output that is to be passed to the reduce tasks if the configuration key mapred.
This key may be set by the JobConf method setCompressMapOutput(boolean)
If codecClass does not implement the CompressionCodec interface, a RuntimeException will be thrown.
If the value cannot be instantiated as a class that is derived from CompressionCodec, a RuntimeException will be thrown.
The Hadoop framework is responsible for loading the job input and converting that input into key/value pairs that are passed to the map method of the mapper, passing the key/value pairs output by the map method of method of the reducer and writing them to the job output.
The class that loads and transforms the input into key/value pairs is derived from InputFormat and requires that the type of the key and the type of the value be specified.
The class that handles loading the input is responsible for producing keys and values of the correct type.
A commonly used class is the KeyValueTextInput class, which parses the input as text files, with each record on a single line and the key and value separated by the first tab character.
The key type is org.apache.hadoop.io.Text, and the value type is org.apache.hadoop.io.Text.
If the job does not explicitly configure the map output class as org.apache.hadoop.io.Text or the job output class as org.apache.hadoop.io.Text, the reduce will fail with a key type mismatch error.
The types of these objects will be used to define the mapper class input key and value types.
The class to receive and transform the output key/value pairs is derived from OutputFormat.
The default value for the key class is LongWritable, and the default value for the value class is org.
The job may specify different classes via the setOutputKeyClass and setOutputValueClass methods, respectively.
By default, the expected map output types are the same as the expected reduce input and output types.
The job may specify that the map output key type and or the map output value type is different from the job output key and value type.
The setMapOutputKeyClass method allows the job to specify the map output key class and the reduce input key class as being different from the job output key class.
The setMapOutputValueClass method allows the job to specify the map output value class and reduce input value class as being different that the job output key class.
The class specified under the key map.sort.class in the configuration will be used to sort the key objects if a reduce has been requested by the job.
The default value for this key is org.apache.hadoop.util.QuickSort, an implementation of org.apache.hadoop.util.IndexedSorter.
The key and value classes can be any type as long as the framework is provided with serializer classes and deserializer classes that implement org.apache.hadoop.io.serializer.Serializer and org.apache.hadoop.io.serializer.Deserializer, and the class names are added to the list stored in the configuration under the key io.serialization.
This method stores the class name of theClass in the configuration under the key mapred.
The type of this class will be used as the type of the map method and deserializable by a class defined in the list of serializers specified in the value of the configuration key io.serializations.
This method looks up the value of the key mapred.mapoutput.key.class in the configuration.
If the value cannot be instantiated as a class, a default for it is LongWritable.
This method stores the class name of theClass in the configuration under the key mapred.
This class must be serializable by a class defined in the list of serializers specified in the value of the configuration key io.serializations.
This method looks up the key mapred.output.key.class in the configuration.
If the value is unset, the class object org.apache.hadoop.io.LongWritable will be returned.
If the value is set and a class of that name cannot be instantiated, a RuntimeException will be thrown.
This method stores the name of theClass in the configuration under the key mapred.output.
This method looks up the value of the key mapred.output.value.class in the configuration.
If the value is unset, the class org.apache.hadoop.io.Text is returned.
The value is instantiated as a class, and the class is returned.
If the value cannot be instantiated as a class, a RuntimeException will be thrown.
This method stores the name of theClass in the configuration under the key mapred.output.
If this class is used as a map output value, it must be serializable by a class defined in the list of serializers specified in the value of the configuration key io.serializations.
Methods for Controlling Output Partitioning and Sorting for the Reduce which adjacently sorted keys are considered equal for producing a value group to pass to the interface.
A class used as a key object in Hadoop may define an optimized comparator class.
The comparator must be registered with the framework by calling org.apache.hadoop.io.WritableComparator.define(Key.
The common key class org.apache.hadoop.io.Text defines a custom comparator that does a byte-wise comparison of the actual serialized text.
This avoids having to deserialize the Text object and then run String comparisons on the data in the reconstituted objects.
This method looks up the value of the key mapred.output.key.comparator.class in the configuration.
If the value is unset, the class org.apache.hadoop.io.WritableComparable WritableComparator will be returned.
If the value cannot be instantiated as a class that is an instance of org.apache.hadoop.io.RawComparator, a RuntimeException will be thrown.
This method stores the class name of theClass in the configuration under the key mapred.
If theClass does not implement the RawComparator interface, a RuntimeException will be thrown.
This method stores the String keySpec in the configuration under the key mapred.text.
This method also changes the OutputKeyComparatorClass (key mapred.output.key.comparator.class) to the class org.apache.hadoop.mapred.lib.
The key fields are separated by the character that is the value of the configuration key map.output.key.field.separator.
If there is no value set for the key map.output.key.field.
The key will be split on map.output.key.field.separarator characters into pieces.
The keySpec String is composed of one or more space-separated groups.
The	character	number	in	the	piece	to	start	the	comparison.
How	to	sort,	either	numerically	via	the	n option or in reverse order via the r option.
This is optional and defaults to the standard String comparison ordering.
This	is	optional	and	defaults	to	the starting piece number.
The	character	number	in	the	piece	to	end	the	comparison.
This	is	optional	and	defaults to the last character in the String: 0
There is a test class for these key fields in the examples that can be run by giving it three arguments: the key, the key spec for the combiner, and the key spec for the partitioner.
This method looks up the value of the key mapred.text.key.comparator.options and returns the value.
Please see setKeyFieldComparatorOption for a discussion of the appropriate values.
When a job is configured to have a reduce phase, the output will be split into partitions (one partition per reduce task)
The framework has a default partitioning strategy of using the hash code of the key, modulus a custom partitioning class.
This method looks up the value of the key mapred.partitioner.class in the configuration.
If the value is unset, the class org.apache.hadoop.mapred.lib.HashPartitioner is returned.
If the value is set, it is instantiated as a class that must be an instance of org.apache.hadoop.mapred.
If the value cannot be instantiated or is not an instance of the Paritioner class, a RuntimeException will be thrown.
HashPartitioner simply uses the hash value of the key, modulus the number of reduce tasks, to determine which reduce will receive any given key/value pair.
An instance of this class will be created for each map task and used to RuntimeException will be thrown.
This method stores the String keySpec in the configuration under the key mapred.text.key.
The output partitioning class will also be set to org.apache.hadoop.
The portion of the key selected will be hashed, and that hash modulus the number of reduces will be the partition number.
The keySpec String is composed of one or more space-separated groups.
How	to	sort,	either	numerically	via	the	n option and or in reverse order via the r option.
This is optional and defaults to the standard String comparison ordering.
This	is	optional	and	defaults	to	the starting piece number.
The	character	number	in	the	piece	to	end	the	comparison.
This	is	optional	and	defaults to the last character in the String: 0
If your job generates ArrayIndexOutOfBounds exceptions, explicitly end the key piece selection for the second key piece: -k2
This looks up the key mapred.text.key.partitioner.options in the configuration and returns the value.
For this value to have an effect, the output partitioner class must be org.apache.
It is often the case that there is a requirement for grouping output data.
Hadoop Core provides a way to group output values that acts very much like a secondary sort on the key data.
For this to work in the manner that the user expects, the output partitioner, the output comparator, and the output grouping comparator have to cooperate.
The outputKeyComparator must order the keys using the primary and secondary sort.
Because keys that must group together may not be equal in this method, the outputPartitioner has to be able to place keys that must group together into the same partition.
The outputValueGroupingComparator must return equality only for those keys that are equal in the primary sort.
This will result in a call to the Reduce.reducer method for each group of keys.
This method looks up the value of the key mapred.output.value.groupfn.class in the configuration and attempts to instantiate a class that is an instance of org.apache.hadoop.
If the value is unset, the comparator class for the Map key class is returned.
If the value cannot be instantiated or the resulting class does not implement org.apache.hadoop.
This method stores the class name of theClass in the configuration under the key mapred.
The use of this method enables a grouping operator on keys and a secondary sort.
The user must set both a partitioner and a comparator that cooperate for this to be used.
It is common for the default output comparator to be used to force complete sorting of the keys output.
The partitioner must ensure that all keys that are to be grouped together are sent to the same partition.
If keys are of the form item rank and the values are of the form data, the partitioner must use only item to partition.
The standard output comparator will sort lexically on item rank.
The method will receive all keys that share item, and the values will be lexically sorted by rank.
The output comparator would fully sort the keys by item rank.
The output value grouping comparator would use only item for comparing keys.
They specify if the map methods may be run from multiple threads or in a single thread.
They specify if the framework will attempt to run multiple instances of a task to see if one will run faster, and when to consider a task completely failed and a job completely failed.
The framework creates an instance of the mapper class in each map task.
The input of key/value pairs are treated as a queue, being serviced by a thread pool,
The user specifies this behavior by setting the map runner class to org.apache.hadoop.mapred.lib.MultithreadedMapRunner and by storing the number of threads to run in the configuration under the key mapred.map.multithreadedrunner.threads.
This method looks up the value of the key mapred.mapper.class in the configuration and attempts to instantiate the value as a class of type org.apache.hadoop.mapred.Mapper.
If the value is unset, the class org.apache.hadoop.mapred.lib.IdentityMapper is returned.
If the value cannot be instantiated as a class of the correct type, a RuntimeException is thrown.
The returned class will provide the map method that all the input data will be passed through.
This method stores the name of theClass class in the configuration under the key mapred.
An instance of this class will be created in each map task, and each input key/value pair will be passed to theClass map method.
If theClass does not implement the org.apache.hadoop.mapred Mapper interface, a RuntimeException will be thrown.
This method looks up the key mapred.map.runner.class in the configuration and instantiates the value as a class of type org.apache.hadoop.mapred.MapRunnable.
If the value is unset, the class org.apache.hadoop.mapred.lib.MapRunnable is returned.
MapRunnable> theClass) This method stores the name of theClass in the configuration under the key mapred.map.
When this is done, there is usually a setInt("mapred.map.multithreadedrunner.threads", threadCount) call.
The multithreaded map runner is very handy when the map method is not blocked waiting on local CPU or IO, such as when the map method is used to fetch URLs.
This method looks up the key mapred.reducer.class in the configuration and instantiates the value as a class of type org.apache.hadoop.mapred.Reducer.
If the value is unset, the class org.apache.hadoop.mapred.lib.IdentityReducer is returned.
If the value cannot be instantiated as a class of the correct type, a RuntimeException will be thrown.
If theClass does not implement the Reducer interface, a RuntimeException will be thrown.
One instance of this class will be created in each reduce task.
A combiner class is a minireducer that is run in the context of the map task to pregroup key/value pairs that share a key.
Combiners can greatly minimize the amount of output that has to pass between the map and reduce tasks and speed up the job.
Please see com.apress.hadoopbook.examples.ch5.CounterExamplesWithCombiner, and look at the NaiveReducer counter values and compare them against the reducer and combiner counter values.
This method looks up the key mapred.combiner.class in the configuration and instantiates the value as a class implementing the Reducer interface.
If the value cannot be instantiated as a class implementing the Reducer interface, a RuntimeException is thrown.
If theClass does not implement the Reducer interface, a RuntimeException is thrown.
In this environment, the amount of wall clock time for any given machine to execute a map or reduce task could vary widely because of differing machine capabilities.
In addition, there is no guarantee that any given InputSplit will take the same amount of wall clock time to execute.
Speculative execution informs the cluster that any unused task slots may be used to run duplicate instances of an already running task.
The first of these duplicates to complete has its results used, and the other task has its output discarded.
If your tasks do not have side effects that Hadoop cannot undo, do not consume resources with some real costs or load your machines so that other tasks run slower.
Ensure that speculative execution is disabled if your tasks have output that Hadoop cannot discard or side effects that Hadoop cannot undo.
If speculativeExecution is true, speculative execution will be enabled for both map and reduce tasks.
If speculativeExecution is false, speculative execution will be disabled for both map and reduce tasks.
This method looks up the value of the key mapred.map.tasks.speculative.execution in the configuration and converts that value to a boolean value, which is then returned.
If the value is not the String true, false is returned.
This method looks up the value of the key mapred.reduce.tasks.speculative.execution in the configuration and converts that value to a boolean value, which is then returned.
If the value is not the String true, false is returned.
This method looks up the value of the key mapred.map.tasks in the configuration and returns the value converted to an int.
If the value cannot be converted to an int, a NumberFormatException is thrown.
This value is the suggested number of map tasks to run.
The actual number of map tasks will be determined by the number of InputSplits that the framework constructs from the input data.
In general, there is at least one InputSplit for each input file.
The input format might be able to make multiple InputSplits from a single file.
The FileInputFormat set of input formats will split uncompressed files on HDFS block boundaries, which by default are 64MB.
This stores the String representation of n in the configuration under the key mapred.map.tasks.
The input format will attempt to ensure that this is the maximum number of map tasks, but may not be able to do so if there are more individual files that this in the input directory.
In general, tuning this and the split size setInt("mapred.min.split.size ", NUMBER), so map tasks take more than a minute to run is considered optimal.
This method looks up the value of the key mapred.reduce.tasks in the configuration and returns the value converted to an int.
If the value cannot be converted to an int, a NumberFormatException is thrown.
Unlike the number of map tasks, this is exactly the number of reduce tasks that will be run.
This method stores the String representation of n in the configuration under the key mapred.
Exactly this number of reduce tasks will be run by the framework.
If this number is 0, no reduce tasks will be run, and no output partitioning or sorting will be done.
There will be one output file per map task, written to the output directory configured for the job.
This method looks up the value of the key mapred.map.max.attempts in the configuration and returns the value converted to an int.
If the value is unset, the value 4 is returned.
If the value cannot be converted to an int, a NumberFormatException is thrown.
This method stores the String representation of n in the configuration under the key mapred.
This is rarely changed by the user other than to set it to 0 to disable the retrying of failed jobs.
This method looks up the value of key mapred.reduce.max.attempts in the configuration and returns the value converted to an int.
If the value is unset, the value 4 is returned.
If the value cannot be converted to an int, a NumberFormatException is thrown.
The framework will attempt to reschedule reduce tasks that fail up to this value times before the job is considered failed.
This method stores the String representation of n in the configuration under the key mapred.
This is rarely changed by the user other than to set it to 0 to disable the retrying of failed jobs.
The framework will attempt to reschedule reduce tasks that fail up to this value times before the job is considered failed.
This method stores the String representation of noFailures in the configuration under the key mapred.max.tracker.failures.
This value is the number of tasks for this job that may fail on a specific TaskTracker before that TaskTracker is considered failed for this job.
This method looks up the value of the key mapred.max.tracker.failures in the configuration and returns the value converted to an int.
If the value is unset, the value 4 is returned.
If the value cannot be converted to an int, a NumberFormatException will be thrown.
This value is the number of tasks for this job that may fail on a specific TaskTracker before that TaskTracker is considered, failed, for this job.
This method looks up the value of the key mapred.max.map.failures.percent in the configuration.
If the value cannot be converted to an int, a NumberFormatException is thrown.
If this value is not zero, a job may succeed if less than this value as a percentage of the map tasks cannot be successfully completed.
This method stores the String representation of percent in the configuration under the key mapred.max.map.failures.percent.
This is the percentage of map tasks that can fail without the job being marked as a failure.
This method looks up the value of the key mapred.max.reduce.failures.percent in the configuration.
If the value cannot be converted to an int, a NumberFormatException is thrown.
If this value is not zero, a job may succeed if less than this value as a percentage of the reduce tasks cannot be completed successfully.
This method stores the String representation of percentage in the configuration under the key mapred.max.reduce.failures.percent.
This is the percentage of reduce tasks that can fail without the job being marked as a failure.
Methods Providing Control Over Job Execution and naming These methods provide a way to specify a job name and a session identifier as well as to specify a priority for a job.
The naming is also helpful for distinguishing jobs in the reporting frameworks.
They also provide a way to enable profiling of specific tasks and of running a debugging script on failed tasks.
This method looks up the value of the key mapred.job.name in the configuration and returns the result.
If the value is unset, an empty String is returned.
This is the name that the job will be identified by to the user.
This method stores name in the configuration under the key mapred.job.name.
Hadoop On Demand (HOD) is a package that provides virtual map/red clusters on top of a larger HDFS installation.
The use of HOD requires an understanding of torque: http://www.clusterresources.com/pages/products/torque-resource-manager.php.
The author and the team the author was working with found it too complex for the benefits provided and discontinued using it.
The author recommends avoiding HOD unless there is a local torque expert to handle the torque installation and day-to-day operation.
This method looks up the value of the key session.id in the configuration and returns it.
If the value is unset, an empty String is returned.
This is primarily used by HOD to distinguish different virtual clusters.
The session name may also help distinguish this job in the metrics reporting framework.
This method stores sessionId in the configuration under the key session.id.
This value will be used as a token in the name used to identify any metrics that are reported by this job.
This method looks up the value of the key mapred.job.priority in the configuration.
If the value cannot be parsed as a JobPriority, an IllegalArgumentException is thrown.
Hadoop versions prior to 0.19 had only this simple mechanism for handling multiple running jobs on a cluster.)
A job with a higher priority has first right of refusal for any map or reduce task slot available on the cluster.
If jobs have equal priority, the first requester gets the open task slots.
Caution Queuing multiple jobs into a cluster with this mechanism can result in a cluster deadlock in which no job can complete.
Hadoop 0.19 also provides a queuing mechanism that provides rich control over how task slots are allocated between multiple competing jobs.
Store the String representation of prio in the configuration under the key mapred.
Jobs with a higher priority have first choice of available task slots when executing in an environment in which multiple jobs are queued into a cluster.
This method looks up the value of the key mapred.task.profile in the configuration.
If the value is unset or not the String, true, false is returned; otherwise, true is returned.
If this is true, the framework may profile specific tasks by using the results of tasks and reduce tasks if enabled.
If only profiling on maps is required, the user must specify a as the first argument.
It is harder to absolutely know the number of map tasks, but the same technique applies.
This method stores the String value of newValue in the configuration under the key mapred.
If newValue is true, profiling information will be collected for tasks that match.
This method looks up the value of the key mapred.task.profile.params in the configuration, returning that value.
If the value is unset, the following String is returned:
This String is passed to the JVM to control how the profiling is performed for the task to be profiled.
At runtime, for a profiled task a single %s will be substituted in the value with the name of the task-specific profile.out file.
This method stores value in the configuration under the key mapred.task.profile.params.
This value, with a single %s substituted with the name of the task-specific profile output file, is passed to the JVM of a task to be profiled.
If the value is unset, the range 0-2 is constructed.
If the value cannot be parsed as a set of ranges, an IllegalArgumentException is thrown.
Ranges are specified as a set of comma-separated values, in which each value is a single positive integer or two positive integers separated by a dash.
No checking is performed to ensure that the individual ranges in a comma-separated set do not overlap and ordering is not required.
A linear search through the list in the order supplied is performed for each task when profiling is enabled.
The value must be a comma-separated list of ranges composed of positive integers.
During task setup, the TaskRunner will get this value in this chapter, for a discussion of range formats.)
No checking is performed to ensure that the individual ranges in a comma-separated set do not overlap and ordering is not required.
A linear search through the list in the order supplied is performed for each task, when profiling is enabled.
This method returns the value of the key mapred.map.task.debug.script from the configuration.
This script will be run for a map task that the framework is going to mark as failed or about to kill.
The value is the script and script arguments to be used to debug failed tasks.
The value will be split into tokens using the space character as a separator.
The	path	to	the	file	containing	the	XML	representation	of	the	JobConf object for the task.
The	program	name	if	this	is	a	pipes	job	or	empty	String.
All the tokens are passed to the shell to be executed as a command.
The input of the command will be connected to /dev/null, and the standard and error output collected in a single stream.
The script is run with the current working directory as the task local directory.
If the script is not resident on all the TaskTracker nodes and normally executable, it must be distributed via the DistributedCache and symlinked.
The following code fragment arranges for the executable program that is on the local file system at LocalFileSystemPathToDebugScript to be distributed to all tasks and made available for execution as ./MyDebugScript.
The script will be invoked in the task local directory via the following shell command:
The user can specify how many lines to keep from the output by setting an int value on the key mapred.debug.out.lines.
The value specified is the number of lines from the tail of the file to keep.
This information is made available via the JobTracker web interface in the task detail output.
Caution Having shell metacharacters in the value of mapred.map.task.debug.script may lead to unpredictable results.
This method stores mDbgScript in the configuration under the key mapred.map.task.debug.
This method return the value stored under the key mapred.reduce.task.debug.script.
This method stores rDbgScript in the configuration under the key mapred.reduce.task.
If a URL is stored in the configuration under the key job.end.notification.url or via.
The parameter job.end.retry.attempts controls the number of retry attempts that will be made if the HTTP GET does not return the numeric status code of 200
The parameter job.end.retry.interval controls the delay between retry attempts, with a default value of 30,000 msec.
If either parameter is set and the value cannot be converted to an int, a NumberFormatException will be thrown in the context of the JobTracker, which may cause the JobTracker to abort or otherwise behave unpredictably.
This method looks up the value of the key job.end.notification.url in the configuration and returns that value.
The value will be used as a URL in an HTTP GET.
This method stores uri in the configuration under the key job.end.notification.url.
This method looks up the key mapred.job.queue.name in the configuration and returns the value.
This method stores queueName in the configuration under the key mapred.job.queue.name.
If queueName is not a valid queue name, the JobTracker behavior is unpredictable.
Hadoop provides a mechanism to control the limit of virtual memory that an individual task and the task’s children use.
The user can specify the maximum amount of memory, in kilobytes, in the configuration under the key mapred.task.maxmemory; the method setMaxVirtualMemoryForTask(vmem) can also be used.
The overall default can be specified by storing the value in kilobytes under the key mapred.task.default.maxmemory.
When the virtual memory consumption of a task and its children exceed this value, the task is killed by the framework, and marked as failed.
This is predicated on the system reporting virtual memory usage for processes in kilobytes.
The value of -1 tells the framework to use the framework limit, which is stored under the key mapred.task.default.maxmemory.
The default value for this key is 536,870,912 kilobytes (roughly one-half terabyte)
This method looks up the value of the key mapred.task.maxmemory and returns it as a long.
If the value cannot be converted to a long, a NumberFormatException will be thrown.
This method stores the String version of vmem in the configuration under the key mapred.
If a task and its children’s virtual memory usage exceed this value, the task will be killed by the framework.
Convenience Methods These methods provide convenience functions for accessing the configuration data.
This method returns the number of keys in the configuration.
This method completely clears all keys and values from the configuration.
This method returns an integrator for to the key/value pairs stored in the configuration.
This method serializes the key/value pairs in the configuration to XML in the standard configuration file format and writes the data to out.
This method is used by the framework to serialize the job configuration and store it in HDFS so that the individual tasks load the job configuration at task start.
This method returns the class loader that is used to search for resources that are added via the.
This method sets the class loader to be used for locating resources and instantiating classes to classLoader.
This is primarily used by the framework when preparing map and reduce tasks to include the DistributedCache classpath items in the classpath.
This returns a String composed of the names of all the resources that were loaded into this configuration.
This method does not return the key/value pairs that are stored in the configuration.
Methods Used to Pass Configurations Through SequenceFiles The configuration class implements the Writable interface, which allows the framework to serialize and deserialize the configuration.
It is not clear that these methods are used by the framework at the current time.
The key value pairs will be read from the DataInput stream in.
This method serializes the configuration into a form that is suitable for use in SequenceFiles.
The serialized data is written to the DataOutput stream out.
ClusterMapReduceTestCase description of, 207 Hadoop Core JAR missing or malformed,
ClusterMapReduceTestCase class description of, 207 Hadoop Core JAR missing or malformed,
See also Eclipse framework; MapReduce framework; monitoring framework for large clusters; Spring Framework, initializing mapper with.
Capacity Scheduler enabling, 281 XML block for each queue to be defined,
Spring task initialization method, 146–147 streaming command to invoke LongSum.
Tasktracker error log message due to TCP port unavailability, 93–94
Sun VM Invocation Options guide, 230 support for multihomed machines, sample.
TupleWritable class, 271–274 type checking for chained keys and values,
No part may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage or retrieval system, without the prior written permission of the copyright owner and the publisher.
The purchaser may print the work in full or in part for their own noncommercial use.
The purchaser may place the eBook title on any of their personal computers for their own personal reading and reference.
You Need the Companion eBook Your purchase of this book entitles you to buy the.
Once you purchase your book, getting the $10 companion eBook is simple:
Cluster-Level Tunable Parameters Server-Level parameters hDFS tunable parameters Jobtracker and tasktracker tunable parameters.
