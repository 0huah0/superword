Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.
To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
The system is built from many inexpensive commodity components that often fail.
It must constantly monitor itself and detect, tolerate, and recover promptly from component failures on a routine basis.
Most of our target applications place a premium on processing data in bulk at a high rate, while few have stringent response time requirements for an individual read or write.
A GFS cluster consists of a single master and multiple.
The master does not keep a persistent record of which.
The master can keep itself up-to-date thereafter because it controls all chunk placement and monitors chunkserver status with regular HeartBeat messages.
To minimize startup time, we must keep the log small.
The master checkpoints its state whenever the log grows beyond a certain size so that it can recover by loading the latest checkpoint from local disk and replaying only the.
Serial defined defined success interspersed with Concurrent consistent inconsistent successes but undefined Failure inconsistent.
We designed the system to minimize the master’s involvement in all operations.
With that background, we now describe how the client, master, and chunkservers interact to implement data mutations, atomic record append, and snapshot.
A mutation is an operation that changes the contents or.
The client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas.
If no one has a lease, the master grants one to a replica it chooses (not shown)
The master replies with the identity of the primary and the locations of the other (secondary) replicas.
It needs to contact the master again only when the primary.
The primary forwards the write request to all secondary replicas.
Each secondary replica applies mutations in the same serial number order assigned by the primary.
The secondaries all reply to the primary indicating that they have completed the operation.
Thus, each machine’s full outbound bandwidth is used to transfer the data as fast as possible rather than divided among multiple recipients.
It sends the data to the closest chunkserver, say S1
Our network topology is simple enough that “distances” can be accurately estimated from IP addresses.
Finally, we minimize latency by pipelining the data transfer over TCP connections.
Once a chunkserver receives some data, it starts forwarding immediately.
Pipelining is especially helpful to us because we use a switched network with full-duplex links.
Sending the data immediately does not reduce the receive rate.
Without network congestion, the ideal elapsed time for transferring B bytes to R replicas is B/T +RL where T is the network throughput and L is latency to transfer bytes between two machines.
It then asks each chunkserver that has a current replica of C to create a new chunk called C’
By creating the new chunk on the same chunkservers as the original, we ensure that the data can be copied locally, not over the network (our disks are about three times as fast as our 100 Mb Ethernet links)
From this point, request handling is no different from that for any chunk: the master grants one of the replicas a lease on the new chunk C’ and replies to the client, which can write the chunk normally, not knowing that it has just been created from an existing chunk.
In addition, it manages chunk replicas throughout the system: it makes placement decisions, creates new chunks and hence replicas, and coordinates various system-wide activities to keep chunks fully replicated, to balance load across all the chunkservers, and to reclaim unused storage.
A GFS cluster is highly distributed at more levels than.
Chunk replicas are created for three reasons: chunk creation, re-replication, and rebalancing.
When the master creates a chunk, it chooses where to.
For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas.
Whenever the master grants a new lease on a chunk, it.
One of our greatest challenges in designing the system is.
Component failures can result in an unavailable system or, worse, corrupted data.
We discuss how we meet these challenges and the tools we have built into the system to diagnose problems when they inevitably occur.
Among hundreds of servers in a GFS cluster, some are.
Both the master and the chunkserver are designed to restore their state and start in seconds no matter how they terminated.
In fact, we do not distinguish between normal and abnormal termination; servers are routinely shut down just by killing the process.
Clients and other servers experience a minor hiccup as they time out on their outstanding requests, reconnect to the restarted server, and retry.
Like the primary, it polls chunkservers at startup (and infrequently thereafter) to locate chunk replicas and exchanges frequent handshake messages with them to monitor their status.
It depends on the primary master only for replica location updates resulting from the primary’s decisions to create and delete replicas.
This allows us to detect corruption in chunks that are rarely read.
Once the corruption is detected, the master can create a new uncorrupted replica and delete the corrupted replica.
This prevents an inactive but corrupted chunk replica from fooling the master into thinking that it has enough valid replicas of a chunk.
In this section we present a few micro-benchmarks to illustrate the bottlenecks inherent in the GFS architecture and implementation, and also some numbers from real clusters in use at Google.
The two switches are connected with a 1 Gbps link.
It does not interact very well with the pipelining scheme we use for pushing data to chunk replicas.
Delays in propagating data from one replica to another reduce the overall write rate.
We now examine two clusters in use within Google that.
Cluster A is used regularly for research and development by over a hundred engineers.
A typical task is initiated by a human user and runs up to several hours.
It reads through a few MBs to a few TBs of data, transforms or analyzes the data, and writes the results back to the cluster.
The chunkservers in aggregate store tens of GBs of metadata, mostly the checksums for 64 KB blocks of user data.
The only other metadata kept at the chunkservers is the chunk version number discussed in Section 4.5
The metadata kept at the master is much smaller, only.
Therefore recovery is fast: it takes only a few seconds to read this metadata from disk before the server is able to answer queries.
Table 3 shows read and write rates for various time periods.
Both clusters had been up for about one week when these measurements were taken.
The clusters had been restarted recently to upgrade to a new version of GFS.) The average write rate was less than 30 MB/s since the.
Table 3 also shows that the rate of operations sent to the.
The master can easily keep up with this rate, and therefore is not a bottleneck for these workloads.
In an earlier version of GFS, the master was occasionally.
After a chunkserver fails, some chunks will become underreplicated and must be cloned to restore their replication levels.
The time it takes to restore all such chunks depends on the amount of resources.
In one experiment, we killed a single chunkserver in cluster B.
This double failure reduced 266 chunks to having a single replica.
In this section, we present a detailed breakdown of the.
Cluster X is for research and development while cluster Y is for production data processing.
For example, GFS client code may break a read into multiple RPCs to increase parallelism, from which we infer the original read.
Since our access patterns are highly stylized, we expect any error to be in the noise.
Explicit logging by applications might have provided slightly more accurate data, but it is logistically impossible to recompile and restart thousands of running clients to do so and cumbersome to collect the results from as many machines.
One should be careful not to overly generalize from our.
For reads, the size is the amount of data actually read and transferred, rather than the amount requested.
We measured the amount of data overwritten on primary replicas.
Although this is minute, it is still higher than we expected.
It turns out that most of these overwrites came from client retries due to errors or timeouts.
They are not part of the workload per se but a consequence of the retry mechanism.
Table 6 shows the breakdown by type of requests to the.
In the process of building and deploying GFS, we have.
When appropriate, we improve the kernel and share the changes with the open source community.
In contrast to systems like AFS, xFS, Frangipani [12], and.
We focus on addressing day-to-day data processing needs for complicated distributed systems with existing commodity components.
The frequency of these failures motivated a novel online repair mechanism that regularly and transparently repairs the damage and compensates for lost replicas as soon as possible.
Additionally, we use checksumming to detect data corruption at the disk or IDE subsystem level, which becomes all too common given the number of disks in the system.
It is an important tool that enables us to continue to innovate and attack problems on the scale of the entire web.
Swift: Using distributed disk striping to provide high I/O data rates.
